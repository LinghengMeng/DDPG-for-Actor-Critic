{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import layers\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "from noise import AdaptiveParamNoiseSpec,NormalActionNoise,OrnsteinUhlenbeckActionNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, sess, observation_space,  action_space,\n",
    "                 learning_rate, tau, batch_size,\n",
    "                 actor_model_save_path = 'results/models',\n",
    "                 target_actor_model_save_path = 'results/models',\n",
    "                 restore_model_flag=False,\n",
    "                 restore_model_version = 0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str\n",
    "            \n",
    "        sess: tf.Session\n",
    "            \n",
    "        observation_space: gym.spaces.Box\n",
    "            \n",
    "        action_space: gym.spaces.Box\n",
    "            \n",
    "        learning_rate: float\n",
    "            \n",
    "        tau: float\n",
    "            \n",
    "        batch_size: int\n",
    "            \n",
    "        restore_model_flag: bool default=False\n",
    "        \n",
    "        actor_model_save_path_and_name: str default = 'results/models/actor_model.ckpt'\n",
    "        \n",
    "        target_actor_model_save_path_and_name: str default = 'results/models/target_actor_model.ckpt'\n",
    "        \"\"\"\n",
    "        \n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        self.s_dim = observation_space.shape[0]\n",
    "        self.a_dim = action_space.shape[0]\n",
    "        self.action_bound_high = action_space.high\n",
    "        self.action_bound_low = action_space.low\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Info for load pre-trained actor models\n",
    "        self.actor_model_save_path = actor_model_save_path\n",
    "        self.target_actor_model_save_path = target_actor_model_save_path\n",
    "        \n",
    "        self.restore_model_flag = restore_model_flag\n",
    "        self.restore_model_version = self._find_the_most_recent_model_version() \n",
    "        if self.restore_model_flag and self.restore_model_version == -1:\n",
    "            logging.error('You do not have pretrained models.\\nPlease set \"load_pretrained_agent_flag = False\".')\n",
    "        \n",
    "        with tf.name_scope(self.name):\n",
    "            \n",
    "            with tf.variable_scope(self.name) as self.scope:\n",
    "                # Create Actor Model\n",
    "                self.inputs, self.out = self.create_actor_network()\n",
    "                self.network_params = tf.trainable_variables(scope=self.name)\n",
    "                self.actor_model_saver = tf.train.Saver(self.network_params) # Saver to save and restore model variables\n",
    "                \n",
    "                # Create Target Actor Model\n",
    "                self.target_inputs, self.target_out = self.create_actor_network()\n",
    "                self.target_network_params = tf.trainable_variables(scope=self.name)[len(self.network_params):]\n",
    "                self.target_actor_model_saver = tf.train.Saver(self.target_network_params) # Saver to save and restore model variables\n",
    "                \n",
    "            # Op for periodically updating target network\n",
    "            self.update_target_network_params = \\\n",
    "                [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                      tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                    for i in range(len(self.target_network_params))]\n",
    "                \n",
    "            # This gradient will be provided by the critic network: d[Q(s,a)]/d[a]\n",
    "            self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "    \n",
    "            # Combine the gradients here\n",
    "            # The reason of negative self.action_gradient here is we want to do \n",
    "            # gradient ascent, and AdamOptimizer will do gradient descent when applying\n",
    "            # a gradient.\n",
    "            self.unnormalized_actor_gradients = tf.gradients(self.out, \n",
    "                                                             self.network_params, \n",
    "                                                             -self.action_gradient) \n",
    "            # Normalized actor gradient\n",
    "            self.actor_gradients = list(map(lambda x: tf.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "    \n",
    "            # Optimization Op\n",
    "            self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "            \n",
    "            # Initialize variables in variable_scope: self.name\n",
    "            # Note: make sure initialize variables **after** defining all variable\n",
    "            self.sess.run(tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = self.name)))\n",
    "            \n",
    "            # Restor Actor and Target-Actor Models\n",
    "            if self.restore_model_flag == True:\n",
    "                actor_filepath = os.path.join(self.actor_model_save_path,\n",
    "                                         self.name + '_' + str(self.restore_model_version)+'.ckpt')\n",
    "                target_actor_filepath = os.path.join(self.actor_model_save_path,\n",
    "                                         self.name + '_target_' + str(self.restore_model_version)+'.ckpt')\n",
    "                self.restore_actor_and_target_actor_network(actor_filepath, \n",
    "                                                            target_actor_filepath)\n",
    "                    \n",
    "    def create_actor_network(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs = tf.placeholder(tf.float32, shape=(None, self.s_dim), name = 'ActorInput')\n",
    "        h1 = layers.Dense(units = 100, activation = tf.nn.relu, \n",
    "                          kernel_initializer = tf.initializers.truncated_normal)(inputs)\n",
    "        h1 = layers.BatchNormalization()(h1)\n",
    "        h1 = layers.Dropout(0.5)(h1)\n",
    "        \n",
    "        h2 = layers.Dense(units = 50, activation = tf.nn.relu, \n",
    "                          kernel_initializer = tf.initializers.truncated_normal)(h1)\n",
    "        h2 = layers.BatchNormalization()(h2)\n",
    "        h2 = layers.Dropout(0.5)(h2)\n",
    "        \n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        out = layers.Dense(units = self.a_dim, activation = tf.tanh, \n",
    "                           kernel_initializer=tf.initializers.random_uniform(minval = -0.003, maxval = 0.003), \n",
    "                           name = 'ActorOutput')(h2)\n",
    "        return inputs, out\n",
    "        \n",
    "    def save_actor_network(self, version_number = 0):\n",
    "        \"\"\"save actor and target actor model\"\"\"\n",
    "        actor_filepath = os.path.join(self.actor_model_save_path,\n",
    "                                      self.name + '_' + str(version_number)+'.ckpt')\n",
    "        target_actor_filepath = os.path.join(self.target_actor_model_save_path,\n",
    "                                             self.name +'_target_' + str(version_number)+'.ckpt')\n",
    "        \n",
    "        self.actor_model_saver.save(self.sess, actor_filepath)\n",
    "        self.target_actor_model_saver.save(self.sess, target_actor_filepath)\n",
    "        \n",
    "        logging.info('Actor model saved in path: {}.'.format(actor_filepath))\n",
    "        logging.info('Target Actor model saved in path: {}.'.format(target_actor_filepath))\n",
    "\n",
    "    def restore_actor_and_target_actor_network(self, actor_filepath, target_actor_filepath):\n",
    "        \"\"\" \n",
    "        The following code is to inspect variables in a checkpoint:\n",
    "            from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "            chkp.print_tensors_in_checkpoint_file(file_path, tensor_name='', all_tensors=True, all_tensor_names=True)\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        self.sess.run(tf.variables_initializer(self.network_params, name='init_network_params'))\n",
    "        self.sess.run(tf.variables_initializer(self.target_network_params, name='init_target_network_params'))\n",
    "        \n",
    "        self.actor_model_saver.restore(self.sess, actor_filepath)\n",
    "        self.target_actor_model_saver.restore(self.sess, target_actor_filepath)\n",
    "        \n",
    "        \n",
    "        \n",
    "        logging.info('Restored acotor: {}'.format(actor_filepath))\n",
    "        logging.info('Restored target acotor: {}'.format(target_actor_filepath))\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        \"\"\"Train actor\"\"\"\n",
    "        self.sess.run(self.optimize, \n",
    "                      feed_dict={self.inputs: inputs,\n",
    "                                 self.action_gradient: a_gradient})\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of Actor Model.\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.out, \n",
    "                             feed_dict={self.inputs: inputs})\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        \"\"\"\n",
    "        Prediction of Target Actor Model.\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.target_out, \n",
    "                             feed_dict={self.target_inputs: inputs})\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update Target Actor Model\"\"\"\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "    \n",
    "    def _find_the_most_recent_model_version(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        the_most_recent_model_version: int\n",
    "            the most recent model version. If no saved model, return -1.\n",
    "        \"\"\"\n",
    "        # Find the most recent version\n",
    "        model_version = []\n",
    "        for file_name_temp in os.listdir(self.actor_model_save_path):\n",
    "            if self.name+'_target_' in file_name_temp:\n",
    "                _, version_temp = file_name_temp.split('.')[0].split(self.name+'_target_')\n",
    "                model_version.append(version_temp)\n",
    "        if len(model_version) != 0:\n",
    "            the_most_recent_model_version = max([int(i) for i in model_version])\n",
    "        else:\n",
    "            the_most_recent_model_version = -1\n",
    "        return the_most_recent_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, sess, observation_space, action_space,\n",
    "                 learning_rate, tau, gamma,\n",
    "                 critic_model_save_path = 'results/models',\n",
    "                 target_critic_model_save_path = 'results/models',\n",
    "                 restore_model_flag=False,\n",
    "                 restore_model_version = 0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: str\n",
    "            The name of this cirtic network. Giving a name to object of CriticNetwork\n",
    "            is necessary to avoid messing up trainable variables together.\n",
    "        sess: tf.Session\n",
    "            tf.Session to run computational graph\n",
    "        observation_space: gym.spaces.Box\n",
    "            observation space of environment\n",
    "        action_space: gym.spaces.Box\n",
    "            action space of environment\n",
    "        learning_rate: float\n",
    "            learning rate to train CriticNetwork\n",
    "        tau: float\n",
    "            hyper-parameter weighting the update of target network\n",
    "        gamma: float\n",
    "            discount rate\n",
    "        critic_model_save_path: str default = 'results/models'\n",
    "            path of critic model we are going to save\n",
    "        target_critic_model_save: str default = 'results/models/target_critic_model.ckpt'\n",
    "            path of target critic model we are going to save\n",
    "        restore_model_flag: bool default=False:\n",
    "            indicator of whether to restore a pre-trained critic network\n",
    "        restore_model_version: int default = 0\n",
    "            if restore model, this parameter gives the number of specific version\n",
    "            of models we are going to restore\n",
    "        \"\"\"\n",
    "        # name is necessary, since we will reuse this graph multiple times.\n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        self.s_dim = observation_space.shape[0]\n",
    "        self.a_dim = action_space.shape[0]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Info for save and load pre-trained critic models\n",
    "        self.critic_model_save_path = critic_model_save_path\n",
    "        self.target_critic_model_save_path = target_critic_model_save_path\n",
    "        \n",
    "        self.restore_model_flag = restore_model_flag\n",
    "        self.restore_model_version = self._find_the_most_recent_model_version()\n",
    "        if self.restore_model_flag and self.restore_model_version == -1:\n",
    "            raise Exception('You do not have pretrained models.\\nPlease set \"load_pretrained_agent_flag = False\".')\n",
    "        \n",
    "        with tf.name_scope(self.name):\n",
    "            \n",
    "            with tf.variable_scope(self.name) as self.scope:\n",
    "                # Create Critic Model\n",
    "                self.inputs, self.action, self.out = self.create_critic_network()\n",
    "                self.network_params = tf.trainable_variables(scope=self.name)\n",
    "                self.critic_model_saver = tf.train.Saver(self.network_params) # Saver to save and restore model variables  \n",
    "                \n",
    "                # Create Target Critic Model\n",
    "                self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "                self.target_network_params = tf.trainable_variables(scope=self.name)[len(self.network_params):]\n",
    "                self.target_critic_model_saver = tf.train.Saver(self.target_network_params)\n",
    "            \n",
    "            #Tracer()()\n",
    "            # Op for periodically updating target network with online network\n",
    "            # weights with regularization\n",
    "            self.update_target_network_params = \\\n",
    "                [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "                                                      + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                    for i in range(len(self.target_network_params))]\n",
    "    \n",
    "            # Network target (y_i)\n",
    "            self.target_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "    \n",
    "            # Define loss and optimization Op\n",
    "            self.loss = tf.losses.mean_squared_error(labels = self.target_q_value,\n",
    "                                                     predictions = self.out)\n",
    "            self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "    \n",
    "            # Get the gradient of the net w.r.t. the action.\n",
    "            # For each action in the minibatch (i.e., for each x in xs),\n",
    "            # this will sum up the gradients of each critic output in the minibatch\n",
    "            # w.r.t. that action. Each output is independent of all\n",
    "            # actions except for one.\n",
    "            self.action_grads = tf.gradients(self.out, self.action)\n",
    "            \n",
    "            # Initialize variables in variable_scope: self.name\n",
    "            # Note: make sure initialize variables **after** defining all variable\n",
    "            self.sess.run(tf.variables_initializer(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = self.name)))\n",
    "            \n",
    "            # Restore Critic and Target Critic Models\n",
    "            if self.restore_model_flag == True:\n",
    "                critic_filepath = os.path.join(self.critic_model_save_path,\n",
    "                                               self.name + '_' + str(self.restore_model_version)+'.ckpt')\n",
    "                target_critic_filepath = os.path.join(self.critic_model_save_path,\n",
    "                                                      self.name + '_target_' + str(self.restore_model_version)+'.ckpt')\n",
    "                self.restore_critic_and_target_critic_network(critic_filepath, \n",
    "                                                              target_critic_filepath)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        \"\"\"Create critic network\"\"\"\n",
    "        obs = tf.placeholder(tf.float32, shape=(None, self.s_dim), name = 'CriticInputState')\n",
    "        act = tf.placeholder(tf.float32, shape=(None, self.a_dim), name = 'CriticInputAction')\n",
    "        \n",
    "        h1_obs = layers.Dense(units = 400, activation = tf.nn.relu, \n",
    "                          kernel_initializer = tf.initializers.truncated_normal)(obs)\n",
    "        h1_obs = layers.BatchNormalization()(h1_obs)\n",
    "        h1_obs = layers.Dropout(0.5)(h1_obs)\n",
    "        \n",
    "        h1_act = layers.Dense(units = 400, activation = tf.nn.relu, \n",
    "                          kernel_initializer = tf.initializers.truncated_normal)(act)\n",
    "        h1_act = layers.BatchNormalization()(h1_act)\n",
    "        h1_act = layers.Dropout(0.5)(h1_act)\n",
    "        \n",
    "        merged = tf.concat([h1_obs, h1_act], axis=1)\n",
    "        \n",
    "        h2 = layers.Dense(units = 300, activation = tf.nn.relu, \n",
    "                          kernel_initializer = tf.initializers.truncated_normal)(merged)\n",
    "        h2 = layers.BatchNormalization()(h2)\n",
    "        h2 = layers.Dropout(0.5)(h2)\n",
    "        \n",
    "        # Linear layer connected to 1 output representing Q(s,a)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        out = layers.Dense(units = 1,\n",
    "                           kernel_initializer=tf.initializers.random_uniform(minval = -0.003, maxval = 0.003), \n",
    "                           name = 'CriticOutput')(h2)\n",
    "        \n",
    "        return obs, act, out\n",
    "\n",
    "    def save_critic_network(self, version_number = 0):\n",
    "        \"\"\"\n",
    "        Function used to save critic and target critic model\n",
    "        Parameters\n",
    "        ----------\n",
    "        version_number: int default = 0\n",
    "            the time when save this ciritic and its target critic models.\n",
    "        \"\"\"\n",
    "        critic_filepath = os.path.join(self.critic_model_save_path,\n",
    "                                      self.name + '_' + str(version_number)+'.ckpt')\n",
    "        target_critic_filepath = os.path.join(self.target_critic_model_save_path,\n",
    "                                             self.name +'_target_' + str(version_number)+'.ckpt')\n",
    "        \n",
    "        self.critic_model_saver.save(self.sess, critic_filepath)\n",
    "        self.target_critic_model_saver.save(self.sess, target_critic_filepath)\n",
    "        \n",
    "        logging.info('Critic model saved in path: {}.'.format(critic_filepath))\n",
    "        logging.info('Target Critic model saved in path: {}.'.format(target_critic_filepath))\n",
    "        \n",
    "    def restore_critic_and_target_critic_network(self, critic_filepath, target_critic_filepath):\n",
    "        \"\"\" \n",
    "        The following code is to inspect variables in a checkpoint:\n",
    "            from tensorflow.python.tools import inspect_checkpoint as chkp\n",
    "            chkp.print_tensors_in_checkpoint_file(file_path, tensor_name='', all_tensors=True, all_tensor_names=True)\n",
    "        \"\"\"\n",
    "        self.critic_model_saver.restore(self.sess, critic_filepath)\n",
    "        self.target_critic_model_saver.restore(self.sess, target_critic_filepath)\n",
    "        logging.info('Restored acotor: {}'.format(critic_filepath))\n",
    "        logging.info('Restored target acotor: {}'.format(target_critic_filepath))\n",
    "\n",
    "    def train(self, observation, action, target_q_value):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        loss: mean square error\n",
    "            \n",
    "        out: output of Critic_Network\n",
    "            \n",
    "        optimize: tf.operation\n",
    "        \"\"\"\n",
    "        return self.sess.run([self.loss, self.out, self.optimize], \n",
    "                             feed_dict={self.inputs: observation,\n",
    "                                        self.action: action,\n",
    "                                        self.target_q_value: target_q_value})\n",
    "\n",
    "    def predict(self, observation, action):\n",
    "        return self.sess.run(self.out, \n",
    "                             feed_dict={self.inputs: observation,\n",
    "                                        self.action: action})\n",
    "\n",
    "    def predict_target(self, observation, action):\n",
    "        \"\"\"\n",
    "        Prediction of Target-Critic Model\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.target_out, \n",
    "                             feed_dict={self.target_inputs: observation,\n",
    "                                        self.target_action: action})\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, \n",
    "                             feed_dict={\n",
    "                                     self.inputs: inputs,\n",
    "                                     self.action: actions})\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "    \n",
    "    def _find_the_most_recent_model_version(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        the_most_recent_model_version: int\n",
    "            the most recent model version\n",
    "        \"\"\"\n",
    "        # Find the most recent version\n",
    "        model_version = []\n",
    "        for file_name_temp in os.listdir(self.critic_model_save_path):\n",
    "            if self.name+'_target_' in file_name_temp:\n",
    "                _, version_temp = file_name_temp.split('.')[0].split(self.name+'_target_')\n",
    "                model_version.append(version_temp)\n",
    "        if len(model_version) != 0:\n",
    "            the_most_recent_model_version = max([int(i) for i in model_version])\n",
    "        else:\n",
    "            the_most_recent_model_version = -1\n",
    "        return the_most_recent_model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_Actor_Critic():\n",
    "    \"\"\"\n",
    "    LASAgent is the learning agent of Living Architecture System. Basically, it\n",
    "    consists of three main components coding policies and value-functions:\n",
    "        1. Extrinsically motivated actor-critic model\n",
    "        2. Knowledge-based Intrinsically motivated actor-critic model\n",
    "        3. Competence-based Intrinsically motivated actor-critic model\n",
    "    And two main components producing intrinsic motivation\n",
    "        1. Knowledge-based intrinsic motivation\n",
    "        2. Competence-based intrinsic motivation\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, agent_name,\n",
    "                 observation_space, action_space,\n",
    "                 actor_lr = 0.0001, actor_tau = 0.001,\n",
    "                 critic_lr = 0.0001, critic_tau = 0.001, gamma = 0.99,\n",
    "                 minibatch_size = 64,\n",
    "                 max_episodes = 50000, max_episode_len = 1000,\n",
    "                 # Exploration Strategies\n",
    "                 exploration_action_noise_type = 'ou_0.2',\n",
    "                 exploration_epsilon_greedy_type = 'none',\n",
    "                 # Save Summaries\n",
    "                 save_dir = '',\n",
    "                 experiment_runs = '',\n",
    "                 # Save and Restore Actor-Critic Model\n",
    "                 restore_actor_model_flag = False,\n",
    "                 restore_critic_model_flag = False,\n",
    "                 restore_env_model_flag = False,\n",
    "                 restore_model_version = 0):\n",
    "        \"\"\"\n",
    "        Intialize LASAgent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        actor_lr: float default = 0.0001\n",
    "            actor model learning rate\n",
    "        \n",
    "        actor_tau: float default = 0.001\n",
    "            target actor model updating weight\n",
    "        \n",
    "        critic_lr: float default = 0.0001\n",
    "            critic model learning rate\n",
    "        \n",
    "        critic_tau: float default = 0.001\n",
    "            target critic model updating weight\n",
    "        \n",
    "        gamma default:int = 0.99\n",
    "            future reward discounting paramter\n",
    "        \n",
    "        minibatch_size:int default = 64\n",
    "            size of minibabtch\n",
    "        \n",
    "        max_episodes:int default = 50000\n",
    "            maximum number of episodes\n",
    "        \n",
    "        max_episode_len: int default = 1000\n",
    "            maximum lenght of each episode\n",
    "        \n",
    "        exploration_action_noise_type: str default = 'ou_0.2',\n",
    "            set up action noise. Options:\n",
    "                1. 'none' (no action noise)\n",
    "                2. 'adaptive-param_0.2'\n",
    "                3. 'normal_0.2'\n",
    "                4. 'ou_0.2' \n",
    "        \n",
    "        exploration_epsilon_greedy_type: str default = 'none',\n",
    "            set up epsilon-greedy.\n",
    "            1. If exploration_epsilon_greedy_type == 'none', no epsilon-greedy.\n",
    "            2. 'epsilon-greedy-max_1_min_0.05_decay_0.999'\n",
    "        \n",
    "        save_dir: string default='')\n",
    "            directory to save tensorflow summaries and pre-trained models\n",
    "        \n",
    "        experiment_runs: str default = ''\n",
    "            directory to save summaries of a specific run \n",
    "        \n",
    "        restore_actor_model_flag: bool default = False\n",
    "            indicate whether load pre-trained actor model\n",
    "        \n",
    "        restore_critic_model_flag: bool default = False\n",
    "            indicate whetther load pre-trained critic model\n",
    "        \"\"\"\n",
    "        # Init Environment Related Parameters\n",
    "        self.sess = sess\n",
    "        self.agent_name = agent_name\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "        self.save_dir = save_dir\n",
    "        self.experiment_runs = experiment_runs\n",
    "        # Temporary Memory\n",
    "        self.first_experience = True\n",
    "        self.observation_old = []\n",
    "        self.action_old = []\n",
    "        self.reward_new = []\n",
    "        self.observation_new = []\n",
    "        # =================================================================== #\n",
    "        #                Initialize Global Hyper-parameters                   #\n",
    "        # =================================================================== #        \n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_len = max_episode_len\n",
    "        self.episode_counter = 1\n",
    "        self.steps_counter = 1      # Steps elapsed in one episode\n",
    "        self.total_step_counter = 1 # Steps elapsed in whole life\n",
    "        self.render_env = False\n",
    "        # =================================================================== #\n",
    "        #                 Initialize Replay Buffers for                       #\n",
    "        #         Extrinsic and Intrinsic Policy, and Environment Model        #\n",
    "        # =================================================================== #         \n",
    "        # ********************************************* #\n",
    "        #         Replay Buffer for Extrinsic Policy    #\n",
    "        # ********************************************* #\n",
    "        self.buffer_size = 1000000\n",
    "        self.random_seed = 1234\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.random_seed)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #      Initialize Parameters for Both Actor and Critic Model          #\n",
    "        # =================================================================== #        \n",
    "        self.minibatch_size = 64\n",
    "        self.models_dir = os.path.join(self.save_dir,'models',self.experiment_runs)\n",
    "        if not os.path.exists(self.models_dir):\n",
    "            os.makedirs(self.models_dir)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #       Initialize Extrinsically Motivated Actor-Critic Model         #\n",
    "        # =================================================================== #\n",
    "        # Extrinsically Motivated Actor\n",
    "        self.extrinsically_motivated_actor_name = self.agent_name+'_extrinsically_motivated_actor_name'\n",
    "        self.extrinsic_actor_lr = actor_lr\n",
    "        self.extrinsic_actor_tau = actor_tau\n",
    "        # Restore Pre-trained Actor Modles\n",
    "        self.extrinsic_actor_model_save_path = self.models_dir\n",
    "        self.target_extrinsic_actor_model_save_path = self.models_dir\n",
    "        \n",
    "        self.restore_extrinsic_actor_model_flag = restore_actor_model_flag\n",
    "        self.restore_extrinsic_actor_model_version = restore_model_version\n",
    "        \n",
    "        self.extrinsic_actor_model = ActorNetwork(self.extrinsically_motivated_actor_name,\n",
    "                                        self.sess, \n",
    "                                        self.observation_space, \n",
    "                                        self.action_space,\n",
    "                                        self.extrinsic_actor_lr, \n",
    "                                        self.extrinsic_actor_tau,\n",
    "                                        self.minibatch_size,\n",
    "                                        self.extrinsic_actor_model_save_path,\n",
    "                                        self.target_extrinsic_actor_model_save_path,\n",
    "                                        self.restore_extrinsic_actor_model_flag,\n",
    "                                        self.restore_extrinsic_actor_model_version)\n",
    "        # Extrinsically Motivated Critic\n",
    "        self.extrinsically_motivated_critic_name = self.agent_name+'_extrinsically_motivated_critic_name'\n",
    "        self.extrinsic_critic_lr = critic_lr\n",
    "        self.extrinsic_critic_tau = critic_tau\n",
    "        self.extrinsic_gamma = gamma\n",
    "        # Restore Pre-trained Critic Model\n",
    "        self.extrinsic_critic_model_save_path = self.models_dir\n",
    "        self.target_extrinsic_critic_model_save_path = self.models_dir\n",
    "        \n",
    "        self.restore_extrinsic_critic_model_flag = restore_critic_model_flag\n",
    "        self.restore_extrinsic_critic_model_version = restore_model_version\n",
    "        \n",
    "        self.extrinsic_critic_model = CriticNetwork(self.extrinsically_motivated_critic_name,\n",
    "                                          self.sess,\n",
    "                                          self.observation_space,\n",
    "                                          self.action_space,\n",
    "                                          self.extrinsic_critic_lr,\n",
    "                                          self.extrinsic_critic_tau,\n",
    "                                          self.extrinsic_gamma,\n",
    "                                          self.extrinsic_critic_model_save_path,\n",
    "                                          self.target_extrinsic_critic_model_save_path,\n",
    "                                          self.restore_extrinsic_critic_model_flag,\n",
    "                                          self.restore_extrinsic_critic_model_version)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #                  Initialize Exploration Strategies                  #\n",
    "        # =================================================================== #        \n",
    "        # 1. Action Noise to Maintain Exploration\n",
    "        self.exploration_action_noise_type = exploration_action_noise_type\n",
    "        self.actor_noise = self._init_action_noise(self.exploration_action_noise_type, self.action_space.shape[0])\n",
    "        # 2. Epsilon-Greedy\n",
    "        self.exploration_epsilon_greedy_type = exploration_epsilon_greedy_type # 'epsilon-greedy-max_1_min_0.05_decay_0.999'\n",
    "        self.epsilon_max, self.epsilon_min, self.epsilon_decay = self._init_epsilon_greedy(self.exploration_epsilon_greedy_type)\n",
    "        self.epsilon = self.epsilon_max     \n",
    "\n",
    "# =================================================================== #\n",
    "#                       Main Interaction Functions                    #\n",
    "# =================================================================== #\n",
    "    def perceive_and_act(self, observation, reward, done):\n",
    "        \"\"\"\n",
    "        Perceive observation and reward, then return action based on current\n",
    "        observation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        observation: np.shape(observation) = (obs_dim,)\n",
    "            observation\n",
    "        reward: float\n",
    "            reward of previous action\n",
    "        done: bool\n",
    "            whether current simulation is done\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        action: np.shape(action) = (act_dim, )\n",
    "            action generated by agent\n",
    "        \"\"\"\n",
    "        self.observation_new = observation\n",
    "        self.reward_new = reward\n",
    "        self.done = done\n",
    "        # *********************************** # \n",
    "        #            Produce Action           #\n",
    "        # *********************************** #\n",
    "        # If this is the first action, no complete experience to remember.\n",
    "        if self.first_experience:\n",
    "            action = self._act(self.observation_new)\n",
    "            self.action_old = action\n",
    "            self.observation_old = self.observation_new\n",
    "            self.first_experience = False\n",
    "            self.total_step_counter += 1\n",
    "            return action\n",
    "        # Action, added exploration noise\n",
    "        action = self._act(self.observation_new)\n",
    "        \n",
    "        # *********************************** # \n",
    "        #        Remember Experiences         #\n",
    "        # *********************************** #\n",
    "        self.replay_buffer.add(self.observation_old, self.action_old, self.reward_new, self.done, self.observation_new)\n",
    "        \n",
    "        # *********************************** # \n",
    "        #             Train Models            #\n",
    "        # *********************************** #\n",
    "        self._train()\n",
    "        \n",
    "        # *********************************** # \n",
    "        #       Reset Temporary Variables     #\n",
    "        # *********************************** #\n",
    "        # Before return, set observation and action as old.\n",
    "        self.observation_old = self.observation_new\n",
    "        self.action_old = action\n",
    "        self.total_step_counter += 1\n",
    "        return action\n",
    "    \n",
    "    def _act(self, observation_new):\n",
    "        \"\"\"\n",
    "        Produce action based on current observation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation_new: np.shape(observation) = (obs_dim,)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        action: np.shape(action) = (act_dim, )\n",
    "        \"\"\"\n",
    "        # Epsilon-Greedy\n",
    "        if self.exploration_epsilon_greedy_type != 'none':\n",
    "            if np.random.rand(1) <= self.epsilon:\n",
    "                action = self.action_space.sample()\n",
    "                if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                if self.total_step_counter % 2000 == 0:\n",
    "                    print(\"epsilon:{}\".format(self.epsilon))\n",
    "                return action\n",
    "        # Action Noise\n",
    "        if self.exploration_action_noise_type != 'none':\n",
    "            action = self.extrinsic_actor_model.predict(np.reshape(observation_new, [1, self.observation_space.shape[0]])) + self.actor_noise() #The noise is too huge.\n",
    "        else:\n",
    "            action = self.extrinsic_actor_model.predict(np.reshape(observation_new, [1, self.observation_space.shape[0]]))\n",
    "        \n",
    "        return action[0]\n",
    "    \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Train Actor-Critic Model\n",
    "        \"\"\"\n",
    "        # Keep adding experience to the memory until\n",
    "        # there are at least minibatch size samples\n",
    "        # ************************************************** # \n",
    "        #  Train Extrinsically Motivated Actor-Critic Model  #\n",
    "        # ************************************************** #\n",
    "        if self.replay_buffer.size() > self.minibatch_size:\n",
    "            # Random Samples\n",
    "            s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                self.replay_buffer.sample_batch(int(self.minibatch_size))\n",
    "            # Prioritized Samples\n",
    "            \n",
    "            # Calculate targets\n",
    "            target_q = self.extrinsic_critic_model.predict_target(\n",
    "                s2_batch, self.extrinsic_actor_model.predict_target(s2_batch))\n",
    "\n",
    "            y_i = []\n",
    "            for k in range(int(self.minibatch_size)):\n",
    "                if t_batch[k]:\n",
    "                    y_i.append(r_batch[k])\n",
    "                else:\n",
    "                    y_i.append(r_batch[k] + self.extrinsic_critic_model.gamma * target_q[k])\n",
    "\n",
    "            # Update the critic given the targets\n",
    "            critic_loss, predicted_q_value, _ = self.extrinsic_critic_model.train(\\\n",
    "                                                                                  s_batch,\n",
    "                                                                                  a_batch,\n",
    "                                                                                  np.reshape(y_i, (int(self.minibatch_size), 1)))\n",
    "            \n",
    "            # Update the actor policy using the sampled gradient\n",
    "            a_outs = self.extrinsic_actor_model.predict(s_batch)\n",
    "            grads = self.extrinsic_critic_model.action_gradients(s_batch, a_outs)\n",
    "            self.extrinsic_actor_model.train(s_batch, grads[0])\n",
    "            \n",
    "            # Update target networks\n",
    "            self.extrinsic_actor_model.update_target_network()\n",
    "            self.extrinsic_critic_model.update_target_network()\n",
    "        \n",
    "    def _save_learned_model(self, version_number):\n",
    "        # Save extrinsically motivated actor-critic model \n",
    "        self.extrinsic_actor_model.save_actor_network(version_number)\n",
    "        self.extrinsic_critic_model.save_critic_network(version_number)\n",
    "        logging.info('Save extrinsic_actor_model and extrinsic_critic_model: done.')\n",
    "        \n",
    "# =================================================================== #\n",
    "#                    Initialization Helper Functions                  #\n",
    "# =================================================================== # \n",
    "    def _init_epsilon_greedy(self, exploration_epsilon_greedy_type):\n",
    "        \"\"\"\n",
    "        Initialize hyper-parameters for epsilon-greedy.\n",
    "        Parameters\n",
    "        ----------\n",
    "        exploration_epsilon_greedy_type: str default = 'epsilon-greedy-max_1_min_0.05_decay_0.999'\n",
    "            str for setting epsilon greedy. Please keep the format and just change float numbers.\n",
    "            For default 'epsilon-greedy-max_1_min_0.05_decay_0.999', it means:\n",
    "                maximum epsilon = 1\n",
    "                minimum spsilom = 0.05\n",
    "                epsilon decay = 0.999\n",
    "            If exploration_epsilon_greedy_type == 'none', no epsilon-greedy.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        epsilon_max: float\n",
    "            maximum epsilon\n",
    "        epsilon_min: float\n",
    "            minimum spsilom\n",
    "        epsilon_decay: float\n",
    "            epsilon decay\n",
    "        \"\"\"\n",
    "        if exploration_epsilon_greedy_type == 'none':\n",
    "            epsilon_max=0\n",
    "            epsilon_min=0\n",
    "            epsilon_decay=0\n",
    "        else:\n",
    "            _, epsilon_max, _, epsilon_min, _, epsilon_decay = exploration_epsilon_greedy_type.split('_')\n",
    "        \n",
    "        return float(epsilon_max), float(epsilon_min), float(epsilon_decay)\n",
    "    \n",
    "    def _init_action_noise(self, action_noise_type='ou_0.2', nb_actions=1):\n",
    "        \"\"\"\n",
    "        Initialize action noise object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action_noise_type: str default = 'ou_0.2'\n",
    "            type of action noise:\n",
    "                1. 'none' (no action noise)\n",
    "                2. 'adaptive-param_0.2'\n",
    "                3. 'normal_0.2'\n",
    "                4. 'ou_0.2'\n",
    "        nb_actions: int default = 1\n",
    "            dimension of action space\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "            action_noise: object of ActionNoise class.\n",
    "        \"\"\"\n",
    "        if action_noise_type == 'none':\n",
    "            pass\n",
    "        elif 'adaptive-param' in action_noise_type:\n",
    "            _, stddev = action_noise_type.split('_')\n",
    "            param_noise = AdaptiveParamNoiseSpec(initial_stddev=float(stddev), desired_action_stddev=float(stddev))\n",
    "            return param_noise\n",
    "        elif 'normal' in action_noise_type:\n",
    "            _, stddev = action_noise_type.split('_')\n",
    "            action_noise = NormalActionNoise(mu=np.zeros(nb_actions), sigma=float(stddev) * np.ones(nb_actions))\n",
    "            return action_noise\n",
    "        elif 'ou' in action_noise_type:\n",
    "            _, stddev = action_noise_type.split('_')\n",
    "            action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(nb_actions), sigma=float(stddev) * np.ones(nb_actions))\n",
    "            return action_noise\n",
    "        else:\n",
    "            raise RuntimeError('unknown noise type \"{}\"'.format(action_noise_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = 'CartPole_v0'\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "observation_space_name = [], \n",
    "action_space_name = []\n",
    "x_order_MDP = 1\n",
    "x_order_MDP_observation_type = 'concatenate_observation'\n",
    "occupancy_reward_type = 'IR_distance'\n",
    "interaction_mode = 'virtual_interaction'\n",
    "load_pretrained_agent_flag = False\n",
    "\n",
    "\n",
    "# Model saving directory\n",
    "model_version_number = 0\n",
    "agent_model_save_dir = os.path.join(os.path.abspath('..'),'Experiment_results',agent_name)\n",
    "tf_session = tf.Session()\n",
    "agent = Agent_Actor_Critic(tf_session,\n",
    "                                agent_name,\n",
    "                                observation_space,\n",
    "                                action_space,\n",
    "                                actor_lr = 0.0001, actor_tau = 0.001,\n",
    "                                critic_lr = 0.0001, critic_tau = 0.001, gamma = 0.99,\n",
    "                                minibatch_size = 64,\n",
    "                                max_episodes = 5000, max_episode_len = 1000,\n",
    "                                # Exploration Strategies\n",
    "                                exploration_action_noise_type = 'none', #'ou_0.2',\n",
    "                                exploration_epsilon_greedy_type = 'epsilon-greedy-max_1_min_0.05_decay_0.9',#'none',#\n",
    "                                # Save Summaries\n",
    "                                save_dir = agent_model_save_dir,\n",
    "                                experiment_runs = datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                # Save and Restore Actor-Critic Model\n",
    "                                restore_actor_model_flag = False,\n",
    "                                restore_critic_model_flag = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Cumulative Reward: -1042.8403127037682\n",
      "Episode: 1, Cumulative Reward: -1277.8464286522083\n",
      "Episode: 2, Cumulative Reward: -1367.0324864114855\n",
      "Episode: 3, Cumulative Reward: -1868.760567071227\n",
      "Episode: 4, Cumulative Reward: -1902.745356392851\n",
      "Episode: 5, Cumulative Reward: -1254.8984344352957\n",
      "Episode: 6, Cumulative Reward: -1390.9585549349958\n",
      "Episode: 7, Cumulative Reward: -1196.269055041974\n",
      "Episode: 8, Cumulative Reward: -1326.800975617984\n",
      "Episode: 9, Cumulative Reward: -1360.4845283948587\n",
      "Episode: 10, Cumulative Reward: -1320.715030337274\n",
      "Episode: 11, Cumulative Reward: -1171.0709221235138\n",
      "Episode: 12, Cumulative Reward: -1315.4513261861832\n",
      "Episode: 13, Cumulative Reward: -1335.1522629062752\n",
      "Episode: 14, Cumulative Reward: -1294.9235279090726\n",
      "Episode: 15, Cumulative Reward: -1222.2351275454396\n",
      "Episode: 16, Cumulative Reward: -1242.1677952277225\n",
      "Episode: 17, Cumulative Reward: -1187.7195944721827\n",
      "Episode: 18, Cumulative Reward: -1240.2543798217432\n",
      "Episode: 19, Cumulative Reward: -1229.6101492594348\n",
      "Episode: 20, Cumulative Reward: -989.2410335159659\n",
      "Episode: 21, Cumulative Reward: -1446.0480211928905\n",
      "Episode: 22, Cumulative Reward: -1228.267331806603\n",
      "Episode: 23, Cumulative Reward: -982.7825750659347\n",
      "Episode: 24, Cumulative Reward: -1435.5569252743362\n",
      "Episode: 25, Cumulative Reward: -1064.6633547025372\n",
      "Episode: 26, Cumulative Reward: -1243.2919841450266\n",
      "Episode: 27, Cumulative Reward: -1207.4233424322508\n",
      "Episode: 28, Cumulative Reward: -1191.4318034115793\n",
      "Episode: 29, Cumulative Reward: -1244.8895279060932\n",
      "Episode: 30, Cumulative Reward: -1191.5975064857346\n",
      "Episode: 31, Cumulative Reward: -1486.389434123805\n",
      "Episode: 32, Cumulative Reward: -1097.0717851772447\n",
      "Episode: 33, Cumulative Reward: -1327.4302546154815\n",
      "Episode: 34, Cumulative Reward: -1118.9330643526944\n",
      "Episode: 35, Cumulative Reward: -1197.3412677477902\n",
      "Episode: 36, Cumulative Reward: -1181.1708280009111\n",
      "Episode: 37, Cumulative Reward: -1124.1357473170178\n",
      "Episode: 38, Cumulative Reward: -1102.2860913305765\n",
      "Episode: 39, Cumulative Reward: -1130.0663448653288\n",
      "Episode: 40, Cumulative Reward: -1226.9893737264256\n",
      "Episode: 41, Cumulative Reward: -1205.5976486467202\n",
      "Episode: 42, Cumulative Reward: -1002.6460203024525\n",
      "Episode: 43, Cumulative Reward: -1162.726385295518\n",
      "Episode: 44, Cumulative Reward: -1105.6175690538382\n",
      "Episode: 45, Cumulative Reward: -1102.5253003299786\n",
      "Episode: 46, Cumulative Reward: -1170.8962137716928\n",
      "Episode: 47, Cumulative Reward: -1110.4047029599922\n",
      "Episode: 48, Cumulative Reward: -1215.6099966841762\n",
      "Episode: 49, Cumulative Reward: -1127.2731387844765\n",
      "Episode: 50, Cumulative Reward: -1068.461031140955\n",
      "Episode: 51, Cumulative Reward: -964.9376606113585\n",
      "Episode: 52, Cumulative Reward: -1213.400194023723\n",
      "Episode: 53, Cumulative Reward: -1000.8784248981091\n",
      "Episode: 54, Cumulative Reward: -981.4240969362934\n",
      "Episode: 55, Cumulative Reward: -499.5248456286627\n",
      "Episode: 56, Cumulative Reward: -872.9517094878482\n",
      "Episode: 57, Cumulative Reward: -981.7945262676888\n",
      "Episode: 58, Cumulative Reward: -1185.8024801834931\n",
      "Episode: 59, Cumulative Reward: -558.7421042552074\n",
      "Episode: 60, Cumulative Reward: -982.0957668056354\n",
      "Episode: 61, Cumulative Reward: -973.3096939950528\n",
      "Episode: 62, Cumulative Reward: -870.6500446792802\n",
      "Episode: 63, Cumulative Reward: -1003.0130984718553\n",
      "Episode: 64, Cumulative Reward: -623.7096625462759\n",
      "Episode: 65, Cumulative Reward: -992.5140062412711\n",
      "Episode: 66, Cumulative Reward: -250.85416908169876\n",
      "Episode: 67, Cumulative Reward: -1217.9420841296553\n",
      "Episode: 68, Cumulative Reward: -736.8366026050134\n",
      "Episode: 69, Cumulative Reward: -630.437808556075\n",
      "Episode: 70, Cumulative Reward: -961.4940488480095\n",
      "Episode: 71, Cumulative Reward: -636.7104185090757\n",
      "Episode: 72, Cumulative Reward: -857.380754122166\n",
      "Episode: 73, Cumulative Reward: -863.5432239758971\n",
      "Episode: 74, Cumulative Reward: -894.7273958687824\n",
      "Episode: 75, Cumulative Reward: -745.3751261518357\n",
      "Episode: 76, Cumulative Reward: -718.7077325641168\n",
      "Episode: 77, Cumulative Reward: -373.2414665566432\n",
      "Episode: 78, Cumulative Reward: -373.51807100425646\n",
      "Episode: 79, Cumulative Reward: -1.0022509113734384\n",
      "Episode: 80, Cumulative Reward: -746.994218380082\n",
      "Episode: 81, Cumulative Reward: -271.84280994867424\n",
      "Episode: 82, Cumulative Reward: -870.7087019817135\n",
      "Episode: 83, Cumulative Reward: -1012.652961757102\n",
      "Episode: 84, Cumulative Reward: -506.834160209268\n",
      "Episode: 85, Cumulative Reward: -1016.4656583025727\n",
      "Episode: 86, Cumulative Reward: -627.5059876617099\n",
      "Episode: 87, Cumulative Reward: -995.0548954994379\n",
      "Episode: 88, Cumulative Reward: -913.5080227083041\n",
      "Episode: 89, Cumulative Reward: -759.5132816125698\n",
      "Episode: 90, Cumulative Reward: -747.7040715771163\n",
      "Episode: 91, Cumulative Reward: -749.0733074460616\n",
      "Episode: 92, Cumulative Reward: -874.1420212262864\n",
      "Episode: 93, Cumulative Reward: -597.1724349034816\n",
      "Episode: 94, Cumulative Reward: -879.0564407584876\n",
      "Episode: 95, Cumulative Reward: -844.3391322338558\n",
      "Episode: 96, Cumulative Reward: -713.9043373835373\n",
      "Episode: 97, Cumulative Reward: -983.4429902501639\n",
      "Episode: 98, Cumulative Reward: -745.7794349849021\n",
      "Episode: 99, Cumulative Reward: -765.2167699826305\n",
      "Episode: 100, Cumulative Reward: -745.7198628155124\n",
      "Episode: 101, Cumulative Reward: -749.3447875419392\n",
      "Episode: 102, Cumulative Reward: -628.364678783126\n",
      "Episode: 103, Cumulative Reward: -749.0101519658327\n",
      "Episode: 104, Cumulative Reward: -756.5816335578554\n",
      "Episode: 105, Cumulative Reward: -757.2700070611604\n",
      "Episode: 106, Cumulative Reward: -631.2441027301873\n",
      "Episode: 107, Cumulative Reward: -689.3630561709542\n",
      "Episode: 108, Cumulative Reward: -819.6117913790285\n",
      "Episode: 109, Cumulative Reward: -862.8062259469165\n",
      "Episode: 110, Cumulative Reward: -623.7510255441443\n",
      "Episode: 111, Cumulative Reward: -913.5342858645031\n",
      "Episode: 112, Cumulative Reward: -621.2450722435507\n",
      "Episode: 113, Cumulative Reward: -909.6983088040568\n",
      "Episode: 114, Cumulative Reward: -751.9116749653984\n",
      "Episode: 115, Cumulative Reward: -626.4408488499648\n",
      "Episode: 116, Cumulative Reward: -630.0671526119817\n",
      "Episode: 117, Cumulative Reward: -632.7045084409262\n",
      "Episode: 118, Cumulative Reward: -626.5857766507418\n",
      "Episode: 119, Cumulative Reward: -871.4871274599363\n",
      "Episode: 120, Cumulative Reward: -753.7826880239181\n",
      "Episode: 121, Cumulative Reward: -876.2499082282532\n",
      "Episode: 122, Cumulative Reward: -795.3069949284572\n",
      "Episode: 123, Cumulative Reward: -736.7154208154469\n",
      "Episode: 124, Cumulative Reward: -640.6880112169084\n",
      "Episode: 125, Cumulative Reward: -645.9940580900941\n",
      "Episode: 126, Cumulative Reward: -864.3122594022119\n",
      "Episode: 127, Cumulative Reward: -634.3400305427056\n",
      "Episode: 128, Cumulative Reward: -620.5269598309073\n",
      "Episode: 129, Cumulative Reward: -505.8215328104181\n",
      "Episode: 130, Cumulative Reward: -759.5842142285725\n",
      "Episode: 131, Cumulative Reward: -624.233722248926\n",
      "Episode: 132, Cumulative Reward: -757.2879637131309\n",
      "Episode: 133, Cumulative Reward: -537.6670430379202\n",
      "Episode: 134, Cumulative Reward: -717.7688799455979\n",
      "Episode: 135, Cumulative Reward: -742.0378805733318\n",
      "Episode: 136, Cumulative Reward: -756.7181460098084\n",
      "Episode: 137, Cumulative Reward: -929.1531203752564\n",
      "Episode: 138, Cumulative Reward: -641.0357783797813\n",
      "Episode: 139, Cumulative Reward: -631.1029476116422\n",
      "Episode: 140, Cumulative Reward: -993.2370435279788\n",
      "Episode: 141, Cumulative Reward: -795.5033263143349\n",
      "Episode: 142, Cumulative Reward: -844.572807535617\n",
      "Episode: 143, Cumulative Reward: -677.0369298638881\n",
      "Episode: 144, Cumulative Reward: -894.1596108325343\n",
      "Episode: 145, Cumulative Reward: -757.5759174560409\n",
      "Episode: 146, Cumulative Reward: -869.2921304106036\n",
      "Episode: 147, Cumulative Reward: -684.8365333801926\n",
      "Episode: 148, Cumulative Reward: -755.8797736626873\n",
      "Episode: 149, Cumulative Reward: -769.6487600154557\n",
      "Episode: 150, Cumulative Reward: -869.9563046794977\n",
      "Episode: 151, Cumulative Reward: -867.2564626636624\n",
      "Episode: 152, Cumulative Reward: -717.9287797512691\n",
      "Episode: 153, Cumulative Reward: -774.6172565827682\n",
      "Episode: 154, Cumulative Reward: -756.5779331418463\n",
      "Episode: 155, Cumulative Reward: -647.9818722822517\n",
      "Episode: 156, Cumulative Reward: -744.7925836949988\n",
      "Episode: 157, Cumulative Reward: -785.3358258802458\n",
      "Episode: 158, Cumulative Reward: -856.4145662180066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 159, Cumulative Reward: -378.45155386000283\n",
      "Episode: 160, Cumulative Reward: -1002.5307696436313\n",
      "Episode: 161, Cumulative Reward: -628.8096606412342\n",
      "Episode: 162, Cumulative Reward: -759.2099014865919\n",
      "Episode: 163, Cumulative Reward: -882.3612787591578\n",
      "Episode: 164, Cumulative Reward: -1022.8283341712487\n",
      "Episode: 165, Cumulative Reward: -507.32480329148194\n",
      "Episode: 166, Cumulative Reward: -783.1620164197386\n",
      "Episode: 167, Cumulative Reward: -621.3016322725445\n",
      "Episode: 168, Cumulative Reward: -698.8533560182775\n",
      "Episode: 169, Cumulative Reward: -861.2189177773024\n",
      "Episode: 170, Cumulative Reward: -769.0064858962597\n",
      "Episode: 171, Cumulative Reward: -770.6520268963274\n",
      "Episode: 172, Cumulative Reward: -732.8582895950838\n",
      "Episode: 173, Cumulative Reward: -267.34115632057296\n",
      "Episode: 174, Cumulative Reward: -743.4238034682569\n",
      "Episode: 175, Cumulative Reward: -505.0848276820102\n",
      "Episode: 176, Cumulative Reward: -831.0266690431075\n",
      "Episode: 177, Cumulative Reward: -233.65713615920137\n",
      "Episode: 178, Cumulative Reward: -607.0839189448956\n",
      "Episode: 179, Cumulative Reward: -750.5706794385725\n",
      "Episode: 180, Cumulative Reward: -649.4165364721906\n",
      "Episode: 181, Cumulative Reward: -812.3897435367511\n",
      "Episode: 182, Cumulative Reward: -356.54891599294615\n",
      "Episode: 183, Cumulative Reward: -992.3167234139664\n",
      "Episode: 184, Cumulative Reward: -873.4792120005839\n",
      "Episode: 185, Cumulative Reward: -933.6379757607099\n",
      "Episode: 186, Cumulative Reward: -870.5818372589694\n",
      "Episode: 187, Cumulative Reward: -0.814324108974131\n",
      "Episode: 188, Cumulative Reward: -877.7622368911301\n",
      "Episode: 189, Cumulative Reward: -499.50802038277277\n",
      "Episode: 190, Cumulative Reward: -382.1017608794425\n",
      "Episode: 191, Cumulative Reward: -755.2501849475151\n",
      "Episode: 192, Cumulative Reward: -768.9471622809981\n",
      "Episode: 193, Cumulative Reward: -252.85073095755945\n",
      "Episode: 194, Cumulative Reward: -121.93924117860665\n",
      "Episode: 195, Cumulative Reward: -0.7575773723832625\n",
      "Episode: 196, Cumulative Reward: -501.7538027601265\n",
      "Episode: 197, Cumulative Reward: -504.3593807455389\n",
      "Episode: 198, Cumulative Reward: -355.24892348506467\n",
      "Episode: 199, Cumulative Reward: -244.26261958082247\n",
      "Episode: 200, Cumulative Reward: -124.30718018934584\n",
      "Episode: 201, Cumulative Reward: -247.15674337649625\n",
      "Episode: 202, Cumulative Reward: -366.10103082543566\n",
      "Episode: 203, Cumulative Reward: -0.3117395804081426\n",
      "Episode: 204, Cumulative Reward: -246.61159236423515\n",
      "Episode: 205, Cumulative Reward: -126.46118789902752\n",
      "Episode: 206, Cumulative Reward: -125.13341638873295\n",
      "Episode: 207, Cumulative Reward: -366.6510332002282\n",
      "Episode: 208, Cumulative Reward: -124.23439247445755\n",
      "Episode: 209, Cumulative Reward: -125.89563742876066\n",
      "Episode: 210, Cumulative Reward: -371.17519945044813\n",
      "Episode: 211, Cumulative Reward: -726.7463880630792\n",
      "Episode: 212, Cumulative Reward: -359.17559900485526\n",
      "Episode: 213, Cumulative Reward: -125.24769374010563\n",
      "Episode: 214, Cumulative Reward: -359.9756759143912\n",
      "Episode: 215, Cumulative Reward: -845.2742625982618\n",
      "Episode: 216, Cumulative Reward: -356.8024262633727\n",
      "Episode: 217, Cumulative Reward: -0.6713519607830837\n",
      "Episode: 218, Cumulative Reward: -124.73004032183333\n",
      "Episode: 219, Cumulative Reward: -364.918986183386\n",
      "Episode: 220, Cumulative Reward: -124.8377479364374\n",
      "Episode: 221, Cumulative Reward: -122.78939036731609\n",
      "Episode: 222, Cumulative Reward: -123.01917630548256\n",
      "Episode: 223, Cumulative Reward: -503.94908095825457\n",
      "Episode: 224, Cumulative Reward: -233.7058462284763\n",
      "Episode: 225, Cumulative Reward: -123.09171789394489\n",
      "Episode: 226, Cumulative Reward: -124.68245651704592\n",
      "Episode: 227, Cumulative Reward: -240.08145031364984\n",
      "Episode: 228, Cumulative Reward: -122.83416565889124\n",
      "Episode: 229, Cumulative Reward: -119.51930200303079\n",
      "Episode: 230, Cumulative Reward: -120.68236376949507\n",
      "Episode: 231, Cumulative Reward: -244.97064542112338\n",
      "Episode: 232, Cumulative Reward: -125.54788662968348\n",
      "Episode: 233, Cumulative Reward: -119.91864681211207\n",
      "Episode: 234, Cumulative Reward: -238.62456078570784\n",
      "Episode: 235, Cumulative Reward: -547.6264695605623\n",
      "Episode: 236, Cumulative Reward: -246.67127768617226\n",
      "Episode: 237, Cumulative Reward: -359.46559722887514\n",
      "Episode: 238, Cumulative Reward: -0.20275280528740472\n",
      "Episode: 239, Cumulative Reward: -600.9039897979272\n",
      "Episode: 240, Cumulative Reward: -244.3139128269416\n",
      "Episode: 241, Cumulative Reward: -125.45950393021126\n",
      "Episode: 242, Cumulative Reward: -125.71617920601793\n",
      "Episode: 243, Cumulative Reward: -477.5506656036048\n",
      "Episode: 244, Cumulative Reward: -362.5252264007493\n",
      "Episode: 245, Cumulative Reward: -591.0046621071925\n",
      "Episode: 246, Cumulative Reward: -590.604582922733\n",
      "Episode: 247, Cumulative Reward: -765.8940022115887\n",
      "Episode: 248, Cumulative Reward: -0.28494807139045775\n",
      "Episode: 249, Cumulative Reward: -363.2127930584503\n",
      "Episode: 250, Cumulative Reward: -251.18288353870622\n",
      "Episode: 251, Cumulative Reward: -123.0568942969814\n",
      "Episode: 252, Cumulative Reward: -361.6240816633466\n",
      "Episode: 253, Cumulative Reward: -121.80721024793726\n",
      "Episode: 254, Cumulative Reward: -0.6792722324238634\n",
      "Episode: 255, Cumulative Reward: -125.05327801969494\n",
      "Episode: 256, Cumulative Reward: -243.63973562294757\n",
      "Episode: 257, Cumulative Reward: -365.73253010745503\n",
      "Episode: 258, Cumulative Reward: -361.20632129994857\n",
      "Episode: 259, Cumulative Reward: -118.81755250486836\n",
      "Episode: 260, Cumulative Reward: -125.93684063432144\n",
      "Episode: 261, Cumulative Reward: -250.30084169611865\n",
      "Episode: 262, Cumulative Reward: -513.84159225486\n",
      "Episode: 263, Cumulative Reward: -364.370797426522\n",
      "Episode: 264, Cumulative Reward: -644.9252203536487\n",
      "Episode: 265, Cumulative Reward: -238.7754854292743\n",
      "Episode: 266, Cumulative Reward: -123.44768328958952\n",
      "Episode: 267, Cumulative Reward: -122.03179270966928\n",
      "Episode: 268, Cumulative Reward: -123.60124081971146\n",
      "Episode: 269, Cumulative Reward: -495.7543371340177\n",
      "Episode: 270, Cumulative Reward: -238.62498668714096\n",
      "Episode: 271, Cumulative Reward: -579.8960153355663\n",
      "Episode: 272, Cumulative Reward: -125.37878375836965\n",
      "Episode: 273, Cumulative Reward: -238.6708413289919\n",
      "Episode: 274, Cumulative Reward: -120.45537519001455\n",
      "Episode: 275, Cumulative Reward: -598.2906611178379\n",
      "Episode: 276, Cumulative Reward: -125.32073733115395\n",
      "Episode: 277, Cumulative Reward: -251.3115342912347\n",
      "Episode: 278, Cumulative Reward: -245.0313518269199\n",
      "Episode: 279, Cumulative Reward: -238.672009186054\n",
      "Episode: 280, Cumulative Reward: -505.0944269158625\n",
      "Episode: 281, Cumulative Reward: -365.8049873970663\n",
      "Episode: 282, Cumulative Reward: -375.4389741650682\n",
      "Episode: 283, Cumulative Reward: -122.22226522413783\n",
      "Episode: 284, Cumulative Reward: -120.47524912782613\n",
      "Episode: 285, Cumulative Reward: -359.83013419861794\n",
      "Episode: 286, Cumulative Reward: -247.86174412069573\n",
      "Episode: 287, Cumulative Reward: -121.1593149900015\n",
      "Episode: 288, Cumulative Reward: -470.9553742119069\n",
      "Episode: 289, Cumulative Reward: -120.09387203447137\n",
      "Episode: 290, Cumulative Reward: -356.26515300250185\n",
      "Episode: 291, Cumulative Reward: -476.18171988255415\n",
      "Episode: 292, Cumulative Reward: -477.44991915973554\n",
      "Episode: 293, Cumulative Reward: -124.81169348113973\n",
      "Episode: 294, Cumulative Reward: -477.142674341409\n",
      "Episode: 295, Cumulative Reward: -496.6380924701441\n",
      "Episode: 296, Cumulative Reward: -118.61371436331727\n",
      "Episode: 297, Cumulative Reward: -124.14761822070977\n",
      "Episode: 298, Cumulative Reward: -126.16829834553148\n",
      "Episode: 299, Cumulative Reward: -1.3359722639954805\n",
      "Episode: 300, Cumulative Reward: -124.86339385092442\n",
      "Episode: 301, Cumulative Reward: -637.7110684275923\n",
      "Episode: 302, Cumulative Reward: -125.28027708016222\n",
      "Episode: 303, Cumulative Reward: -550.0901919803805\n",
      "Episode: 304, Cumulative Reward: -251.18841221074027\n",
      "Episode: 305, Cumulative Reward: -248.87337687305646\n",
      "Episode: 306, Cumulative Reward: -356.613642709686\n",
      "Episode: 307, Cumulative Reward: -359.51232699534296\n",
      "Episode: 308, Cumulative Reward: -124.32697258172259\n",
      "Episode: 309, Cumulative Reward: -122.30993683867965\n",
      "Episode: 310, Cumulative Reward: -122.95234263512296\n",
      "Episode: 311, Cumulative Reward: -354.2862455840351\n",
      "Episode: 312, Cumulative Reward: -236.6363145628584\n",
      "Episode: 313, Cumulative Reward: -121.69607559970783\n",
      "Episode: 314, Cumulative Reward: -123.05729171014978\n",
      "Episode: 315, Cumulative Reward: -367.12013446872874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 316, Cumulative Reward: -120.8905131682626\n",
      "Episode: 317, Cumulative Reward: -241.65193288607398\n",
      "Episode: 318, Cumulative Reward: -358.29725412132063\n",
      "Episode: 319, Cumulative Reward: -122.73069321248693\n",
      "Episode: 320, Cumulative Reward: -484.3754658647334\n",
      "Episode: 321, Cumulative Reward: -241.87669493187374\n",
      "Episode: 322, Cumulative Reward: -125.43101694281246\n",
      "Episode: 323, Cumulative Reward: -0.9253132700298894\n",
      "Episode: 324, Cumulative Reward: -124.84199799063038\n",
      "Episode: 325, Cumulative Reward: -232.1816011193139\n",
      "Episode: 326, Cumulative Reward: -122.63726123482013\n",
      "Episode: 327, Cumulative Reward: -123.54839555794976\n",
      "Episode: 328, Cumulative Reward: -122.98443074078382\n",
      "Episode: 329, Cumulative Reward: -120.5967404703593\n",
      "Episode: 330, Cumulative Reward: -122.6878092164808\n",
      "Episode: 331, Cumulative Reward: -601.674648396079\n",
      "Episode: 332, Cumulative Reward: -123.92160397937346\n",
      "Episode: 333, Cumulative Reward: -595.8196791189757\n",
      "Episode: 334, Cumulative Reward: -0.24709486777092657\n",
      "Episode: 335, Cumulative Reward: -121.68967644463223\n",
      "Episode: 336, Cumulative Reward: -363.72094900313857\n",
      "Episode: 337, Cumulative Reward: -123.32590183316927\n",
      "Episode: 338, Cumulative Reward: -240.833304884943\n",
      "Episode: 339, Cumulative Reward: -238.9799208779002\n",
      "Episode: 340, Cumulative Reward: -241.82483463896693\n",
      "Episode: 341, Cumulative Reward: -124.99262417750124\n",
      "Episode: 342, Cumulative Reward: -0.52379306496933\n",
      "Episode: 343, Cumulative Reward: -252.33040264644575\n",
      "Episode: 344, Cumulative Reward: -124.05439369807922\n",
      "Episode: 345, Cumulative Reward: -486.0807376775873\n",
      "Episode: 346, Cumulative Reward: -124.89196364381111\n",
      "Episode: 347, Cumulative Reward: -125.67011291804761\n",
      "Episode: 348, Cumulative Reward: -126.13060459267517\n",
      "Episode: 349, Cumulative Reward: -124.79063691950746\n",
      "Episode: 350, Cumulative Reward: -507.34313437085234\n",
      "Episode: 351, Cumulative Reward: -239.55260227165112\n",
      "Episode: 352, Cumulative Reward: -122.12175560404287\n",
      "Episode: 353, Cumulative Reward: -0.2851501763204955\n",
      "Episode: 354, Cumulative Reward: -548.0016363825698\n",
      "Episode: 355, Cumulative Reward: -497.6823860144197\n",
      "Episode: 356, Cumulative Reward: -123.90908140038559\n",
      "Episode: 357, Cumulative Reward: -123.56572942719885\n",
      "Episode: 358, Cumulative Reward: -119.05657640481625\n",
      "Episode: 359, Cumulative Reward: -0.21357789996285206\n",
      "Episode: 360, Cumulative Reward: -244.7463020475327\n",
      "Episode: 361, Cumulative Reward: -242.5542265180914\n",
      "Episode: 362, Cumulative Reward: -121.96648553580366\n",
      "Episode: 363, Cumulative Reward: -0.27108865535301113\n",
      "Episode: 364, Cumulative Reward: -363.57888425611736\n",
      "Episode: 365, Cumulative Reward: -241.42488509455873\n",
      "Episode: 366, Cumulative Reward: -125.215140203042\n",
      "Episode: 367, Cumulative Reward: -493.5635099854399\n",
      "Episode: 368, Cumulative Reward: -607.7717847279399\n",
      "Episode: 369, Cumulative Reward: -241.4663808559989\n",
      "Episode: 370, Cumulative Reward: -126.55630935268378\n",
      "Episode: 371, Cumulative Reward: -121.55562203425065\n",
      "Episode: 372, Cumulative Reward: -125.55637122809932\n",
      "Episode: 373, Cumulative Reward: -491.57009885598944\n",
      "Episode: 374, Cumulative Reward: -124.07633965324843\n",
      "Episode: 375, Cumulative Reward: -480.47942032477874\n",
      "Episode: 376, Cumulative Reward: -614.3566139362346\n",
      "Episode: 377, Cumulative Reward: -121.5588036153414\n",
      "Episode: 378, Cumulative Reward: -486.97137740237645\n",
      "Episode: 379, Cumulative Reward: -239.3091095536151\n",
      "Episode: 380, Cumulative Reward: -123.57698218782943\n",
      "Episode: 381, Cumulative Reward: -355.7239712291009\n",
      "Episode: 382, Cumulative Reward: -118.40826850474035\n",
      "Episode: 383, Cumulative Reward: -124.77502414257133\n",
      "Episode: 384, Cumulative Reward: -126.2848593731039\n",
      "Episode: 385, Cumulative Reward: -120.68080127323911\n",
      "Episode: 386, Cumulative Reward: -365.08237922505253\n",
      "Episode: 387, Cumulative Reward: -119.67672650540833\n",
      "Episode: 388, Cumulative Reward: -125.6092931743077\n",
      "epsilon:0.04710128697246249\n",
      "Episode: 389, Cumulative Reward: -122.94119761208094\n",
      "Episode: 390, Cumulative Reward: -124.30785446812207\n",
      "Episode: 391, Cumulative Reward: -356.922144839584\n",
      "Episode: 392, Cumulative Reward: -626.4780445034238\n",
      "Episode: 393, Cumulative Reward: -120.7977084358927\n",
      "Episode: 394, Cumulative Reward: -0.1951820195814799\n",
      "Episode: 395, Cumulative Reward: -121.46096307066637\n",
      "Episode: 396, Cumulative Reward: -125.43971402101373\n",
      "Episode: 397, Cumulative Reward: -0.5244095666520192\n",
      "Episode: 398, Cumulative Reward: -243.02829862575757\n",
      "Episode: 399, Cumulative Reward: -124.26736882382028\n",
      "Episode: 400, Cumulative Reward: -361.71426507059334\n",
      "Episode: 401, Cumulative Reward: -619.3687049152048\n",
      "Episode: 402, Cumulative Reward: -124.3120517928571\n",
      "Episode: 403, Cumulative Reward: -473.97332303561564\n",
      "Episode: 404, Cumulative Reward: -125.44103456774458\n",
      "Episode: 405, Cumulative Reward: -119.36200291116278\n",
      "Episode: 406, Cumulative Reward: -475.9353394282943\n",
      "Episode: 407, Cumulative Reward: -242.23986582731644\n",
      "Episode: 408, Cumulative Reward: -474.78442063403907\n",
      "Episode: 409, Cumulative Reward: -495.86831209907143\n",
      "Episode: 410, Cumulative Reward: -510.17619476336006\n",
      "Episode: 411, Cumulative Reward: -359.1486222167717\n",
      "Episode: 412, Cumulative Reward: -124.83091537477188\n",
      "Episode: 413, Cumulative Reward: -125.08117940612753\n",
      "Episode: 414, Cumulative Reward: -560.791278485297\n",
      "Episode: 415, Cumulative Reward: -242.09025322846998\n",
      "Episode: 416, Cumulative Reward: -125.25422995093344\n",
      "Episode: 417, Cumulative Reward: -624.1724886940058\n",
      "Episode: 418, Cumulative Reward: -122.29017662381855\n",
      "Episode: 419, Cumulative Reward: -360.4548232194441\n",
      "Episode: 420, Cumulative Reward: -644.5821513791215\n",
      "Episode: 421, Cumulative Reward: -243.41704707874334\n",
      "Episode: 422, Cumulative Reward: -472.2176113689669\n",
      "Episode: 423, Cumulative Reward: -367.1807147910182\n",
      "Episode: 424, Cumulative Reward: -489.84436225096823\n",
      "Episode: 425, Cumulative Reward: -121.49508730036642\n",
      "Episode: 426, Cumulative Reward: -244.99099756901452\n",
      "Episode: 427, Cumulative Reward: -120.56987981802459\n",
      "Episode: 428, Cumulative Reward: -241.84276730618913\n",
      "Episode: 429, Cumulative Reward: -582.7457405672219\n",
      "Episode: 430, Cumulative Reward: -358.87277009254245\n",
      "Episode: 431, Cumulative Reward: -125.6600275083417\n",
      "Episode: 432, Cumulative Reward: -123.72614543375579\n",
      "Episode: 433, Cumulative Reward: -627.4887695737723\n",
      "Episode: 434, Cumulative Reward: -122.70201261863973\n",
      "Episode: 435, Cumulative Reward: -630.3571741137415\n",
      "Episode: 436, Cumulative Reward: -124.9750979517889\n",
      "Episode: 437, Cumulative Reward: -597.0694622647827\n",
      "Episode: 438, Cumulative Reward: -122.43490736545789\n",
      "Episode: 439, Cumulative Reward: -121.74055917692948\n",
      "Episode: 440, Cumulative Reward: -125.30185367947769\n",
      "Episode: 441, Cumulative Reward: -0.7317700907294813\n",
      "Episode: 442, Cumulative Reward: -123.46470232354892\n",
      "Episode: 443, Cumulative Reward: -243.5328514111782\n",
      "Episode: 444, Cumulative Reward: -360.58934940704137\n",
      "Episode: 445, Cumulative Reward: -352.06405576118937\n",
      "Episode: 446, Cumulative Reward: -238.54166989424635\n",
      "Episode: 447, Cumulative Reward: -120.9673824056484\n",
      "Episode: 448, Cumulative Reward: -123.64268497816713\n",
      "epsilon:0.04710128697246249\n",
      "Episode: 449, Cumulative Reward: -350.5779131863303\n",
      "Episode: 450, Cumulative Reward: -362.8257362708901\n",
      "Episode: 451, Cumulative Reward: -707.2649307759042\n",
      "Episode: 452, Cumulative Reward: -232.54581595632246\n",
      "Episode: 453, Cumulative Reward: -121.06537389657778\n",
      "Episode: 454, Cumulative Reward: -125.38378339236219\n",
      "Episode: 455, Cumulative Reward: -123.72735042218939\n",
      "Episode: 456, Cumulative Reward: -366.61716474848987\n",
      "Episode: 457, Cumulative Reward: -120.17768012072794\n",
      "Episode: 458, Cumulative Reward: -650.363755932962\n",
      "Episode: 459, Cumulative Reward: -124.28199908079281\n",
      "Episode: 460, Cumulative Reward: -578.3671774653136\n",
      "Episode: 461, Cumulative Reward: -483.57529313453074\n",
      "Episode: 462, Cumulative Reward: -124.14608576622535\n",
      "Episode: 463, Cumulative Reward: -234.77687748483456\n",
      "Episode: 464, Cumulative Reward: -369.8625218606858\n",
      "Episode: 465, Cumulative Reward: -125.88361624572258\n",
      "Episode: 466, Cumulative Reward: -124.37983936558307\n",
      "Episode: 467, Cumulative Reward: -124.89427651436546\n",
      "Episode: 468, Cumulative Reward: -124.67925031994527\n",
      "Episode: 469, Cumulative Reward: -506.5916013412479\n",
      "Episode: 470, Cumulative Reward: -480.7657958218997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 471, Cumulative Reward: -120.63901926755362\n",
      "Episode: 472, Cumulative Reward: -242.1816269330705\n",
      "Episode: 473, Cumulative Reward: -478.7856290913613\n",
      "Episode: 474, Cumulative Reward: -243.151512233741\n",
      "Episode: 475, Cumulative Reward: -592.4110505227856\n",
      "Episode: 476, Cumulative Reward: -363.7354833712076\n",
      "Episode: 477, Cumulative Reward: -588.0250893383599\n",
      "Episode: 478, Cumulative Reward: -120.73245531335121\n",
      "Episode: 479, Cumulative Reward: -124.17520177203627\n",
      "Episode: 480, Cumulative Reward: -1.6212301209653057\n",
      "Episode: 481, Cumulative Reward: -232.7544320525982\n",
      "Episode: 482, Cumulative Reward: -122.93534192450639\n",
      "Episode: 483, Cumulative Reward: -241.8320039671489\n",
      "Episode: 484, Cumulative Reward: -243.57384242637104\n",
      "Episode: 485, Cumulative Reward: -241.7445925967384\n",
      "Episode: 486, Cumulative Reward: -126.37430898974412\n",
      "Episode: 487, Cumulative Reward: -373.3146344454824\n",
      "Episode: 488, Cumulative Reward: -121.36586732931195\n",
      "Episode: 489, Cumulative Reward: -122.64977010990317\n",
      "Episode: 490, Cumulative Reward: -121.84483560463649\n",
      "Episode: 491, Cumulative Reward: -123.46210973897796\n",
      "Episode: 492, Cumulative Reward: -635.8903508236281\n",
      "Episode: 493, Cumulative Reward: -124.9276309231048\n",
      "Episode: 494, Cumulative Reward: -244.8215914438621\n",
      "Episode: 495, Cumulative Reward: -123.24411685604456\n",
      "Episode: 496, Cumulative Reward: -236.13470485076456\n",
      "Episode: 497, Cumulative Reward: -244.01076791835726\n",
      "Episode: 498, Cumulative Reward: -123.75752995795585\n",
      "Episode: 499, Cumulative Reward: -241.8600891655046\n",
      "Episode: 500, Cumulative Reward: -360.3664817311425\n",
      "Episode: 501, Cumulative Reward: -639.6134014081921\n",
      "Episode: 502, Cumulative Reward: -235.0703044134788\n",
      "Episode: 503, Cumulative Reward: -126.18720003425415\n",
      "Episode: 504, Cumulative Reward: -480.4581531716878\n",
      "Episode: 505, Cumulative Reward: -235.99396898635717\n",
      "Episode: 506, Cumulative Reward: -241.16935648610007\n",
      "Episode: 507, Cumulative Reward: -125.30719071032507\n",
      "Episode: 508, Cumulative Reward: -608.1761745862619\n",
      "Episode: 509, Cumulative Reward: -123.23702795212822\n",
      "Episode: 510, Cumulative Reward: -579.6684816188614\n",
      "Episode: 511, Cumulative Reward: -123.51475390387536\n",
      "Episode: 512, Cumulative Reward: -606.4382823859796\n",
      "Episode: 513, Cumulative Reward: -121.00964484876853\n",
      "Episode: 514, Cumulative Reward: -124.75429587184551\n",
      "Episode: 515, Cumulative Reward: -125.18822326115263\n",
      "Episode: 516, Cumulative Reward: -123.81018849267194\n",
      "Episode: 517, Cumulative Reward: -118.39880081598577\n",
      "Episode: 518, Cumulative Reward: -124.49393258770651\n",
      "Episode: 519, Cumulative Reward: -245.93064018707662\n",
      "Episode: 520, Cumulative Reward: -246.1283596693076\n",
      "Episode: 521, Cumulative Reward: -125.17831871114025\n",
      "Episode: 522, Cumulative Reward: -356.37128251910315\n",
      "Episode: 523, Cumulative Reward: -120.13844887405813\n",
      "Episode: 524, Cumulative Reward: -632.1139022856808\n",
      "Episode: 525, Cumulative Reward: -124.94396110379368\n",
      "Episode: 526, Cumulative Reward: -123.59056865482253\n",
      "Episode: 527, Cumulative Reward: -244.9165031988091\n",
      "Episode: 528, Cumulative Reward: -125.12002359802575\n",
      "Episode: 529, Cumulative Reward: -0.1611690633867029\n",
      "Episode: 530, Cumulative Reward: -121.55628171597935\n",
      "Episode: 531, Cumulative Reward: -121.18542326950734\n",
      "Episode: 532, Cumulative Reward: -121.88455586378724\n",
      "Episode: 533, Cumulative Reward: -236.59889329329727\n",
      "Episode: 534, Cumulative Reward: -241.36358387838268\n",
      "Episode: 535, Cumulative Reward: -366.69479663170944\n",
      "Episode: 536, Cumulative Reward: -124.08406743420305\n",
      "Episode: 537, Cumulative Reward: -119.46140209876657\n",
      "Episode: 538, Cumulative Reward: -123.64817152028375\n",
      "Episode: 539, Cumulative Reward: -368.07464780698604\n",
      "Episode: 540, Cumulative Reward: -0.5491568812125143\n",
      "Episode: 541, Cumulative Reward: -477.7683666597958\n",
      "Episode: 542, Cumulative Reward: -351.558235864596\n",
      "Episode: 543, Cumulative Reward: -125.69471337377476\n",
      "Episode: 544, Cumulative Reward: -498.65014314465697\n",
      "Episode: 545, Cumulative Reward: -123.79072557797082\n",
      "Episode: 546, Cumulative Reward: -234.70360255494379\n",
      "Episode: 547, Cumulative Reward: -622.0639381106104\n",
      "Episode: 548, Cumulative Reward: -123.36011033859187\n",
      "Episode: 549, Cumulative Reward: -244.6228957942669\n",
      "Episode: 550, Cumulative Reward: -238.39754962416174\n",
      "Episode: 551, Cumulative Reward: -244.13182936606842\n",
      "Episode: 552, Cumulative Reward: -123.67974972338276\n",
      "Episode: 553, Cumulative Reward: -605.9152932189025\n",
      "Episode: 554, Cumulative Reward: -243.62452290932663\n",
      "Episode: 555, Cumulative Reward: -236.6849080120615\n",
      "Episode: 556, Cumulative Reward: -121.68249719191162\n",
      "Episode: 557, Cumulative Reward: -544.321120861694\n",
      "Episode: 558, Cumulative Reward: -561.0120944871309\n",
      "Episode: 559, Cumulative Reward: -484.5556223099145\n",
      "Episode: 560, Cumulative Reward: -122.07591484075414\n",
      "Episode: 561, Cumulative Reward: -481.85742125915004\n",
      "Episode: 562, Cumulative Reward: -231.867365512387\n",
      "Episode: 563, Cumulative Reward: -122.88767941792862\n",
      "Episode: 564, Cumulative Reward: -243.14934364928828\n",
      "Episode: 565, Cumulative Reward: -118.93189669257096\n",
      "Episode: 566, Cumulative Reward: -123.68492958986053\n",
      "Episode: 567, Cumulative Reward: -123.19338137585582\n",
      "Episode: 568, Cumulative Reward: -120.58306568232467\n",
      "Episode: 569, Cumulative Reward: -241.0653518546952\n",
      "Episode: 570, Cumulative Reward: -478.36638451703504\n",
      "Episode: 571, Cumulative Reward: -244.7724764256764\n",
      "Episode: 572, Cumulative Reward: -360.57151063165395\n",
      "Episode: 573, Cumulative Reward: -363.52523730895024\n",
      "Episode: 574, Cumulative Reward: -240.33076863683442\n",
      "Episode: 575, Cumulative Reward: -123.5322688783412\n",
      "Episode: 576, Cumulative Reward: -240.98989217851027\n",
      "Episode: 577, Cumulative Reward: -0.1653003131653066\n",
      "Episode: 578, Cumulative Reward: -492.95312114469\n",
      "Episode: 579, Cumulative Reward: -122.48299335921068\n",
      "Episode: 580, Cumulative Reward: -510.1092528988149\n",
      "Episode: 581, Cumulative Reward: -528.4379810315883\n",
      "Episode: 582, Cumulative Reward: -121.87500196942244\n",
      "Episode: 583, Cumulative Reward: -474.2277290831348\n",
      "Episode: 584, Cumulative Reward: -120.30642453444509\n",
      "Episode: 585, Cumulative Reward: -119.42539196030054\n",
      "Episode: 586, Cumulative Reward: -0.04600264309369761\n",
      "Episode: 587, Cumulative Reward: -354.04662371084345\n",
      "Episode: 588, Cumulative Reward: -125.48139947289627\n",
      "Episode: 589, Cumulative Reward: -559.4057323956588\n",
      "Episode: 590, Cumulative Reward: -616.4067452488322\n",
      "Episode: 591, Cumulative Reward: -593.9566333959048\n",
      "Episode: 592, Cumulative Reward: -243.3511600362682\n",
      "Episode: 593, Cumulative Reward: -364.15502127566805\n",
      "Episode: 594, Cumulative Reward: -123.70032028819662\n",
      "Episode: 595, Cumulative Reward: -468.2984735764951\n",
      "Episode: 596, Cumulative Reward: -607.2898859301363\n",
      "Episode: 597, Cumulative Reward: -123.18348254573188\n",
      "Episode: 598, Cumulative Reward: -124.09895592699688\n",
      "Episode: 599, Cumulative Reward: -239.01289515276096\n",
      "Episode: 600, Cumulative Reward: -359.83105033569177\n",
      "Episode: 601, Cumulative Reward: -351.7828455359082\n",
      "Episode: 602, Cumulative Reward: -489.40888984669704\n",
      "Episode: 603, Cumulative Reward: -601.6452782947035\n",
      "Episode: 604, Cumulative Reward: -668.7704861726869\n",
      "Episode: 605, Cumulative Reward: -237.26015790295756\n",
      "Episode: 606, Cumulative Reward: -122.96780838759375\n",
      "Episode: 607, Cumulative Reward: -124.50771498041216\n",
      "Episode: 608, Cumulative Reward: -521.0757458300956\n",
      "Episode: 609, Cumulative Reward: -125.09067044147491\n",
      "Episode: 610, Cumulative Reward: -357.6575935081819\n",
      "Episode: 611, Cumulative Reward: -351.9330437662964\n",
      "Episode: 612, Cumulative Reward: -122.44697610441197\n",
      "Episode: 613, Cumulative Reward: -243.87202248392117\n",
      "Episode: 614, Cumulative Reward: -119.62922813262203\n",
      "Episode: 615, Cumulative Reward: -471.7025067659994\n",
      "Episode: 616, Cumulative Reward: -239.7424092627468\n",
      "Episode: 617, Cumulative Reward: -242.42718521654427\n",
      "Episode: 618, Cumulative Reward: -242.84542491259356\n",
      "Episode: 619, Cumulative Reward: -237.72995983796966\n",
      "Episode: 620, Cumulative Reward: -362.62988140694125\n",
      "Episode: 621, Cumulative Reward: -238.38085861785052\n",
      "Episode: 622, Cumulative Reward: -119.41072420647009\n",
      "Episode: 623, Cumulative Reward: -124.21641488395842\n",
      "Episode: 624, Cumulative Reward: -123.35673255753022\n",
      "Episode: 625, Cumulative Reward: -123.12550010776147\n",
      "Episode: 626, Cumulative Reward: -357.7939637398997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 627, Cumulative Reward: -123.48889296467141\n",
      "Episode: 628, Cumulative Reward: -124.62910336949875\n",
      "Episode: 629, Cumulative Reward: -476.1177881087296\n",
      "Episode: 630, Cumulative Reward: -123.0327284812863\n",
      "Episode: 631, Cumulative Reward: -123.12299463413905\n",
      "Episode: 632, Cumulative Reward: -122.12622463514543\n",
      "Episode: 633, Cumulative Reward: -239.3382029664931\n",
      "Episode: 634, Cumulative Reward: -240.1497485582993\n",
      "Episode: 635, Cumulative Reward: -0.7334808782809177\n",
      "Episode: 636, Cumulative Reward: -580.3128872484713\n",
      "Episode: 637, Cumulative Reward: -123.93235461824462\n",
      "Episode: 638, Cumulative Reward: -124.36746738829554\n",
      "Episode: 639, Cumulative Reward: -1.8631312235098547\n",
      "Episode: 640, Cumulative Reward: -476.5568565884675\n",
      "Episode: 641, Cumulative Reward: -486.6095319485106\n",
      "Episode: 642, Cumulative Reward: -354.1390358573103\n",
      "Episode: 643, Cumulative Reward: -242.91679081123405\n",
      "Episode: 644, Cumulative Reward: -123.61517653899769\n",
      "Episode: 645, Cumulative Reward: -365.6247273914781\n",
      "Episode: 646, Cumulative Reward: -247.78069298227123\n",
      "Episode: 647, Cumulative Reward: -124.43142298953713\n",
      "Episode: 648, Cumulative Reward: -122.75929753569135\n",
      "Episode: 649, Cumulative Reward: -580.776787911858\n",
      "Episode: 650, Cumulative Reward: -1.4063541670598685\n",
      "Episode: 651, Cumulative Reward: -245.14388434607912\n",
      "Episode: 652, Cumulative Reward: -595.7396836791243\n",
      "Episode: 653, Cumulative Reward: -123.11857330921012\n",
      "Episode: 654, Cumulative Reward: -121.07471200097135\n",
      "Episode: 655, Cumulative Reward: -243.42292456969395\n",
      "Episode: 656, Cumulative Reward: -352.4216385482926\n",
      "Episode: 657, Cumulative Reward: -470.65384109154746\n",
      "Episode: 658, Cumulative Reward: -120.6851586394982\n",
      "Episode: 659, Cumulative Reward: -359.02140914933426\n",
      "Episode: 660, Cumulative Reward: -580.5581264335399\n",
      "Episode: 661, Cumulative Reward: -244.74908118499076\n",
      "Episode: 662, Cumulative Reward: -0.5288212739723873\n",
      "Episode: 663, Cumulative Reward: -246.30280402151166\n",
      "Episode: 664, Cumulative Reward: -361.0628331361388\n",
      "Episode: 665, Cumulative Reward: -233.58129922305045\n",
      "Episode: 666, Cumulative Reward: -489.85007078682804\n",
      "Episode: 667, Cumulative Reward: -125.13601185626621\n",
      "Episode: 668, Cumulative Reward: -123.83740293530545\n",
      "Episode: 669, Cumulative Reward: -246.9823977869236\n",
      "Episode: 670, Cumulative Reward: -125.40725958324511\n",
      "Episode: 671, Cumulative Reward: -0.6171977821089476\n",
      "Episode: 672, Cumulative Reward: -472.9141502180013\n",
      "Episode: 673, Cumulative Reward: -124.74940608216428\n",
      "Episode: 674, Cumulative Reward: -234.24982546295863\n",
      "Episode: 675, Cumulative Reward: -0.3463293109915371\n",
      "Episode: 676, Cumulative Reward: -365.90531908162296\n",
      "Episode: 677, Cumulative Reward: -364.6055948292755\n",
      "Episode: 678, Cumulative Reward: -352.9613747690901\n",
      "epsilon:0.04710128697246249\n",
      "Episode: 679, Cumulative Reward: -237.4532693196462\n",
      "Episode: 680, Cumulative Reward: -472.7418682116278\n",
      "Episode: 681, Cumulative Reward: -235.1993920975087\n",
      "Episode: 682, Cumulative Reward: -124.43009456991341\n",
      "Episode: 683, Cumulative Reward: -360.9598064574672\n",
      "Episode: 684, Cumulative Reward: -121.10686945203922\n",
      "Episode: 685, Cumulative Reward: -122.56021008205397\n",
      "Episode: 686, Cumulative Reward: -122.81743065182944\n",
      "Episode: 687, Cumulative Reward: -586.0410892380819\n",
      "Episode: 688, Cumulative Reward: -124.38150722962773\n",
      "Episode: 689, Cumulative Reward: -238.26834971296483\n",
      "Episode: 690, Cumulative Reward: -248.74843459153402\n",
      "Episode: 691, Cumulative Reward: -474.97826758853057\n",
      "Episode: 692, Cumulative Reward: -242.18856593526743\n",
      "Episode: 693, Cumulative Reward: -242.22763851038127\n",
      "Episode: 694, Cumulative Reward: -363.3360018400895\n",
      "Episode: 695, Cumulative Reward: -487.1108339373525\n",
      "Episode: 696, Cumulative Reward: -122.92544097903017\n",
      "Episode: 697, Cumulative Reward: -0.32647933783148286\n",
      "Episode: 698, Cumulative Reward: -122.53819430324944\n",
      "Episode: 699, Cumulative Reward: -614.1730557693021\n",
      "Episode: 700, Cumulative Reward: -476.6085816800096\n",
      "Episode: 701, Cumulative Reward: -676.9024693531197\n",
      "Episode: 702, Cumulative Reward: -238.65548684194457\n",
      "Episode: 703, Cumulative Reward: -123.5278917016973\n",
      "Episode: 704, Cumulative Reward: -490.4321893067127\n",
      "Episode: 705, Cumulative Reward: -0.4131365369671259\n",
      "Episode: 706, Cumulative Reward: -355.2436354420408\n",
      "Episode: 707, Cumulative Reward: -124.72690498002605\n",
      "Episode: 708, Cumulative Reward: -357.18085341224685\n",
      "epsilon:0.04710128697246249\n",
      "Episode: 709, Cumulative Reward: -357.9910496826577\n",
      "Episode: 710, Cumulative Reward: -595.4045261790889\n",
      "Episode: 711, Cumulative Reward: -122.84266265741402\n",
      "Episode: 712, Cumulative Reward: -355.35611215631553\n",
      "Episode: 713, Cumulative Reward: -572.4558345573186\n",
      "Episode: 714, Cumulative Reward: -122.36057264573252\n",
      "Episode: 715, Cumulative Reward: -119.95632915145512\n",
      "Episode: 716, Cumulative Reward: -124.30912828146076\n",
      "Episode: 717, Cumulative Reward: -486.70783123732986\n",
      "Episode: 718, Cumulative Reward: -362.320824232501\n",
      "Episode: 719, Cumulative Reward: -483.4899182552574\n",
      "Episode: 720, Cumulative Reward: -122.52710288422874\n",
      "Episode: 721, Cumulative Reward: -121.70298824381733\n",
      "Episode: 722, Cumulative Reward: -350.5442773008599\n",
      "Episode: 723, Cumulative Reward: -362.49403757298984\n",
      "Episode: 724, Cumulative Reward: -124.0867409504838\n",
      "Episode: 725, Cumulative Reward: -119.48325642831875\n",
      "Episode: 726, Cumulative Reward: -123.44315663070077\n",
      "Episode: 727, Cumulative Reward: -121.74468548772265\n",
      "Episode: 728, Cumulative Reward: -122.49603425132351\n",
      "Episode: 729, Cumulative Reward: -354.5551125120261\n",
      "Episode: 730, Cumulative Reward: -123.6837001337584\n",
      "Episode: 731, Cumulative Reward: -243.3031401844534\n",
      "Episode: 732, Cumulative Reward: -122.53046902767949\n",
      "Episode: 733, Cumulative Reward: -120.66462781943984\n",
      "Episode: 734, Cumulative Reward: -235.48369938771486\n",
      "Episode: 735, Cumulative Reward: -123.76654573269684\n",
      "Episode: 736, Cumulative Reward: -120.93070598968112\n",
      "Episode: 737, Cumulative Reward: -526.0439415988567\n",
      "Episode: 738, Cumulative Reward: -124.50571103063957\n",
      "Episode: 739, Cumulative Reward: -365.6069065594503\n",
      "Episode: 740, Cumulative Reward: -120.1491734287437\n",
      "Episode: 741, Cumulative Reward: -364.63976371420955\n",
      "Episode: 742, Cumulative Reward: -245.2929842602562\n",
      "Episode: 743, Cumulative Reward: -123.91996634925917\n",
      "Episode: 744, Cumulative Reward: -122.834952300626\n",
      "Episode: 745, Cumulative Reward: -245.21572923430253\n",
      "Episode: 746, Cumulative Reward: -491.1648997125678\n",
      "Episode: 747, Cumulative Reward: -121.95662942891988\n",
      "Episode: 748, Cumulative Reward: -122.7485931807452\n",
      "Episode: 749, Cumulative Reward: -120.64318932500903\n",
      "Episode: 750, Cumulative Reward: -124.80861780720335\n",
      "Episode: 751, Cumulative Reward: -124.00186487727298\n",
      "Episode: 752, Cumulative Reward: -243.32155979294717\n",
      "Episode: 753, Cumulative Reward: -475.1543001837884\n",
      "Episode: 754, Cumulative Reward: -125.15251612672212\n",
      "Episode: 755, Cumulative Reward: -245.63192338797867\n",
      "Episode: 756, Cumulative Reward: -124.25744929726106\n",
      "Episode: 757, Cumulative Reward: -359.89316999755346\n",
      "Episode: 758, Cumulative Reward: -120.77377072661949\n",
      "Episode: 759, Cumulative Reward: -121.38919251678251\n",
      "Episode: 760, Cumulative Reward: -351.56697729836395\n",
      "Episode: 761, Cumulative Reward: -124.80394582151426\n",
      "Episode: 762, Cumulative Reward: -555.7189519607567\n",
      "Episode: 763, Cumulative Reward: -490.37565298049145\n",
      "Episode: 764, Cumulative Reward: -122.63100022826492\n",
      "Episode: 765, Cumulative Reward: -120.85492075010194\n",
      "Episode: 766, Cumulative Reward: -120.24382066848835\n",
      "Episode: 767, Cumulative Reward: -119.73848461238664\n",
      "Episode: 768, Cumulative Reward: -242.79988199537988\n",
      "Episode: 769, Cumulative Reward: -125.50484105990954\n",
      "Episode: 770, Cumulative Reward: -124.77896536805726\n",
      "Episode: 771, Cumulative Reward: -121.0822063901721\n",
      "Episode: 772, Cumulative Reward: -603.5754962423297\n",
      "Episode: 773, Cumulative Reward: -236.38381686764072\n",
      "Episode: 774, Cumulative Reward: -615.738521850732\n",
      "Episode: 775, Cumulative Reward: -122.75767733433808\n",
      "Episode: 776, Cumulative Reward: -123.46833507976439\n",
      "Episode: 777, Cumulative Reward: -559.8433727500466\n",
      "Episode: 778, Cumulative Reward: -240.24699673689292\n",
      "Episode: 779, Cumulative Reward: -244.48381417584864\n",
      "Episode: 780, Cumulative Reward: -122.10275622338477\n",
      "Episode: 781, Cumulative Reward: -122.87573315980768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 782, Cumulative Reward: -122.36761244827443\n",
      "Episode: 783, Cumulative Reward: -351.2427218561528\n",
      "Episode: 784, Cumulative Reward: -689.3442418525037\n",
      "Episode: 785, Cumulative Reward: -120.74870950729698\n",
      "Episode: 786, Cumulative Reward: -609.0666873074234\n",
      "Episode: 787, Cumulative Reward: -125.03299816471149\n",
      "Episode: 788, Cumulative Reward: -126.31255647708836\n",
      "Episode: 789, Cumulative Reward: -627.7268515382053\n",
      "Episode: 790, Cumulative Reward: -120.74741141647601\n",
      "Episode: 791, Cumulative Reward: -122.88472912094005\n",
      "Episode: 792, Cumulative Reward: -242.78637066590395\n",
      "Episode: 793, Cumulative Reward: -361.2655839116635\n",
      "Episode: 794, Cumulative Reward: -481.32813532435495\n",
      "Episode: 795, Cumulative Reward: -126.44919149680298\n",
      "Episode: 796, Cumulative Reward: -242.6345596073584\n",
      "Episode: 797, Cumulative Reward: -485.3457997360751\n",
      "Episode: 798, Cumulative Reward: -239.95320540952582\n",
      "Episode: 799, Cumulative Reward: -243.48712871465602\n",
      "Episode: 800, Cumulative Reward: -229.20625617968125\n",
      "Episode: 801, Cumulative Reward: -124.20935087772219\n",
      "Episode: 802, Cumulative Reward: -123.18814620637033\n",
      "Episode: 803, Cumulative Reward: -521.2327032867701\n",
      "Episode: 804, Cumulative Reward: -624.206124289246\n",
      "Episode: 805, Cumulative Reward: -122.49443500318806\n",
      "Episode: 806, Cumulative Reward: -121.72643074887372\n",
      "Episode: 807, Cumulative Reward: -481.38123680157105\n",
      "Episode: 808, Cumulative Reward: -605.6258052124231\n",
      "Episode: 809, Cumulative Reward: -496.5421927646905\n",
      "Episode: 810, Cumulative Reward: -481.9863899064386\n",
      "Episode: 811, Cumulative Reward: -247.31679042693543\n",
      "Episode: 812, Cumulative Reward: -121.08399819363487\n",
      "Episode: 813, Cumulative Reward: -233.42980821539433\n",
      "Episode: 814, Cumulative Reward: -249.3745883050504\n",
      "Episode: 815, Cumulative Reward: -606.4356604356221\n",
      "Episode: 816, Cumulative Reward: -244.85380779509038\n",
      "Episode: 817, Cumulative Reward: -0.7453521204216709\n",
      "Episode: 818, Cumulative Reward: -364.2279969216299\n",
      "Episode: 819, Cumulative Reward: -0.3349761835175106\n",
      "Episode: 820, Cumulative Reward: -124.71422784098058\n",
      "Episode: 821, Cumulative Reward: -477.7153683207009\n",
      "Episode: 822, Cumulative Reward: -597.1897904995383\n",
      "Episode: 823, Cumulative Reward: -231.5813026007256\n",
      "Episode: 824, Cumulative Reward: -357.22057700969475\n",
      "Episode: 825, Cumulative Reward: -124.53846152396652\n",
      "Episode: 826, Cumulative Reward: -124.94465683215235\n",
      "Episode: 827, Cumulative Reward: -122.23883766451073\n",
      "Episode: 828, Cumulative Reward: -469.8922101542663\n",
      "Episode: 829, Cumulative Reward: -123.49560102663334\n",
      "Episode: 830, Cumulative Reward: -182.10438067287916\n",
      "Episode: 831, Cumulative Reward: -123.91737014496948\n",
      "Episode: 832, Cumulative Reward: -121.25849385610191\n",
      "Episode: 833, Cumulative Reward: -482.51049816547794\n",
      "Episode: 834, Cumulative Reward: -238.03698683904358\n",
      "Episode: 835, Cumulative Reward: -244.78081270351828\n",
      "Episode: 836, Cumulative Reward: -122.8056853162909\n",
      "Episode: 837, Cumulative Reward: -124.21238700902316\n",
      "Episode: 838, Cumulative Reward: -119.00874951409679\n",
      "Episode: 839, Cumulative Reward: -239.1566176777226\n",
      "Episode: 840, Cumulative Reward: -0.5302907973373547\n",
      "Episode: 841, Cumulative Reward: -592.518931156771\n",
      "Episode: 842, Cumulative Reward: -359.1524629121994\n",
      "Episode: 843, Cumulative Reward: -123.88777965276766\n",
      "Episode: 844, Cumulative Reward: -119.36885041662178\n",
      "Episode: 845, Cumulative Reward: -122.21878807643398\n",
      "Episode: 846, Cumulative Reward: -614.2450414284043\n",
      "Episode: 847, Cumulative Reward: -243.34432810948687\n",
      "Episode: 848, Cumulative Reward: -365.2764338208768\n",
      "Episode: 849, Cumulative Reward: -596.6276032280529\n",
      "Episode: 850, Cumulative Reward: -231.03483690506425\n",
      "Episode: 851, Cumulative Reward: -604.3509475944534\n",
      "Episode: 852, Cumulative Reward: -240.59622984218018\n",
      "Episode: 853, Cumulative Reward: -534.3247660393013\n",
      "Episode: 854, Cumulative Reward: -119.25821839615475\n",
      "Episode: 855, Cumulative Reward: -0.24100411510028194\n",
      "Episode: 856, Cumulative Reward: -240.89009370351044\n",
      "Episode: 857, Cumulative Reward: -125.42617251274115\n",
      "Episode: 858, Cumulative Reward: -353.52107265498563\n",
      "Episode: 859, Cumulative Reward: -237.70597456094706\n",
      "Episode: 860, Cumulative Reward: -489.2688730590794\n",
      "Episode: 861, Cumulative Reward: -488.890084028763\n",
      "Episode: 862, Cumulative Reward: -119.07905856766462\n",
      "Episode: 863, Cumulative Reward: -124.58058112976411\n",
      "Episode: 864, Cumulative Reward: -363.2049759626904\n",
      "Episode: 865, Cumulative Reward: -240.33435420237893\n",
      "Episode: 866, Cumulative Reward: -236.1433068391738\n",
      "Episode: 867, Cumulative Reward: -244.62518160621298\n",
      "Episode: 868, Cumulative Reward: -489.50348083474677\n",
      "Episode: 869, Cumulative Reward: -118.42356230783268\n",
      "Episode: 870, Cumulative Reward: -119.23497874302637\n",
      "Episode: 871, Cumulative Reward: -123.488241499731\n",
      "Episode: 872, Cumulative Reward: -648.2734326325874\n",
      "Episode: 873, Cumulative Reward: -360.34599721057396\n",
      "Episode: 874, Cumulative Reward: -123.5143354057713\n",
      "Episode: 875, Cumulative Reward: -121.84880455333622\n",
      "Episode: 876, Cumulative Reward: -477.1509123959673\n",
      "Episode: 877, Cumulative Reward: -119.32512604696163\n",
      "Episode: 878, Cumulative Reward: -125.21169298932882\n",
      "Episode: 879, Cumulative Reward: -595.0736293840821\n",
      "Episode: 880, Cumulative Reward: -122.64723873944317\n",
      "Episode: 881, Cumulative Reward: -504.81759546900867\n",
      "Episode: 882, Cumulative Reward: -121.86022841909094\n",
      "Episode: 883, Cumulative Reward: -0.5067998971148443\n",
      "Episode: 884, Cumulative Reward: -125.25783687559694\n",
      "Episode: 885, Cumulative Reward: -362.15860898212503\n",
      "Episode: 886, Cumulative Reward: -122.26472096271179\n",
      "Episode: 887, Cumulative Reward: -123.84951413482364\n",
      "Episode: 888, Cumulative Reward: -355.2059340358678\n",
      "Episode: 889, Cumulative Reward: -125.65058597528794\n",
      "Episode: 890, Cumulative Reward: -121.09376651072228\n",
      "Episode: 891, Cumulative Reward: -1.3912830868460233\n",
      "Episode: 892, Cumulative Reward: -119.83167647016175\n",
      "Episode: 893, Cumulative Reward: -123.99173097900945\n",
      "Episode: 894, Cumulative Reward: -125.71481713757434\n",
      "Episode: 895, Cumulative Reward: -353.9344619885394\n",
      "Episode: 896, Cumulative Reward: -120.63557933610414\n",
      "Episode: 897, Cumulative Reward: -604.7684276186048\n",
      "Episode: 898, Cumulative Reward: -357.0708538315383\n",
      "Episode: 899, Cumulative Reward: -383.1685760363223\n",
      "Episode: 900, Cumulative Reward: -363.4579351725412\n",
      "Episode: 901, Cumulative Reward: -479.2163849990726\n",
      "Episode: 902, Cumulative Reward: -124.54146036311101\n",
      "Episode: 903, Cumulative Reward: -124.39032028353958\n",
      "Episode: 904, Cumulative Reward: -244.1542085419014\n",
      "Episode: 905, Cumulative Reward: -488.63916602707235\n",
      "Episode: 906, Cumulative Reward: -0.3507533634282417\n",
      "Episode: 907, Cumulative Reward: -120.26059522377751\n",
      "Episode: 908, Cumulative Reward: -125.73680610551818\n",
      "Episode: 909, Cumulative Reward: -602.8494564351886\n",
      "Episode: 910, Cumulative Reward: -354.3098709314868\n",
      "Episode: 911, Cumulative Reward: -121.12697461119129\n",
      "Episode: 912, Cumulative Reward: -365.1477613997639\n",
      "Episode: 913, Cumulative Reward: -468.77525807317016\n",
      "Episode: 914, Cumulative Reward: -358.0280041067043\n",
      "Episode: 915, Cumulative Reward: -124.99942467855517\n",
      "Episode: 916, Cumulative Reward: -124.77435185496323\n",
      "Episode: 917, Cumulative Reward: -236.76502114872858\n",
      "Episode: 918, Cumulative Reward: -496.12825336136297\n",
      "Episode: 919, Cumulative Reward: -241.78587371431178\n",
      "Episode: 920, Cumulative Reward: -480.4556958058117\n",
      "Episode: 921, Cumulative Reward: -122.15301700329532\n",
      "Episode: 922, Cumulative Reward: -348.15646658210545\n",
      "Episode: 923, Cumulative Reward: -582.6849192686748\n",
      "Episode: 924, Cumulative Reward: -361.3153412068287\n",
      "Episode: 925, Cumulative Reward: -475.5828518199775\n",
      "Episode: 926, Cumulative Reward: -232.5603748326661\n",
      "Episode: 927, Cumulative Reward: -494.7917491531619\n",
      "Episode: 928, Cumulative Reward: -239.9927066656889\n",
      "Episode: 929, Cumulative Reward: -243.68764739737014\n",
      "Episode: 930, Cumulative Reward: -124.00268478094944\n",
      "Episode: 931, Cumulative Reward: -241.86846569273658\n",
      "Episode: 932, Cumulative Reward: -0.5504444087086533\n",
      "Episode: 933, Cumulative Reward: -238.06273605540056\n",
      "Episode: 934, Cumulative Reward: -125.86747166287442\n",
      "Episode: 935, Cumulative Reward: -121.11777111916463\n",
      "Episode: 936, Cumulative Reward: -123.75873325176607\n",
      "Episode: 937, Cumulative Reward: -126.0506544810917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 938, Cumulative Reward: -124.90909287617666\n",
      "Episode: 939, Cumulative Reward: -120.91773324791106\n",
      "Episode: 940, Cumulative Reward: -363.0066902538082\n",
      "Episode: 941, Cumulative Reward: -123.38900146861663\n",
      "Episode: 942, Cumulative Reward: -123.95050050916296\n",
      "Episode: 943, Cumulative Reward: -126.77214823595709\n",
      "Episode: 944, Cumulative Reward: -365.12314488219016\n",
      "Episode: 945, Cumulative Reward: -360.8558858777742\n",
      "Episode: 946, Cumulative Reward: -348.85753466979065\n",
      "Episode: 947, Cumulative Reward: -358.1721174321567\n",
      "Episode: 948, Cumulative Reward: -474.9755321490512\n",
      "epsilon:0.04710128697246249\n",
      "Episode: 949, Cumulative Reward: -124.95963119040705\n",
      "Episode: 950, Cumulative Reward: -123.3499961644961\n",
      "Episode: 951, Cumulative Reward: -600.2830493027876\n",
      "Episode: 952, Cumulative Reward: -124.1761738378542\n",
      "Episode: 953, Cumulative Reward: -123.88086332239024\n",
      "Episode: 954, Cumulative Reward: -360.5038477533114\n",
      "Episode: 955, Cumulative Reward: -489.410710311959\n",
      "Episode: 956, Cumulative Reward: -363.38939739235485\n",
      "Episode: 957, Cumulative Reward: -357.39761351822597\n",
      "Episode: 958, Cumulative Reward: -618.5573616897744\n",
      "Episode: 959, Cumulative Reward: -122.47024584880782\n",
      "Episode: 960, Cumulative Reward: -123.40408310958851\n",
      "Episode: 961, Cumulative Reward: -120.97179538055241\n",
      "Episode: 962, Cumulative Reward: -239.5805618190807\n",
      "Episode: 963, Cumulative Reward: -0.5874227791776162\n",
      "Episode: 964, Cumulative Reward: -236.96849361585208\n",
      "Episode: 965, Cumulative Reward: -0.8191648254491842\n",
      "Episode: 966, Cumulative Reward: -365.7497988830932\n",
      "Episode: 967, Cumulative Reward: -125.45321932368695\n",
      "Episode: 968, Cumulative Reward: -492.55335360746926\n",
      "Episode: 969, Cumulative Reward: -245.90012654309453\n",
      "Episode: 970, Cumulative Reward: -123.66681077664309\n",
      "Episode: 971, Cumulative Reward: -238.352136533672\n",
      "Episode: 972, Cumulative Reward: -240.02354813859623\n",
      "Episode: 973, Cumulative Reward: -351.2235017772007\n",
      "Episode: 974, Cumulative Reward: -572.4575075310169\n",
      "Episode: 975, Cumulative Reward: -0.3614244751611526\n",
      "Episode: 976, Cumulative Reward: -124.37236001101769\n",
      "Episode: 977, Cumulative Reward: -361.13215050451936\n",
      "Episode: 978, Cumulative Reward: -240.2458289433085\n",
      "Episode: 979, Cumulative Reward: -484.932655246037\n",
      "Episode: 980, Cumulative Reward: -235.9647521650729\n",
      "Episode: 981, Cumulative Reward: -119.19234150896288\n",
      "Episode: 982, Cumulative Reward: -124.28976378943705\n",
      "Episode: 983, Cumulative Reward: -241.36240754658917\n",
      "Episode: 984, Cumulative Reward: -485.6736831400504\n",
      "Episode: 985, Cumulative Reward: -117.37936266402086\n",
      "Episode: 986, Cumulative Reward: -122.67734751372205\n",
      "Episode: 987, Cumulative Reward: -126.23869546885267\n",
      "Episode: 988, Cumulative Reward: -0.5465423296711558\n",
      "Episode: 989, Cumulative Reward: -238.65523776621305\n",
      "Episode: 990, Cumulative Reward: -608.0695789945644\n",
      "Episode: 991, Cumulative Reward: -632.6674323550192\n",
      "Episode: 992, Cumulative Reward: -243.45739278158968\n",
      "Episode: 993, Cumulative Reward: -476.9476201385166\n",
      "Episode: 994, Cumulative Reward: -477.66050069309296\n",
      "Episode: 995, Cumulative Reward: -123.46460648725646\n",
      "Episode: 996, Cumulative Reward: -483.35506347362656\n",
      "Episode: 997, Cumulative Reward: -232.38098497315642\n",
      "Episode: 998, Cumulative Reward: -123.11239925897378\n",
      "Episode: 999, Cumulative Reward: -479.8132587217511\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_episode_num = 1000\n",
    "cumulative_reward_history = []\n",
    "episodes = []\n",
    "for episode in range(max_episode_num):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "    i = 1\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.perceive_and_act(observation, reward, done)\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        #print('Episode: {}, Step: {}, Reward: {}'.format(episode, i, reward))\n",
    "        i += 1\n",
    "    print('Episode: {}, Cumulative Reward: {}'.format(episode,cumulative_reward))\n",
    "    cumulative_reward_history.append(cumulative_reward)\n",
    "    episodes.append(episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._save_learned_model(version_number=1)\n",
    "tf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAAAXNSR0IArs4c6QAAQABJREFUeAHsnQn8fcX8/ydSKa2SSlrIli2VpRQR2QlR1iLyt2QrUUi2sqS0IVLKFll+tiRLJcpSSlSEorSX9k24/3l97/fdfd/3nZkzc86593Puva95POZzZnnPe97znHPmvD9zzr3XOQYSIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIAESIIGZIrDETI1m8oMBvzV9vHHyXbNHEiABEiABEiCBBgSW920v9bHXQMfUNl1yai3vhuFw/v7ZDVNoBQmQAAmQAAmQQCGBtbz8JYVtZkKcDmCzaVy083fxxRe7FVZYoZkmtiYBEiABEiABEpgIgRtuuMHd9773RV9z+wSPDmALpxqcPzqALYCkChIgARIgARIggYkQuMtEemEnJEACJEACJEACJEACnSFAB7AzU0FDSIAESIAESIAESGAyBOgAToYzeyEBEiABEiABEiCBzhCgA9iZqaAhJEACJEACJEACJDAZAnQAJ8OZvZAACZAACZAACZBAZwjQAezMVNAQEiABEiABEiABEpgMATqAk+HMXkiABEiABEiABEigMwToAHZmKmgICZAACZAACZAACUyGAB3AyXBmLyRAAiRAAiRAAiTQGQJ0AJ17g5+NC328zcczfNzCRwYSIAESIAESIAESmFkC8+4Abudn9pM+ftjHR/l4io8/9HFtHxlIgARIgARIgARIYCYJzLsD+HY/q5/38XAfz/PxrT5e7OPrfWQgARIgARIgARIggZkkMM8O4FJ+Rjf28QQzs8hvZsoku7RPrKDi8lIx08c//cm5Y491rteb3mF+//vOnXxy3/4bbnDuU59y7vLL+/mTTvL7wH4j2I7vqqucO+gg5665Jj3uI45w7phjBjL//Kdz739/X/8Z/q0C1FvdA+my1M03O/ftbzuHY1W46CLnjjzSuX//eyB55pnOfeAD/oUHvPHgwxe/6NxPftJPy99DDnHuRz/qj/u973XuuOP647n+epHIO57n/6fac89+27PPzmtTJXXTTc4deqhzl1xSJZmux9yfdlpapk7t737n3F57OXfLLYPWf/yjcwce6NwddwzKSlOYxy9/edDqS1/qn8ODknZSp5/uHHT/7W/O4fzR4de/9v8m4/9kFf7yF+f239+5W2/tF17s/3/GtYTzZ7/9nPvf/5RwIPnTnzp38MH9iv/+17lPfMK/iOOvmWkP4LLDDs7tu2+awe23O3fAAaNcMf5//KN/Ll15ZT4NXMtHH92XB/sf/tC5K64YbY/r50MfCteNSjuH8wJrZuk6hnY77ujcN7850Ir198P+odv55w/KkIKdOGew7jKQwJgJrOn1w6PZzPTj71juz6ZMsnv7BNoMxeuvv74306F/2fd6xx03ncO85BIsW/2IEWy3XT/98If3xyN13//+8Pi22KIv9+QnD5fr3KWXDnT/5z/9mg026Jc97nGDOqtb6yhJb799XyfGUBWWX74vu88+A0kZ695793rnnjuwTyROPXVQts02gzTa7bijSOUdl1hiuH1eq7TUzjv3da6zTlquqlY4nHZalWRZvejdc89BOyn75CcHZSWpK68ccLzttl7vv/8d5C++uERTtazYKsf//a/fRp/nWovI7b57v3SNNQa2oe4rX9HSo2lpf/LJvd5nPztoOyo5XSUyLhy//OW47R/4QHzMOMfRfqut4u1tjfR7zjm93lFH9duvsoqV6vWw9kF2881H60IlovfrXw/VxsukHY6wCUGu4SWX7Ofl72Me07dpyy2lZKxH3LcX38uxqTOX4S5zOerhQeMk0GEJn7FlUu//nXMrqriWVMzFEf/NTWOw/wHLf6N/+MPwaP761+H8Kaf08z/72XC5zuldMSxfCOee2z/+6lf9I/7avgY1ZSnZafza16rb3XhjXwa7MTZgl8Xu8EDmwgsHkrJjKiWlO2bCQ9q3ccROLgJ2R9oIdoxt6IQO7LTaUPf6wY6JBOyS6Z3EnJ1gaVvnKDt4+rwI6fnFL/qll102XPvn2P/Sw2Lu7393rq1dYqN6wbOyHoQMOfXUUGm/TM7xE0+My8RqsMP33e/2a//1r1EpWY9k3kYlwiXYza4bMMcIMub//Kefl7+/+U0/hScyDBMhsOREeulmJ1d7s/xq6lY35q3m84E980VSfr/eIc5nWAK+8QwEu/A0GZJ2cnS6ic5JtAUDOBM2aDa2fskZXC7uMgX/A+vzCtegdgDHPSfSd11OuWtGrpw9X6chb68jbbM42LrMpu96V1synXmZ47vdbTrtn0Grp2D1Gxv1f3vNeNnkqaYH5BP/lhnpecrWvQlMCyO52ZXYq9votNUhi58tX6g8nIjQjWneHMBJ3lzrngPaScA5pt/pHLf90re+9lPnuT2fc8cMnbmyto+u50PXmdicwzJ3jlP9SH9dOOaOpwu2zrgNM/gvfdGM+beX3Rd9xLPN03zc2ce1ffyMjwyWwCws0KkFN1VnWUhet9Fpqe/qMbYDqHeX7A1lFhdu7di0OVdtXiv6vEJaz9G47BcW0rceD5zCcZwLug/pfxaO9jrSYxIHW5fZdC5rfV5YHV3Iy/yOe9e6C2OdEhvm3QHEi1T39HEvH9fwES84PNPHll4w8poYukVAbmghq1J1IXlb1rS91TfOvHUAYTsW6NQOYO6NaJx2t617XA5Um+eC1gWHQe8A6rq22UCf6JebN8rg0OSeCym+2vnR+tHHLAV9TdlxaQa2TvIphiKDoz4vdHmbaTkf6uiUcdABrENvLG3m3QEEVP/Z9kVxLIBnSuksL9KYqCaLG9rnLOaQm3QIjcs6gLAdN3W9i2B3LmZx4c51ZNqYs7rXjz6vMJd6jnRdGzZaHaJfbt6ot+eFbaPzqTHrcWBcKVmtc9rSKQcwdG3a8eWeo5qn1dGFvMwv3wHswmwssmGe3wHszCRMjSFyAU+NwQFDcxbcQLNokdan09EGHanAzULfyCWtb1Zy8xeTu+AAtn0O5t5chUGTY13b9XmFtN7psXPUxL5QW+lb217Sp25n9XfdYbH21s3LtRVqL3xDdVKWe47q8yLVp+hdqGPVOpI6ZxbK5hntlw7gjE7sWIY1CxdmasFN1cWA6jY6HZPvSjkcPX0jlxuGdgCtrVULt5Xval7GCvv0zlZX7dXnFeZMO066bhz2yzmiOWl+TfrUDksTPV1vm+IlfFNjyHUA9Xmh0yndk6yT+0fVOpI73knaPqN90QGc0YnlsCIEUjdMW5ezEOk2Oh3pvjPFcPT0jUluRCkHMIdHZwaYMETfHKdhTDI3GBLOMe046brEkGtXyTld1wGUm37IAD0PGEdKNtR+WspS11TO/Gn2qTFrnjodarMQ573Mb5UDmDve0LhYVkSADmARrjkXlgt4VjHIzU7Gl7NI6jY6LTq6cAzZhRuEdgAlnbpx5PDownirbNBjnIabjZ4/pLX9OQ5EFY9UvfStr/2SPlN89TiQ1n2kbJq2Orm2QnbnsMy97vQ/BkineFY5YSFbm5aJPVV9p86Zpjaw/RABOoBDOJhJEpiFC1NuaKGB2rrchVd02fZSjmOqTstNKo1dCX3zkXRqt6Jq4Z6U7U370Y7HJM9puQGW2q/PHaS1/TJvpTpD8rofqRf92vaUQyPt5KjbSZkctcOixyT1s3JMXVMh5nbcueuQZqjTVh/yC3Ety7lQ1XfueEPjYlkRATqARbjmXFgu4GnGkFpwbV3VQgUOuo1OW0apm4CVnUQeNwh9I5d06saRw2PctrdxDmrHY9z2av11bRcnDLqQ1vbrOt1XnXRIV+iclnMlp4/UmPW5VrVjldNXV2VS136IuR1HrkOkeeo09Nl+FuJalnOhqu/c8VpOzBcToANYjIwNpppA6IYWG1DVQoV2Wp9OW536pm3rxp0P2YWbkr4xyQ1Cl1m7cnjYNl3M65ujjLsNO0Oc29CrbUQf47I/5NjJmOSI8Wh7moxPj0Onm+jsYtsQV7Ezh2XuLrVeY3QafVm+C3Et5zqAueMVhjzWJkAHsDa6OWwoF/CsDN0uNPomhzHm/Ceq2+i0ZWQXYFvfdr7qxmIdQLlJpRzAHB5tj2Mc+vRcyLjb6EfrTZ0LpX1pG6FX39yr5rmkL92PtBP9ckR5SE7k7TG1ZuhxaHZWx7TnU7w019g4c687zVCnodfmF8IBlPFVfQ+gXZelHY+tE6AD2DrSGVaYWsynZdj6xmwXQV2H8eQuvDJ2217KcbQLsK4bR7qqPzh6+gYsN6l5cABD425jDrTekL6614/MDXTCYdBzmzrnQjakynQ/Iif65YjykJzI22PqZq7HAXZ1+dg+u5ZPXVOaa8zu3HVI89Rp6LXnpl37Yn03KbdjE2e3qu/UOdPEHrYdIUAHcAQJC6IEZmGB1ouSXYh0HSDY+hAY3UanraxdgG1923l7A7D6cVPSMrI46zLbJoeHtBF9ku/SUY+xxJmpGoPW2+a1olniHNPnkq6rsq+qPqRLyvS5LWVV+lCf4qB5IZ2SzemrqzKTcgD1eQGeep40a3AquZb13Jcw1v2jneipcmir6ktsoGySAB3AJB5WzjQBuwjKAiWDLl2IbHvRg6NdgHXdONL6ZhDSD3u0jDhCqZuV5RXSK2WTGG+Kt9gROmrbZNwhudIyrdfe/KCrroOjbcSYq/optVvkdT9SJozliPKQnMjbY2rMehz6XLQ6pj2f4hU6T+x4c9chy9Pmtd6SaznHRq1b0nbcokf3bWXQNne80g+PtQnQAayNbg4bphbzacGhb2RYaHRepzGenIVIt5EFLsRCL8ah+rbLdH+hRTa2A9h1B1CfgylbUzy1sxFik2qbqtN6Nf9Um5w6bSPOMd1P6pzL0a1ldD9SLue3HFEekkuViy571OMALz23Vnaa8zFeGFPO/OWsQ9Bledo8ZCRUvYcncjhq+/V5oGVCaTs2yeu+tY2ig4+AhcTYj3QAx454hjqYhQtTL2D4T1Tn7VTp/1RtneRlUUM+pSu00ImOcRx1fyFnxDqAssinnKrcGxHGo/sfx/ia9KF56PlraqfWq9NN9WobcY5p3bquaT9yDmg9ol+f21Km5ZDWdkldyqnT8jotbWflmLqmNFc9Xl2eu+5qhkjrvL0ec9Y2sSd0Xkhd6mjbyXmj+9Y2iq7c8Yo8j7UJ0AGsjW4OG6YW82nBoRdW6wDqOoxHL1QyPsgcfbRzX/mKc9/4hnMXXig1aQfwvPOc2203566+ui+Pm8Ltt/fTl13m3M9/PtCDhfLHP3buL39x7pvfdO7Xv+7XIR0LsOu665y78krn0JdeWLH4n3++c3/4w3BrfVOQxTp1sxIH8IYbnENEwHhg70c+4tyHP+zckUc699Wvxh1A8PrEJwZj72vp/5UbBMZy7rnOnXLKMFOUY3wSYMMf/9jPXXWVc5df7tzxxzv3t7+JxOB46aXOnXlmP3/NNYNyGTfqoRt9VIV//as/75jPQw/tjwXnAvqX8Nvf9nV9+ctSEt/huvbavgzmA+N45zud+/a3nfv73/vlYiNyYPSb3/TLJT/IxVM41z79aef++te+DPgecsjweSLno9YiPOSIOrS98cb+/GhZfc5JudzMb73VuXPOGear5fW5KG1xvOAC5771rX6766/v9wvmJ57Ytx3nAK4dnAdgd+yxzkEOAeM5+eTBuYYxnH66cxdf3K/HdXXwwf1rA9eM5ow05KH3iiucwzVqr5++lr4dt9zSz+F6wLmkg1xToh8241zD+MFEwvvf79wll/Rz0gY5MATvvfbqrwnoa6ednHvve/s6zj67f03JNYk2GLtmCtvBUoJe23BO3Hab1PSPun+5LlGDawt6f//74bkcbt3P3XTTcKno0X1Dz0c/6tzuuw9kZZ0ZlDBFAp0ksIK3qnf99df3Zjr0L/Ve79BDp3OYv/sdlvJ+vOGGQfo+9+n1dtllkIfM1lv3er/4Ra93yCG93tJLD+pWXXWQFl04LrfcoHy33Xq9v/xlkNdykl577V7v3HN7vTXX7PVWWaXX+9KXBvKve12f78tfPiiTdnvvPVr2q1/1epde2us9/OGjddIudVxjjUG7pZbq9d761kE+1G6JJXq9U07p9cAN9SuskJa3OrbbbiB/xBG93iWX9Hq7797r7bdfr3eXu/T1XXFFr/fa1w7k3vjGQdrqe+Yz+3Xve1+fp67fdtte7/Wv7/UwJ6961UDHn//c69m5/N73BvWiA/O0zTZ9vj/+ca933nm93iabjMqJfOgYkn/MY3q9ddbp9d70pl7vOc/p9bbaqq9z2WVHda+3Xv98ePSjR+ukv3337dsIXTifttii1zv//H47jBvn2XOfO9wetZttNlwm+uwR593GG1fLPuxhvd6LXzwqd/DBvd4ZZwyX77FH375jjhmUv+xlgzRs+MhHhufpGc/o9ZZccljG2prK77rrcNv73384j7aPeESvh3nffvvha1/rPfzwXg/X3X//2+sdcECvJ3pwbeh1RrfBnH/wg8P96bVFy2KuEW65ZVge8yxyP/3pIC1lucfrruv1QtcUrmlci1/+cr8e9l144SJTet/4Rry/o4/u9f73v15P1qcNNuj1jj++1/vc50bboA7X7A47DOrud79BWsaw/vq93kUX9XpPfnKvt+KKvR7OjTEE3Ldx//YR93EGEigmQAdwDBdm6yr1wgxnXRaacRwf+MBq/fvvH5fRtubY9/GPx3XltF8oGdzQ3vGOUdvf+97RsjZtxA2rTX3j1oWLoaqPF7xgWOad7+xfQrF2OTpjbUvL4QCG2px1Vt/ZkLomzp3omOTxoINGx3XYYaNlsAlOeIltmJ+bboq3+cIX4nVV/eh/OKtk4SgiVMmddNKwDP5xeelLh8uqdOj6hzyk13v2s4fbX3NN35YW/9IB9JvLxS4PG8wvgVl7BDyOmcSj1qqgH81Y2Zz2uo08etJl05BeZZXhR6ZiM5b9cQY82tpii3H20K7uHB7y2FB6xuNWedwmZfa48ca2ZDz52Johrw5Ir/qRo5R1+YjH0DbErus612hq/uzjWmuHzj/oQTrnXIktOecetONVFR3wmkfKfi0bSq+44vCrHpCRVwlC8iyrTYAOYG10c9JQLwKxxXyaUOjxLJTdsRsF7JH3hHJtK5XP1TtuOdwgQnPR5MaRYzN4tdXHZpvl9NhMJudmbzni3TL9fl3Igqr6UJs6ZdY20YH+25oH0TnJY+h9yVAZbCq9RsEsxaZk7vQnbkttic3dE54wTDpkT6ztcMtwLsRrFu494dEuaCkdwAXFPwWd64WIF2E7E5ZyAEOLaarX0GKZku9KHc4rfW6JXU1uHKIjdcROU1t9LL10qqd26m6+uVqP5Ygxps4xaKyqr+41T8LaJq1gY6xOZLp8DM1LzAEs3d3E+Zk6R0v02Z2zkvUiZsMyywzPTMieWNvhluEcbLTtp/lcCY+yE6V0ADsxDR02YtYuvJIFcFzTknoME1pMU3Z0YTwp+/Qn/rQcziu7yKM+VKbbNU2Db1t9LLVUU2uq24ccDdvKngMYY5WDV/qPhu0zNx9jDRuneW0JXcMxB7B0nGCWalMyd/YTtfZcSc1jbO7ufvfhViF7UvYPtx7NhWxsom+0B5YsJkAHkKdCmoC+8Ox/k+mW3aytujFOwurUTT20+KVs0vOTkluouq45gODbFrNp3gEM3bTHcY7EWLfpiI/D7iqdIQcw9ri+9JqG4xVzvmAX2OUGu2aXtI3ZkOMAxtrm2I1zxj5taqIvp885laEDOKcTnz1sfeHZizJbSYcEu+4AlizQwFp6c5n0VHTNAWzT8eiyA1jl4E3qOog5gLAvVjfpc7ROf3pdlPaxHcDSaxS6U2yq5lbswdGu2SW2xGywj4BD9oT4aLtS6ZCNMVtSelhXSYAOYCWiORfQF55dTKYRzaRufCk2qR3AWXMA7SMo4RK7yTW5cYju1BE3F31Op2Sr6rriANobJs6hqvM8dNOuGm+d+hjraXcAQyxizEvPaTBLtSlZI+yabc+V0Diqyux5H7InNu9VulEfsrGJvpw+51SGDuCcTnz2sGftwost0tlAWhBMOYChxS/VZal8Stc46mIOYOwmN+7zDTer1M21hIG9EZa0zZVNnSuiwzKbBgcQNlq7ZTzTeoztAJaOB+dnik0T571kvYhdJ3ZXP+QAxtrmsICNtn2KR45OygQJ0AEMYmHhnQT0hWf/m7xTaIoSXXcAQ4tpCq+en5TcQtXFzhnYbRd52Bgqa9N28G2rj0l8CCT0rpnlYc8BOAhVTsKkroMYazqAdhYHeTCzczqoLXsH0PJvwwG07xWGzjXbr7a/Kh0ae6isSg/rKwnQAaxENOcC+kKO3cynCdGkbnwpJqldnVIHsGRBT9k0rrrYOYMFXZ9b0n+oTOrkGNMp9akjeLV1M+nqDiBuyFXneeimneJWty7Guk1HPGWbdVZSsk3r2toBjF0bYl+TuStZL2LXot3VD9kTm3cZQ+oYsrGJvlRfc15HB3DOT4DK4esLr8mNt7KjMQrohazqxjhGM+5UTQew74Tpc0vg6LmSMnts4ni16Xg0scOOKZZPnSvSxt4wcY6nznNwt21EV9vH0ByjDzgNsbo2bZjEHIm9sU8BS33uEddAik3pP4m635K2sWvROoAhnbG22pZYOnRupnjE9LC8kgAdwEpEcy4waxde6L/VSU9x6qYeWkxT9oUWy5T8pOtiOzA4r0I3iZzzrclNHbxy+sjhNIlHwKlzRWy044HzlzrPU3Wis62jtU304jyP1YlMG0f7idU2dMZ0pJzuWJtQOa6L0LUhsk3mr2S9wPyE7LDXdMieUDuxv+oIG+1mQxN9Vf3NcT0dwDme/Kyh60XaXvhZCjogpBePth7TNBlW6qZeskDDhlL5JnbXaWsXctGBOdHnli6XdOzYxAGE46HPh1gfOeUxO+zPb+XoismkzhVpYznihpxyRkI3bNHV9tHaJvon5QDG5kjsaPPY1toCZjFusLfkn0R7/ZWuFyF5uwMYOp9S9lcxR1t7jTbRV9XfHNfTAZzjyc8aur0Qsxp1TEiPIXVjbMNsfELOLrpWb+qmXrK4Q2/XF8YYi9Aij/HouUI+FJrc1HFDy+kj1K8ti9nR5s5gzodA7E0a53jqPE/V2TE2zcfOTzgNsbqmfer2sTnSMm2l23IAcX6mztGQw5U7hpL1BTaEzpUcBzBlf5Wt9nyG/CTOlSq7ZrCeDuAMTmqrQ9IXXpOLulWjCpVpu0MLWqG6pHiOA5hahFN1oY5Di2VIbqHKYrvGOK/0vIh9oTKpk2MTBwt89TktOuscY3bEyuv0kfpnQfTZcwAOQspJSNWJzraOsesN85Az103tmFYHMHWONpm/lF7LGvMT6ss6gKE1q8nc2vMZdpXYbcfBfJQAHcAoGlYsIqAvvCYXdVdwhha0Nm3LcQBT/YUW05R8aLFMyU+6LrUDGLI95xxrclNv0/GI2bHQj4DhdMUcL8z/uK8BfY7FPhiBedBri27TZnqS7wC2tQMILqnroGSNsHpC11yMN9qGziP7T13IniZzG7Kxib7Y+Fju6ADyJEgT0BeeXUzSLbtTq+0OLWhtWorFMeb05PQTWkxT7UKLZUp+0nUxFjivQrbruYrZGnO8YvK6HH3qc1rXlaZjduAcsLskpbpFPmcH0I4HDl7qPE/VSb9tHRfaAYzNUVvj03racgBxDdg51f2UrhF128KG0D8L9twOyeRcx9ounQ6tCykeui3TRQSm0QFc14/w8z5e6OOtPv7Nx/f7uJSPEtb1CX8VjcSni8Di4xP98Qwfb/PxAh//n48MmoC+kHVay3QtfdNNzv3wh4OboLa7rUU6Nmb0FXN6Ym10eWjx0/U23eRmYHWNI293C6QPLOihseYs9E1u6uClzwexp84xZUdbj4HrOIBw8FJOXuiGXWf8OW1iDiBsyJnrnD5SMqk5SrWrU9fW2nLooc494xlxC0rmz65FoWsu1tPllzv3vveN1o7bAQydF21ds6OjmeuSaXQAH+xnDHa/zseH+vg2H+G47eOjDU/xBWuo+DMlsJ5PH+fjKT4+yke0P8jHF/rIIAT0xTgtF+Hzn+/cM5/p3Lve1R+Ftjt1Y5QxNzmCl110S/QddliJdP4nAp/whDK9bUnHWGBO9Lkl/em5kjJ7bHJTxw0wpw/bZyifsqOtx8A5DqC9qcNBSDkJH/pQaDTjKTvqqLBeOoBhLijde2+/HXFBvP4HP4jXVdXYcyUlf+KJzh1++KiE/acudK6Fru1RTeGSkI1N9IV7Yakn4D+yOHXheG8xogRcKQ/y8fU+7iaFi4/X+KP/NyYY4DRe5ONbF9ee54+b+Agd31xcxoO+8Nq6cY6b6k9+0u/hs591bv/9h3sLLVbDEs1y4BVzenI0lzKu2gF8zWuce85z/D653yj/+c9zLGhXJsbiIx9x7hGPGO0Lc1YVmuyuHX10lfb8+pQDuPbazv3xj/m6rCR2WXAjPAX/n1aEG24YFsA/Oanz/EtfGpZfiNzBB0+m19QctW1BWzuAbdul9VWtF1o2lF5hhdHXG0I6S9cx3RfO39/+VpeE/1kclmCuBgHspM1CWNEP4l+BgXzXl13p4y993NbUb+rzJ5iyH/k8nMC7mfL5yOJGghuOdvp0etooyCIkR9jfxg7gqqvGSTR1AOOawzXH6/+FAiL3v79zz31uM6c0oDa7KOYAQsHZZ2erGRJs66b+9rcPqS3OxD5g8Cj/QGFbu9wUat9ww8IGSnxSu2uqywVNPvrR8e7bOlfiPQxq6jiAdjdtoG08qdDuWklP55wz6gCG/tmQNXf55Uu0x2Wn+T4UH9WC18yCA+jvcG4XHz+jaN7k01jdsQr7Z4Hupz5+zceX+yhhdZ+4QjKLj8hjVzR2h1/a1/l/ge6MLZ3dXmMXwmabOYdHhUccMbBGLmSU6PRAovspbXdTB/BFL/J7yn5Tedddw+NGXymnJ9xqfKXyvk7oRvPud1f3i3eANtqoWi4mgX5//ONYbb3ytm7q223nVwYsDYGw9daBQlMEx2ODDUyhz+L6wU5J3YDdMTjudQPOcX3O5+rBubLxxrnS3ZFL7RpXPYr/mr8tNP1HQEjUWVt+9jNpPZljXQfwJS/pbwystdbo+hbaARSH7cgjnbvPfZqPTfQ110QNikCXHMC9vV3+7pmMmyjbkVzTR2yBHOujflnhap8/wMff+Hi6j3v5+Ckfd/dRB/SnwxKLM7ZcZPbwietV/KdUzMQR/90hfPnL/SP+6guvzk1loGnyKbFXjrAgtFiVWIabJOJ++zm3/vqjLTWv0drJl4gDGHJK4Uh9/ONpm/AI+Z3vTMukatHvGmukJMrr2nIAYduTnxzu//3vD5fr0uWWcw7vSenwyU86t7r/37LK8dBtbPqxjx3dZbEyqTzO8To3+tA5kuonVIdH35MOyy4b77FqHu55T+ce/vB4+5Iavc7ktltttVzJduTqnBfoGZsDsfMjtQOINnW42NF2bV219k1pvksO4CGe4UMq4h8VZzh/WH1P83FnVR5L/spXPEBV+m0ch11AHXA1+tXTXaMLVXpfn8bjZon+36EZDPpC1xdeGxfyOHHhP/Bj8b/A4hCyV49H5EqOeidNcxId0K9lpHyhjmJLyFbY9NrXpi1De9GRlgzXol9xQsMS5aVN3gHUvaXGVeU4QA/a27EJ55z22hadhk6rV9fnpOvsRkFv6JrJ6U9kmoxbdJQeU/OI7+VMBbQNtZ/UOOR8SdnYZl1dB1Cfj/YcCf1TLTIYX9M1F+MXfW2yoK5OfQgEu3aIOQF7ynD+zvDxVT76u25lwCd9L1NScByfo/JI4rkPdgzvQCYQbvdliLMd9MWmL17ser3udd0d+z7+g9x650bGIUdYrsdTZyT6ZhFavKE/VF6nrzbayMKt7dZ6Y+Uig7E0GQ/0iw2is+mxzR3AmC05Tia4WH7CqokD0QaznPfRYKu+NmIsSsrbnuucvu0c6DZV8wAGMme2XWhnS8u0kQ713YbemI6QsxaT1eUpxiFO4miiXRvnWNN1W4+F6TsJ+NmZuoCdv5N8vNjH3Xy8l4/YydO7eTv4/Et9xI7ig3yE3Jt99C/X3Bk+41Pr+Li/j5B7tY87+ei9HIY7CeiL969/Hf101p2CHUh885thI/QYmi4kC3GDC48qr1Tsjd1oUgs7ekB9lUzKEvTbpH1Id1sOYMquKsdB7BK+khedue2lnT5Cp+jR5SXpHAfQ2o7rRF8rJf2JbFO7RU/JMXZuQ0fVPMDeUPuqncMS+1Kyob5T8k3rxDEr1aPPFWtzygG0sqX9inzTdVv08DhEoGJ/fEi2Kxns0uHlK8R/GqP83ebO8B6fgoPnv0vBne8jHLwv+SjhQp/AB0TwruAbfbzURziJES/C18xL0BetvfCuvHL6KOibmh1P6Wj0DU7r1Xpi5VpmUmmxV8+p7jtWLjJoLzqkrOQI/frmUdI2JtuWA5gae84OIOyzYxOdue1DYwRvqzcklyrLcQDh5NTdEYr1PUnHSa6z1PlZZU/s/K5yHGPjLy2X86W0XV35ug6gZizcxYaQAyhlGJ+Vl3Ylx6brdklfcyTrV4CpC1/wFiOmwlG+ErEqnOwFGnzEsUr9DNTbC2/SC1YThKGFx46nVH/OjblpH6U2peTFXr2Aa/lYucjEbpBSX3VEe7GhSja3fhIOYK4DYMcm10du+9CYodPqDcmlynIcwFAfYn9Kd6oupDMlX7cO/YjzmjqHq+xB29CYqxzHunbbdqG+rUybeWFWqjPFMaRTysA3tA6X9t+lNbXU9g7L+9lhIIEEgWm+8GThkSOG2XQ8qZtNAuOCVcnCHbvRVI0H7apkUoNr2j6kuy0HMDWu3B084St2CudpcABD49fXioyp5Dgpx0lzF+YhO7VcqB4MQu2r2oV01SkL9V1HT6yN1V93/UvxEGdP2yBl6L/pOQW9de3WNjE9QoAO4AgSFgwtiG1cvAuNVI+h6UKib5p2cZVx6v6kbKGOYq8crR2xcpFDfZWMyIaOYJS6eYTaVJW15QDG5g/95zpwdmzCKrd9aKxNmUNnnR3AFI+QnaEyyyMk00aZ7keYh/Sm6iCPMYdkchxZbUOo75yyNpjn9CMybTwCtjbL417pA0c6gJpGZ9N0ADs7NR0xrKnDNMlhWMdL8nKELU3H08aiP0kmYq9dtMWG0M1P6nBEfaytloul0baqj1jbWHnu7lysvZSnxpXbh9Uh+SYOIOZM5k1sLT3WcQDRh75WSvuEfNtzHbNB9yPMQ7JaLlYfap/Dv8kciy2hvqWujaOdT3HMSnVrHlbnJBxA22ep/ZQPEqADGMTCwjsJNHWY7lS0AInQotF0PDkLdqjfBRj+oi5l4Y7ZHSsXe1FfdRMV2dARbcWGUH2dsrZ2AFPjyr25W36Sz3UgQ+Nvg1mOA5izyxWyL1U2Dp2h/vQ5lZpHmY+QDpShbUgmZxw5MrF+pTzUt9SN49jGDqC1K+RUSllqbqyeVL7pup3SPcd1dADnePKzhm4vvEkvWFlGLhaytokjJkeI2fGU6Ies7kPr1Xqa9qF1NU3LAixHq0+Px9YhH2sXkg2VQb++WYdkSsvacgBTY891AK3twqtue+gDr6bMbrvNWjaaF1t1Teyc1jKpdFO7U7p1ne4nNA6RTdVBBvUhGa1fdNljGw5gqG/bT5v5ug6g5mGvm0nsAHZpTW1zPhZYFx3ABZ6Aznff9IYwyQFaWyUvR9gyiYVE9zfJ8Yf6koXbLtoh2VAZblB1bxrQh37bvsk12V3TY0zZhbpUvdaj08J5oR3AnB1AO742zls53zSTcaS17cI81I+WC9Wjbah9jnPXxlhDfYfsrFtm9cvOXKk+zdGeJ1UOYBtrbhs6Ssc8B/J0AOdgkhsNcRYuPL1gNR2PXVBDcHV/ofpJlslNKsfukF1Y+OveNKAP7cWGkP46ZZPYAYRdOU6AtV84N3EAwUzfcG0fOfkcBzA0PrE/p4+QTNtzHeoDZdrOFKtUHfSgXutCGULOOKp09zWl/4b6TrdoVlt3/UvxCOmUNQOM2lgPQ300I8HWnoCfHQYSSBCwF96kF6yEaZVVoYXHjqdSiRGYpvHDdLlJydEMpzKL8cpiXikcEED71M0j0KSyaNYdQPBqyizHAbR9YK5C10zlhCgBq1NVtZrU16FOl3aC6yJ0bYScY6u7Sb+iqw0doivnWHc3P8Qo1Z+sGaXji8k3XbdTts5xHR3AOZ786ND1RTgLF56+qdVdAKOwAhVdYiY3ZD2nAZOjRVj4ZTGPCiUq0G/pzSOhblFVWw5glV3CrsoeXS86mzymho46fWs7chzAuueE7semZfy2fJz5VJ/62g/ZgLYhDjn8Q+1CfaTK2tCR0m/r6l7LOTx0X9IPxlc1B7pdbC5LdGh9TCcJ0AFM4mHlyMU76QWrjSnQi0dT50yPX6e1nbo/Xb4QaVm4Y7ZW2YQFuYnTjH7Fhqq+cuvbcgCrmOTsAlmbRWeTR8Dg1ZRZjgNobUe+6bkbu4GH+mqrrEmfmC+ZM21PDv8m/Upfob6lro2jnc+613IOD22vOIBgZG3QcjYd49F03bb9ML+IAB1AnghpArNw4ekFSKfTI6+ubVNXdW/1JGThrnuzwoIsi3kdC8Cobt+x/tpyAKvsWkgHsMq2GBsp/9vfJDXZY+wGPk4rUn1WXaPgHGIt103K7lC7lHyoLmV7SL5pWV0HsHSs0g/GVzUHekyxfmbhPqTH2ZH0NP4WcEfQzbAZelGa9gvvzW92bsUVB5PVdDyazUBrd1OyoNa1G+1kMa8zSvCu23esv7YcwCq76jiAwrvJDiB05DggMT5Nyktu1qF+qpiG2jQtE+YhPVXjQduQzTlzH2oXsiFV1oaOEv0XXpiSjtfVPR8xvqo50L3G5rLpuq37YPpOAtwBvBMFE3cS0BesvfDGvWDdaURLiYMPdu5DHxooa+LMQEtXx3+PewzGqFOycMcWVi0bSzfdAYzprVuu3697xCPqaqmeyxwnwPYu50edtqILcybzJmXjOIqtoltf91JWerQ6S9vnyn/rW86ttJJzRxyRnseqMcUcwBz+Ta4pGee4eWHtO+UU6a3+se5Y0a5qDrRVsX7sfUi3Ybo2ATqAtdHNSUN78dr8tGFoupDUWbBXXnm8lJ7yFOeuvda5l798tB+5kYXsXmedUflQyX3uEyodLttxR+f22mu4DLmmvEc1Orf66s6tu65zcP5e85qQRF5Z7GYjravqRe7rX5fUwBkR7oOa/BT6zen7AQ/I1xmam+c/37n/+798HTmSofMsp12pzOMf79w11zj3qlelWdn1av31nXvIQwa9xVjn8M+RGfQUTuXwetvbwm2rSi+7zLmnPtW5zTevkqyu1+fzBhtUy4sExqfbSnnsGOMxjnUkZsMcldMBnKPJzh6qvghPOCG72VQI2hsCjMaOEh4Vx8KDHzyo0WwGpeHUK17h3B/+0L/J3vvezuHm03Z43OOc+/GP+99ZByfMBrlJWbvf8AbnXvYyKx3OP+1pw7uoIakHPtC5t7/duc02c+6jHx1IhHgPauul8Aj4T39y7swznYvtfOZotkxsm6p6kb/vfSU1cACXW67sxjfQ0G+n+951V107SGuZQWk4tfXWg/L3vc+5o45y7gMfcO55zxuUl+gbtHJu2WUHOTnfUGJ3QfV1NGhRltp9d+d+//t+G+lLjqLp9a+X1PDuExzmv/zFuVVXHdRjzKFxW52DFoNUqN2gdji1xhrDecnl9KMdqIc9TFrGjy99qXNnn93/RykuVVaj7cS5dPjhzh1zTP+fsJQmMPrBD1ISw3W6H11DB1DTaC1NB7A1lDOqyD4+KFn0xoUEuxbPfrZzV11V3oNtg4X5ppucO/BA52680bkttxzVeeqpo2U5Jdtu6xwW7Cc8wbnLL3cu5KDl6EnJ6PnYaivnzj/fuV/8YtBCbh52Yd1nn3wHBX28+90DnaEUZPCu5S9/6ZzesRiHA4i+4ARiTHpcH/7wqGUf+chomZRI2403lpLho2Y7XDOc03KiE8cbbuhH7WQNtwzntD5IvOc9YTnpS9fKfOsypLUzhn9GXvlK55ZZxkqF89/4RrhcSvX7jtp2XE86aBt0eUn6Gc8YdTp0nzgHDj00rFHORS0PhiGOoTKrtUom558TbYvVL3ndz557Smn8iJ3xhz88Xl+nRp9XsHmnnZzbbjvnzjorrQ22Y/3LDXqsuo3MnS5jujEBPzsMJJAgcPe7JyoXqAqPrvBf5bveVW7ASScNt8FNSW5gWLCxc2ODXpRyFmxpb2W1HpGJHfFYNyfYPrDLoedMFm4rZ/M5faVktD7pE/LjWLg1R53W/aLvDTd07p3vRCocxOYTT3QO58V664Xlqkq1DaITbbAztvzyzj396VUa0vV2XCKt+5KymJOldWh7pZ0cQ/Ml14fI2KPWp23Sfdo2dfNav+jQ/aNMy+jxSFrLI63lRWeO7aF20h5H3Y/0reuRrtIBGW2L1om6SYVYv1X2V9Vb+2P9cAfQkmolTwewFYwzpkRftPbC03ULPWy7m1fHHntzCy1Adcds2+mFvMrW3N1C2wf06jFI2srZfJU9VfVan07HbnxV+lL1Wr+MD/I6nWovdaIHTtoTnzjaXupFPnbUcjot8qV2STs5xtqHyu35LDr0uReyUeRCx5hOkQ3ZgbpYubSrcwzpTI1Hr19yLmp56NN5sSnUj9TJsUqmqh56Qn2LfjlqPTot9fYo47TlTfL6/CnRkzM+rS82Pj2PWp7pRgToADbCNweNZ/3CszsmoQVIL2I6XTX9VjakO6aj6qYba4dy3Y8s3LrMyqR05dbpser0OM4frV+PS6dht5YLjcPKW5mq9iKv5ap0SpuSo8yhbaP7lTp7PofKQ+1ELuQ86E9di5w+6jHr9rYfXafbl6R1X9LO9iPlsaPWgbY6L21izKUex1A7Xa/t0umYjC7XaW1LVZ+6XZtpbUOJ3ti4Yzpi8uNYR2I2zFE5HcA5muxaQ21j0a7VcY1GdWy1jlZogdVlsQUqZK6VLVlErV0h/SizfaBM2yt9WjmbR7txhHEs3Hp8Ol06JitflY/x0e10WuSbfvWQzKHok6Meu5TFnDWtI2Qj2seun6pzUevTOnS52Nf0GBpzSqe2R9JaB9IhO7VMTH+onZZtQwf06blrS6e2Myed029IT2m7mPw41pGQvXNWRgdwzia8eLj2wqta9Io7WOAGdscktADVHbNtF9IdG37VTTfWDuW6X+lTl1mZlK7cOqtf2slNV/JtHHVfMj7o1eU5/ei2kLe25urTekJtQt+jqNtU2aodAC0b6it23mgdoXZar03HdIqcHotmWNqP6EsddV8puVCdtk3qoS9kp+YlsvZYZUtVPfSF+k71kyMfGqfVWZrP4RHSmWOvbhdjZu9Dug3TtQnQAayNbk4aTtOFV7rYYArtzS20AGm9Ol16CpQsotYxjfUVskePQfrUZdAVahfrI6c8pm8cNyPdl06XjlG3zRljTEbr0WmRDzmAITmRt0c7LqkPlcfOGzkPpG3oGLMptqsoOrQd414vYjaKLfaoz7+QbbBd2y/tc3iF2kl7HHW9tkPL5IxH26J1aj3jTtftN2d82vZYPzF+ui3TxQToABYjm7MGdtEsvaC7jsveMEPj04uSrq9alLQsOGg9VVysYxqTt33YfuTmYeVsPqY/tzymr4pRrn4tpznqdMwG3VanrXxVXrfVad1O2yMyIQdQ6pocdb+ix57PUq5ldVrqU8eqc1GP2a4XWm9pv7qtpHVfUpY6antC5yJsCtmV00+onbalDR3Qp/XotO5Lp6vs0rK5aVlHcuVFLsdekcUxZrueRy3PdCMCdAAb4ZvRxvoi7PKFp+2sOxX2hhlasOr2Y9uVLKJVN10Zr+0D5XoMkrZyNi/66h5j+sZx/ui+ZHywW5eH8nZsuq2ty2kvbXS/Oi31oXcAq/qWtqljSAf6D51nWlanrf6Qk1S1A6jHHGpv+2iST9leRy/0aftFR6hM6uRYZUtVPfTk9KPnM0en2NfmsW6/OePTdsb6Gcc6ovud0zQdwDmd+Oxh2wtv3At8tmEBwTq2WUcrtADpRSyWDpgzsriHdIfaocw6pjG5ULnuR24eugxt9DhCOkrLYvrqzElJ33pcOp2jw9psbbX1MZ2631Cb0A5gSC6mP1Ye02HPabSvsjHWB8pD+rS81m0Zark20rqvHH3aHknLEe2hL6Qzxlb3GWqn63PSOTrkGoa+HLty+i2V0TaUtC21N8bD3odKbKBslAAdwCgaViwiYC88vXjOAiLraIUWIF1WsqBZ2ZJFtOqmm2Kv+5U+dRna2nxKX5O6cZ8vdecGY9JtQ2PMZaTlQjpDDmCov9KyUF/gHTp3tI06bfsMzVdIn22Xkw/pzmmnZUJj1vU2rfvUaZGDvhQPkQsdq9rpep3WumLlWkaPWae1zLjTdfstbReTt/ehcY93TvTTAZyTiS4apl6U7IUXWkSLlLcorO2EWpvP6cre3EILkNar01X6S2StLmuXrZd8qA89BknLMdVO6uocQ3ZAjz1/6uhOtdHj0ulUG6mzNtu8yFUddTudlnYhB7DUVtGlj6G+cH2Gzh3dX6gd9Mau7apHwLrduOc7ZrvmotPaNp0WGXAJ6QyVSRs5aqZSFjuG+oZsTj/yTxzkS/qEfFtB21CiM2d8Wl9sfOM+r7QNc5SmAzhHk11rqPbCiy1ktZS33KiObTk7gKWLWGxYlmVMDuXWrphsyDa9iMrCbeVsPqY/tzymr86c5PYJOT1Wa4PNW7223tpq6217yWs5nZb6kAMYkhP53KMeu24Tctia9BdyKHV/mptOaxmkc21IycXGbPuSvLZHp6UefZXq1G0lXfeYGqvolGsY+RxbQ+MUXXWP2oYSHTnj0/pi8uMYk+53TtN0AOd04rOHbS88m89WNAbB2GJR0pVd2EI6dZlOV7HQsrCpxAGsuunKGG0fKNc3CRmflbN50Vf3GNNXxahuf9JOj9XaYPPSRo66rZTpY1V7kdV6Qm1CHwIJyYm+3GNIB3iHzp0qG9FnSB/KQ/pQHgptzLe21faRqrOyyGt7JC1H1ENfaNyhMsjrUGqLbivp0n7a6FP6LjnW7be0XUy+ZO0sGdecy9IBnPMToHL49sLTi2dl4ykQEAdJTI0tQFKfs2DHZEvYldx0pb/QUcZnx1UyjpBeWxbTZ88f265pXo9Lp3P0WpttPkcHZHS7kA2T3AHEORY6d7Rd2t6cMYZ2FHU7fV63Md/aVt0P0qW26/baTilHX3V1puwU/VXHnL7lGoauHPkcmSq7bL22wdal8qW2xJi2cV6l7JzTOjqAczrx2cO2F15oEc1W1kFB+6g1tgDVMd0ufpZlSqe1KyZr+4CcniMZT0guprPNcm1Lm3pFl4wPeTtGm5c2ctRtpUwfq9qLrJbTaakflwMY6gt9hhxALZsad2i+QvpkbDjqNjqtZaycrdP5lH2h3VTd1qa1PZKWI2TRV6g/zcvqlHyOjMjGjjk6tH06HdM5jvK6/eaMT9sbky9ZO7U+ppME6AAm8bBy5LGlXjwXGo9dLGw+xz77n23VQlfSh5UtWcSqbroyNtsHynU/Mr6QnOho4xjjNu7zRY9Lp3PGZOWtrbY+plOPPdRmXA6g7lfbFtqx07IhG3V7m5ZzyJZLXnPT557Ulx5T/YVYav12bNoebae0gbxtI3VVx7rttN4cHZqHnketZ9xpbUNJX6X2xnjoeSzpn7JJAnQAk3hYOeRMAEdoEe0Kpjq22QXK5lNjiy1W0sbWlyxicAC33FI0xY+2Dysp45GjrW8rH7OjZMx1bNHj0ukcXTGbc9pqGa0nZEPIadFttK6SdEgHdshC/zxou+55z3gvda4h3Sa2c32Pe+Q7WtpWa2mIpZbRtuhypEN1YBjiGCqz+kqdoqWXthrCfVsp3U+KjW1Xml9xxXiLuv3mcIz3OqgZ9zoy6GmuUtPqAP7dz5J/2WUofsTM3No+/z0fb/bxah8P8nEpH3V4os+c4eNtPl7g4//zkUFftPbCCy2i00zM3rCqFjrNpmrcVtayTLWHXV/7mnOvfnVKKlwXmiNrS7hl+6UhW9rsRc9X0zHa9jYfs1vL6bTIr7qqpAbHkNyg1rm7313nRtPbbRd+dBlzANHfkUc697a3OfeUpwzr23DDfv4FLxguT+V++tNBrT6voWPTTZ3bbbd+PeQe+UjnfvKTgXxVSs+pla3zCPhZz+predObrLa+A5bqb7TFoKRqDgeSfefzwgud+8EPdGleWttX0mee9r7Uqac6d9RR8RbaCbVSOBdjoS179TkW64vlxQT8XWZqw17e8s8p629S6bv6NK60q3zc3Ef8y4uz26+CbhcfEdbz8TgfoePlPj7ex0/5iDbf9JEBBOyFN+4begn1NhYXu7DpxTZki+6zlIVlGdIvZdjFWWkl5z7/eec22cS5N7xBaoaP2h6p0WWSlqPItH2M6Q8x2n135z72sXYs0POl021oj43J6tZyOi1y73qXc3/+s3Pf/raUVO/87LCDc1/5inNPe9qgjU599avhupgDCDY77qg1DNI/+pFf8fyS97KXOfeYxwzKU6knPzlci10uOBMSIHfWWZIbHH/zm3hf9poctPL/wtv/4XWlT1v+OP+OPda50093brPN+sL2nLRtIBUq67ce/C0533Dtr7FGP4Ixxr/OOgNdqZTmEepz2WWdu+WWlIbhupA8bLka+ySREOpXRI8+2rnrr3fu+OOlZHDM4TiQjqfsnMUlWVNAwK8KUxtu9JZfrqJ2ALf25Rv4CHuvw2oAAEAASURBVMfuTB/x7+euPr7WxxV8RMBu30U+vtXH83w83McjfFz8r6tPMXTbAWxjfvTiCn1VC1ZVvbbJypY6gKLr9a93bvXVJVd9XHtt557+dOewG4NHbwipBfyhD+3LNPm78cbh1qGF+x3vCMvWKdXjsrxDj0JL+tD69tgj3jJlA1rh0dq3vjXcXuvWNX/4Qz+HHcCf/9y5d79b1w7SaB/SAQcw5CRpGwda+qnVVnMO59gKsjQqgVVWUZlIUs9xLvNHPzqizBfHbMU5ht3EkgDbwHKLLcK/kQxdsf5C/Zx8snOPeES/JnQeh+YE0nrnEucC2p54YqiH0TLNVKdFcs89JZV3DDl6dh20mlL1ON/wT2oopNg++9nO4R8GubawXml+e+/t3HOf69w558T/AQ71ybJsAtPsAL7Tj/IaH/HvJVZJ/a/hpj7/Rx8v9VGC/zfX+bPNyZ0KMidI5eIjZHAm++2XGQ9X+Y1O/Gf873+PDlRfhNZp0Yv9aMvpK7ELW2rBwug0m6rRWlnLMtXePpped92wtO0DUij74Q/7uzrSKiQndb/9bX+HSvKlx+c/37nYDT005tQ7aNL3/8t8G0PPlx1jyBES/TlHre8+94m30HLanniL8Hn0wAc697CHhVuJ06FrQ33hHbmQk6Bt1DpS6V//2rnf/S4l0a/DmoBdTjz63XbbanmRuOAC5w48UHKD4yc+MUgj9aAHOXfKKf1dvNxxyGNfOLY23HHHcElIZ6gMrZ7whP6O5g03OPeoRw3rQe7+9x8tQ4l+dxHnEnbA11svLGtLt9qq/w/dy/1+Bs6DY44Z7jtmq9UjeTjEBxzg3EYbSUncORaJ0Lkmdamj2PaMZ/SlXvrSgfSrXuXcTTc5t88+zl12Wf9+pO8vu+7q3He+47dy/F7Ove41aMdUawSm1QHEqrG9j0/y8RAfsYv3KR8lYLvkCsksPl7rj/B2ZCslJIM2eCy+qo+hAAdyBRWXDwlNRdnjHufci1/s3L77jpqrL0KdhuR1143KNynBDeZaTE2NgMUF9oWcjFx1emFGm7oLXag/WfykzrKU8tDR2iE3SiyKOtg+dJ1Op+RwQ4DzURJOUP87xR5TQl9ozClb0AaPIz/9aaSqg+ak02hpHSE8ghtH0OPR6dK+MA+xEHqHLtRX6hFwTLcu1/OFR5U5jynRBusIHv2GPugg+rVulMEBeuELpbZ/PPvsvsOnS9day7/Is7kuiaelj+9+17l//cs5eb9Rt7jtNp0LO+PDEsM5cF/eL/32fIPUuusOy0rOrjNSnnPEDjL+ofviF/u/EIR37uAQSwjZIXWx41v9LVN23iADHU99qnMPeIBzW289aIVzcq+9yhiBvQQ5R/E+M16B+JS6TWPdln908YTDjsPmRSePrRHokgO4tx+VX0mScZPFIz/AH0/20a8Wix7dYrtgJx/v6aME6LLBX7mL9Eu5lUE9gi3vl/pLxieuV/GfUjF1R/z3jYCbbSpY5wr/taVeFk7psnV4nILHOuuvb2vy83ivB/+JWztzNVjHp2rRefzjczWPLprWRrzflxtwM77xRuc+/vHhFrLADpeOP3ff+w76SNkgN+SBdHUqpc+21vNl29kdQNzcUsHaavXF2mo5nbbyp51mS4bzyywznNc57IDA6dBBj13KmzqAoqfkaLmVtLVjQN7uytvrJkc/9Ky8cljy1luHy60NqE3No7QOycQc4CYOoPSnj9pma4dd03Q7ndZcwRzn35/+NHxPOPhg597/ft2qOq1tkzTO3W22cW655Qbtdf9Sqsei01LPY6sEuuQAYifvIRXxj5HR/2pxuXgSl/u8/5diKGA1wKPdKxaXhmRW83X+GcqiR8uLxYYO2C7z/47dGf2/pjMY9IUXukh33LGdQWN7HwH/qdcJeHzwKz/12DW46KIyDW95i3OHHebfCn3tcDtZsIZL/WfEvcOM/8Kf9CRbE89rjpCyLG09ZLDg4tF8KJR8lUaofVUZXuLOeTwLPXp3LcYMcnbMKEPAhxxiIaXPttGylqfYeNBB/iNe3nF/z3ts63Te6otJaxt02spj111CyGlK7QCineyWiI6QfTEHMCQrevQxZJeuD6Vz24RssGXgZx3AXP2wzeoL2ZuzA5ijJzTX9p8O6V+/AyhlTY7aPp2GTjhaOUFzFeYh/jm6tIzoQpm1TdfF1gbRFeIrdTy2QqBLDiA+guT//UjG2yKj9ltAi8Jli4/4Vxsv0+hnPtjXvt3HM3xEgMxTF6UGfyBzuo/mJZE7BdD+BhX9lsyMh6qLtMnw21wU9WKWYxMeOey88+iL73bBEl14XIUPVugQkxUZW/+Sl0hN/Iivqih5j8r2EddcXYNHuXg3VEJKt3ZGUnKxeTn00PD7X+g7pU9sk6OWtTcMcQB32cW5X/wiviMU0iVlOUdtg06n2oa4VDmAetcVuu14UYadppgTgvquBcsLee0kwN6216BZdgDxWo9lqudc1+lzUJ9Llr9un5PWunR/aKvzoXmN2ZTTL2WKCXTJAcw1flMv+DYfN/TR35WdP+PdYT5+10fZBsILSuf6+EUf4Rxu5eN+Pn7ORzhwCJ/xcR0f9/cRO4+v9nEnHyE3P0FfcKFRhy7SkFydsqYOoF5oqsZh7dMLka3LzZf2CSdSv3jdhg25tubKaZv04xrbXpwrlOs2Vi7GCLrf/GYr3c/reQ1LDEq1rLVD24gWWnagIZ6y+mKSWk6nY/Kx8tQjYLS53/2GW4b6iu0ANr2pD/c8nIvN8bBUOGfHgLydpyb6Q73mPAIOtbNl1k7UT8r51txCdlhbJa9l9dquzw8to/sRHVVH3Ubrsu2q5jXV1upivhaBaXQAsQvn34J1J/kIJ+8DPsKx09sr/rsQ3LN8xI7hL338uo//5+NuPkq40Cee6eOWPp7l43t9xB3pmz7Od9AXsF4k2qbS1AHU9lQtJloWaT1GW9dWPtSHfi8pVF/adxs6Yn3KV8iE6rVzlbKhzvlTsvBrWZ2GzfZmbOtD49JlqXFpuVK9uq1OV+0APvaxWnrUUZJaPTf4NDVek9A7tiIXOpZeR9CR2yYkZ9khr50R6M85h2R89kuu0d6GtnYA7fmFfjR722+bec0t9zxF/1pWz4dmrtN1bNZ96LTVFZpXLa/Tti3zrRBYshUtk1XyO9/d4zK6vMjLPLtC7mRfv1GFzGxX60UgNNLQRRqSq1OmX4zG41g8/gx93UVMt7atahxWh15AbV3dPBZO7dROYgEbZx+5DqAes2VXOi9oXzImPY+2nb0Za1lrZyhv9YVkUJYrp9uHuFTtAL797c6df/7gVYRYv3rc+NoS6zhqO9pIh8aSq9eOAXnrgOTov/zy/nvAoa9msbbYHUBrg5WP5UMOYF1dsT5i5bofnY7JS7mW1eunZq5lpF3JUbfXaatD92/rkE+1DcmzrJjANDqAxYNkgwSBqsW16iJNqK6s0ro/5zdxEavsiSktbTeOxQW7ENoZCvURKouNaaHLrQOoHVztZGhH3tpcOi9oX+KoaVnL1t6gbb21tW6+Lb1VO4AYzxFHDKzUYx+UDu98tmWb1m/TuXMcssWWhcak1wnbt+Tx4aXcDzBZe0N9WrukH32s207rqErjuxhDQdun0yFZXaZt1hxKdGh9obTuQ6etrO7f1iHfpk0h/Sxzfr+dgQQSBHIW30TzZJV2lpKCkUq9gJTaOY7FRf8XDZNDfYTKIsPLKm5bn+7UOoDymA0yXXQA7c1G26jHFUvr8wkyuWxz5WL9Srm1X8pjx1i/etylOmN9pcott5SsrbP2YUy2rIl+218oH+MYkq0qa1MX+or9NJ/ux/JK2ajbjYur7kOnrV2la7Ztz3xjAnQAGyOccQXjWiSALbVzlIPV2mbzKR2xhSlWHtKFrxfRIccB1PIlfel2Oh37BQ4tU5qWTyvbnyHTDqBOpxz5Oot8CRd987Pt7A5gyfkBZlZfjKO2IbePkBx+DaEk6H51O+0A5o5B2ofskrrYsU4b0WXtw5jsddREv/STOlobIBsqS+mQurrtpH3uUfej01XttWyda7NKv63X/dm6SfRv+2R+iAAdwCEcc5g513+OBj8/he/UC4VxXqQpxyFkiy3TNwadtnKhfOzmGZKNleHnjfCTUBKw2FXpTS2IoqfkuPvuJdJ5sl/6knOXXDJ410xaaadPjzPlyJfOC/oqYaTtsO1KHUDb3uaFgz3mytl2Nv+Pf9iSdD7Wr3YANZ+0toWptWNA3jqAba9BsquG9yMRus6ob2X8r2ZYdb3psVbJxntM1+g+dNq2anterX7mKwnQAaxENAcC+LHtI48cDFQvKOO8SNt0AGGntnswmnAqJlu6KOpfZ4BOffMK9aHLdDpsZboUP5EV++WBdMt0LRbtNdccldEOoK5NzWMVz4c+VGvqp6va6Bb6BqPTkNGOEPIleiGfOz9aTqehoyTgN19Lgh2ftNWObxN7RF/VsZSr1mfnDHlb1kS/7kvS/+e/EAI/XfeNb/RLQoxe9CKRLjuGdJVpKJcu6VPL5nDV8rmW6TY6bduH+k/J2/bMNybAD4E0RjgjCmI38S47gBp9aDHR9TY9roUGDqD82Py4+rBjmVS+jgNYdf6ceGL/x96vvnowipK51Ix1Gtq0I4R8iV7I5wbbb047bcsHPuDcaqs59/KX57QcyMS+p1E7htaZGrQOp7RdYYnR0jptRItlh7y1ueocEl25R/wm9LveFZf+wx/6T0XiEvEaO564ZHs1mldV/7q+ba4yIt2HTku9HEP9NzmXRC+P2QS4A5iNag4Fx30xph4d5uDW9ul0TtvYwhQrz9EJmUnuAJaOOXcMMbmYA5iaxyob8Ru31vGpaqPt0zc/nYaMdoSQL9EL+dxzwfaLtiXhPvdx7nWvG/6d1Jz29kM60kaPO3cM0rbOsZSr7sPaB5a2rHRnVOuvk8YrMXWDtb2unpJ2JX1q2SbzlrJP96HTts2DH2xLmJ8wAe4AThh4Z7vTi4FctKH/0NocQFP9uj3s12OosrPpTTukH9xKHED9+Dikr2tl43AAMUY7bzaf4qDnUc5bkd9gA0n1jyV60cLqG9Y2yOXKDVoMp+q2H8cO4EorDduWk4vZkdNWzx/kwUKX7bVXercup49Zlyk5f7Rs6fWQy1H3oedS2v/2t/4nHPy756HfVddtRZ7HsRHgDuDY0M6AYu1gjWM4scfOdfoq1TWOhQY69YIX6kOXPfvZZb/7a7loXbZuHPmYA5hiX+cmU9ImxRu/baxDiV7dripdZx7asCW2A6gffZfahg8A4cuUv/3t4VF///vDeeS++13nHvAA50J1o9Kjv7sNGWsf5lOX4ZdMqr4fMdRXSVkbcyH9adtRZvMi1+axpA99veSs75tuWm6ptkenRdMmmzj3yldKjscFJEAHcAHhd6rr0CKYs0DgG/jrhtSjwxyd2uYcW7XO0MKk6+ukofN2/FLh4lDVB3YLjz3W/xjhltKi28eYA9h0HmOcrrjCuW99K81E39B0+lWvGr356vNl2WXTelEbs8u21P3qPqxc2/nYzpt+BKxty+n/If5n0X/nf2xpm22GpZ/lf1nzwAOHy57znP4vk+T+0gg+aIZf+vna1wZ6LGPkddkkeQ6sqp/StkNL7Jqp38Noy5I51val2GJd//3vnavzmFbbo/sbtZwlC0yADuACT0Cnu69yqj75SefwQvUHP1hvGKmdoxyNegGrstXqG8fCBJ3gkQrj6DfVX5t1sZtZUwdQzyPslTw+GLHRRukR6JuNlgxxxs7Wwx/u3IMe5FzOd+6FdOg+JJ0rJ/L2WLd9bAdQO4B1dVsbkV933VBpftkDH9h3Kl784kEbO3/I6x2/HEd9oK1eSs63eq2HW1neq68+XB/Kbbtt9boRaidlus/NNpPS8FHLptbMe9+77Gc5dW+6Dzu/Wo7pBSdAB3DBp6AjBoQWwVCZNvdtb+vn8J5OndCmA1iqq82FCV/HgvD0pzu3yy79NP7qhVBKQ2Wf+ERf9r3vFan2jyuuOND5ve8N0iWpmANYyr6qz9R5d/DBw631PKbaoRXsP+ss5847L/w4clhzeP6sDPJ6TnU6JCtlVbaKXOoY2wHUj4A1n5SunDrs+H3oQ84df3yOdJ6M5YU8nD58RQt2Cuu8k5jX82SkcK3hy9pPOCHe39e/7tzFF8frq2rA7M9/du6ww5x74xvT0vp80I52ulV17eMfP5DRc6rTAwmmOkKAHwLpyER00ozUf4htGNzUcdA30VJb21yYTjut/yh3p52cO/roAZlQH6Ey7HLh0bHeuRloaSe1/vrOffSj/d9LxbuHdQLe98JjIRuazqNloufV9vWmNw2X6BuaronpiMmjrW2j7VphBa19+BO7Wm5YKi+34YZ5clZKfwfk/e7n3AUXOAdHf5llBpKp8Q6k8lIYp/11mLyWcSnLTux94Qvjbdquydmly+0TXw6P9yfli9Qf+UjnfvObdGsw0B8eE2m8/5gT0B67q4ixsPPOzn32s8NPa/Dp+69+1bk2PmWN923xTii+1/Nf/xpYYed3UMNUBwjQAezAJHTCBHvzg1GlTlXpQNp8dFhqa5sLE3YAZTdUL+ShPkJl4DZO50/mpemvhhx6aH8XDV9ZooPecdLlddOhczGmSxyGWH1b5dtv79x3vtP/bVY8en3mMwea69rwpz85h1//qOsA6l+hOfnkvoOPHaBLLx3YFjvfBhILm7L22fwkrEOf2KnDDmeT8LOf9d/n/dWv+ufKDjvU04bzAd+PqXftU5pyzr9Pf9q5PfYYfoyPfyB+8pOU5vw6MBR+1147aFc6n9O+4zsY+VSk6ABOxTRNwEj9YQ65aEudKmvm2Wc7h59bwoKGhcY+Qmy6c6Ttw3+4JUHGWNImR1aPcVx95NgxDhm8k4cdAwn4LeSjjsr7mo511uk7O9I2dWzDAazD3rbReTjo8ssR1nYtZ+tieYwR7yIi1g3yCU3sTuKfEHk8ft11A405zsFAevIpsMMHDeAMIyyUvZtv3u8f52mdAGdNvtYEc1H1KDbUB/7BwJeCf/GLZY++c84/cF133VCv7ZetvHJ9nVXvUNfXzJYBAv6sYCABTwDvoUnA9zTht4G1gyV1crzoIkkNjnix/g1vcA7fpI9fdsDjj+uvdw67E7iBYqHSj5BC773gVzRyXtBHr3j0KgHfK1USYjeanMU01U+VA9hUf6rvSdfhfcfTT3du1VWre/7hD/u7Zr/+9agsHk/rUOIAap4l7XR/sbTWHZMpLZevv9DXQakOkccN/fzznfv736Wkf9Tvdo1jDMO9Nc9hZ1nCQtmLnSfsqP7lL2JJ3lHew8RXmzQNz31u/3rCJ7FLwkIxi9mI1xH239+5L3whJhEvpwMYZzOGGjqAY4A69SqxG7jees6lduhC76fsuKNzeNSAr3rALzyEwj779EtvucU5vesoss97Xv93aM88U0ryjyl7rZZxLZrz5ABapjp/zDH9d9HwXhACbmo/+EH/EWq/ZPAXu7f6p7m0I4dPI6aCduR1O51OtZ903ec/33+PEo/j2gh4L9PuuOhPzmo+bfQ3Dh3aRp0eR18pnfhi9tSrGLJLqHXgH6C3vKW/a6fLJ5ke11rWZAx4JabOI/DQb5A3sYNtkwToACbxzHEldvBCTp4gCTlvZ5whtekjHlHtuWdYBjtFCHgcUhpuuy2/RWzRbOo46HcA862pJ9nU1nq95rXabrv+LrK8F5RqhZvuvvuGJfCBhquu6n9xLD7laEObDoP9zrPYOWJtQP7JT+4/ysQ/P6mAfxAgU6I7pS9Upx3AcfYT6rtOmZ7DLtqLx8J4qhF6Xw7njHwdVp2xt9FG82tD30LqeMUr+r0/5SkLacXc9E0HcG6musZAq76Et4ZKh8dW+EJZ+6WyVlfo3Sg4PPZxl25X4gCOa9GcxA7g4Yf3XxDH10d0OdR1hq1ji0fMeNcw9J6nnkfdro4j8bnPOYcvkJbH1PJ9ddgNrwpwDs45J72DVKWjrXrtAJbsirfVf6kePVd6Pkv1jEse5xV+H1h/6npcfdXRq/nVad+lNtgBxKN42Qjokm0zaAsdwBmc1AUbUs5ChBvSj39cbWJIF3aJUjfjW2+t1isSIf2oi5VLu6pjlQP4spf1Ndjdpiq9uh5fN4OvWqj60lfdZprSj3lMvrXaYdAOYL6GgSS+DuSIIwaPqV/zmv73t+ExX1XAeaNtqZIfZ71+B7Dkn6Jx2pTSrbk1vf5S/dSta3pe1e03t11XHdNc+60cHsXrddTWM98aATqAraGcM0V1F2o4gGuvXQ0r9AGUqhfnS252de2vslzveoX6eMEL+i96V303WFU/+qZZJTst9Xg1AB8uKnkPyHKQF+hf8pLmo4bupz7VuVVWaa5rkhq0Q1ByTUzSRt2XnkOd1jILme6qA4j3SPHqQc5rFgvJj313loB/IYWBBGoQqLsowgHMeSwVcgCrzMQniHNDyDnLbZuS0/+5hvpA2cYbpzTMbx2+SiP3u8+EkmaMcxLvoeI1AXEERW6ejprJtDmA2vZ5mrM6Y5UP1NVpyzYk4AlwB5CnwWQJwLHL+QLoOg5gyUjGtdNQ5QCW2EjZagLaYYADiMef8+z8WWLT4ADqORzXdWm5lOTr/rNb0gdlSWABCNABXADoc90ldv9yHEDIlezolULVN53Stil57QCm5FhHApMggO9k63rQTt+4rsuuM6B9JLAABOgALgD0me0y5z/lXAfw1a/u/7pByQc7SsCO60ZT9Q5giY2UJYG6BPBJ5iOPdO6JT6yrYXLtuv5PU866Njla7IkEWiPAdwBbQ0lFWQRyHwFD2ZVX9n9bE99HiK/naDOMywHUN7Nx9dEmB+qaTQL4JHXJp6kXkgK+FxEfZsCnsLsY6AB2cVZoUwsE6AC2AJEqFhPIcXhKH+3i07KIbTuA+rFTmxNIB7BNmmW6eKMu49UVaVyLP/1pV6wZtYPn1SgTlswEAT4CnolpXIBB1F0U4QDefvsCGGy6zHFWTZOsLB8BZ2Eai1Ddc3IsxlDp1BOQnyfEL30wkMAMEqADOIOT2ukh4RFwHQew7Q+ExBzAWHkuVO4A5pKiHAl0mwC+eB5fuI6fNWQggRkkQAdwBid1IkOq6yjV3QFs+8Mgde2vgksHsIrQ+Oq5Azg+tvOqeeWV53XkHPccEKADOAeTPJYh2ptt6IfSQx3X3QFs2wGMvQNoxxUaQ6pMO4ApOda1T+CRj2xfJzWSAAmQwIwS4IdAZnRiJz4s/GTWqqvmdVvny2n32itPd67UuHYA+Q5g7gy0J3fJJc5de23eTwy21ys1kQAJkMBUE6ADONXTN6XG19nN++xn2x1szAGMlef2rncAm+rK7XPe5fDbwSW/HzzvvDh+EiABEvAEpvER8Jbebv+bT8H4aF+OsK6PIZmno1IFfEuq//FQd5uPF/jov3COYewEbrll7F1UdhBzzlZbrbJpUoAOYBIPK0mABEiABLpBYBodwFM9ujVMPNzn/+7j6T7q8BSf0bI/U5Xr+fRxPp7i46N83MfHg3x8oY8M4yRQZwewbXti7wDiS6df8Yr6vfERcH12bEkCJEACJDAxAtP4CPjfns7litDdfPq5Ph7iI3b9dLjGZ7SsrsNu30U+vnVx4Xn+uImPu/n4zcVlPIyDQNMPWrRhU2wHcOmlnTv6aOeOP965q64q74k7gOXM2IIESIAESGDiBKZxB9BCgvOHTx98wVb4/Hd99L8n5n7p47Y+6rCpz5ygC3z6Rz7CCYRTyZAicNZZo7VXXz1a1tWSmAPY1F7tAOITzwwkQAIkQAIk0EECs+AA7uS5wnG7WPG9yaff7iOcvmf6iN8Z+pqPL/dRAn548grJLD4ij13RVU25ZP32kFtBxeWlYqaPD3nI7A2vygGsqo8R0Y+A//OfmBTLSYAESIAESGBBCXTJAdzbkwh9cEOXYXdOh7V85mk+fl4X+jS2og7w0f+I7KL3AvEdIp/ycXcfdbCPjJdYXGnLpc0ePnG9iv+Uipk+vvGNzs3ad6zF3gGUiaz7mFo7gNwBFJo8kgAJkAAJdIxAlxzAQzwbbDWl4h8Nv1f5PN7zw6PeqvArL/AAJYR3A7ELqAM+AoptG+gMhX194YoqwgGd/bDUUs5tsUXeOPFdgFXOVZ6m6ZTSY8evnjCQAAmQAAmQQAcJdOlDINi1K3mJDLt1cACP9vEOH6sCPul7mRI6zaefo/JIbu3j6T7G9N3u6xDnK+C9Nu3YpEZ/4IH+obt/6l7yE0r77ednwk/Fgx6U0txu3YUXpvXVfQQMZ1nC3e8uKR5JgARIgARIoFMEuuQAloJ5sm+wno/28S/07OAjnLgzfcSb+HD03uzjO32U8BmfeJOP+/v4OR839RHvE77ERwZN4G7+MzG5DtH22+c7i+hjdb8Ju+uu3vUv8f21cTXTW22Vblj3ETBYfcafWjff7L+AaI10H6wlARIgARIggQUiMM0OIJy1U33E17eEwnt84To+4jnc+T6+2scv+SgBW0D4gAjeFfQvublLfYST+E0fGTQB7ADmOICPfWyZ84c+xNHSn57VfZemd97ZudSvhqCff/xjvL8c8brXlVpNeRIgARIgARKYKIFpdgBfmiB1lK9DrAone4GNqoTmvj7XAcxxEmMwcx3AjTf2v91yRkyL/0pvPOlPBDyWzfnZsCZjSXTPKhIgARIgARLoAoEufQikCzxoQ4hAySPgUPucshwHEJ+wXRGfwUmEZZZJVPqqXMdOdibT2lhLAiRAAiRAAlNJgA7gVE7bhI3O3QFsYlaOAwhHtMrBq6rPdQCbjIVtSYAESIAESKDjBOgAdnyCOmHeJBxA/f15sUHDjioHr6o+ptuW01G0RJgnARIgARKYIQJ0AGdoMsc2lNxHwNpp0ukcw3Lk4QDit3pTocoBzP06Gz4CTlFmHQmQAAmQwJQToAM45RM4EfPr7ADmOlolA4BTVuXgVdXnOJolNlGWBEiABEiABKaQAB3AKZy0iZucuwOoDct1AEt22vDTalVfrqwdwA9+UFvUT+c6gLlyoz2whARIgARIgAQ6T4AOYOenqAMGYgcwx6HTTpP+RYy2hgAHsOoRsHYQ8RvGDCRAAiRAAiRAAiME6ACOIGHBCIE6j4DvcY8RNY0L4ADqHb6Qwqr6HEc2pJdlJEACJEACJDBDBOgAztBkjm0odR4BL5QDuO66AwzLLTdIS0rvUkoZjyRAAiRAAiQwZwSm+ZdA5myqFnC4dXYAl1++fYNzdgDxiPjGG/tf+Bz6ahk6gO3PCzWSAAmQAAlMHQHuAE7dlC2AwbkOoHauFsoBBB7sPmL3T9sj2EJlUqePj3iEzjFNAiRAAiRAAjNFgA7gTE1n5mBKPnkLlbmPgLVzNa5HwFUfAtEItD1SHiqTOn08+mjndt7ZubPO0qVMkwAJkAAJkMBMEOAj4JmYxsJBlDqAuTuA2oxxOICwW7/jp/sLpUPOXqgs1HaNNZw77LBQDctIgARIgARIYOoJcAdw6qewxgBKHcDcHUBtyv3vr3PtpbfeOu8raWI95jqAsfYsJwESIAESIIEZIEAHcAYmsXgI+DBFSaizA/jud6d7eOQjnVt2Wee+/vW0nK7Fd/zBgdtrL13qHN7XwyPbM84YLg/l6ACGqLCMBEiABEhgzgjQAZyzCV803NIdQDiAoU/UWnbaucIj4I03HpZYc81BHu/X3XCDc0984qAsltpyS+fueU/njj8+LPGf/zj3ilc4t9FG4Xpdqm3U5UyTAAmQAAmQwBwR4DuAczTZdw611AHEI2A4gTZgB++WWwalVc7VOec4t/LKfflttslzKiF94onO/fe/A3lrPxzA3LD99rmSlCMBEiABEiCBmSUQuKvP7Fg5MCFQ5xEwnEAbDjzQuZtvdu6tb7U1/bx1CFdaybk77ui3WXHFcJtYqd6BtA4gnMOcsOmmzoV+HzinLWVIgARIgARIYIYI8BHwDE1m9lCsA1XVELt/IQcQ5W95S1Xr4Xq0KXX+hjU4Z+3P3QHcYYfq3xK2fTFPAiRAAiRAAjNIgA7gDE5q5ZCsA1XVADt5oUfAIaewSldp/UtfWt0i1wHEI2sGEiABEiABEiABRwdwHk+C3EfAr32tc7/7XZ9QyNmzTqF95NuU7SmnOHfEEaNarANLB3CUEUtIgARIgARIIEGA7wAm4MxslXWgYgPdYgvnHvWofm2OAxjTU7d8883DLa39ue8A4ufhGEiABEiABEiABLgDOJfngHWgciDY3T60CZVpXfe73yAX2skb1JalrP3cASzjR2kSIAESIIG5J8AdwHk8BXIfAWs2oR3AZZbREv0vadYlBx/cdxJf/WrnttpK17SbpgPYLk9qIwESIAESmHkCdABnfooDA7Q7aAGRkaLQbp91AG2j1VZz7stftqXN89b+3EfAVfY2t4waSIAESIAESGAqCPBDIFMxTS0baR2oHPU5O4A5elIy97pXqnZQZ+3P3QG07QYamSIBEiABEiCBuSJAB3CupnvxYMf1CLgpS3ziGL/3WxWsI5frAK69dpVm1pMACZAACZDAXBCgAzgX02wGaR0oUx3M5jwCbvo1MGut5dzrXhfsPlmYM54Xvaj5F1AnjWAlCZAACZAACUwPATqA0zNX7Vk6rh3Apg4gRljHthwyD3xgjhRlSIAESIAESGAuCNABnItpNoPM2TEzTYI/Bbf00laqeT7HAWzL/ubWUgMJkAAJkAAJTCUBOoBTOW0NjIbz9Oc/lyvIeQRcrnW0RY5zlyNjNYfstzLMkwAJkAAJkMCcEKADOCcTfecwDzjAuSc96c5sMqHlJvEpYBhTx7lLDmJxZcj+nHaUIQESIAESIIEZJEAHcAYnNTmkPfdMVt9Z+ac/OYcPZUgI7aDZR8CTegewjpMYsl/GxiMJkAAJkAAJzBmBLjqA7/ZzcKqPt/h4XWQ+8H0e3/PxZh+v9vEgH5fyUYcn+swZPt7m4wU+/j8fbXiDL7jQR8hA1v/47ZSFO+4o++BE7lemrL76MIjQDlobDt9wL3k7gHUcwJD9tm/mSYAESIAESGBOCHTRAYQjd6yPn47MwV19+Q98XM7HzX3c3scX+vgJHyWs5xPH+XiKj4/ycR8f4SRCTsJ2PvFJHz/sI2Qg+0Mfp+fL4m7zfuuaazr32Md6szND7q9m3MWcGncF9orQhkM4rg+BcAewYvJYTQIkQAIkME8EzF2+E0N/n7fCv6jm/hCxZmtfvoGPL/fxTB9/4uOuPr7WxxV8RMBu30U+vtXH83w83McjfNzNRwlv94nP+4g6yED2Yh9f72P3w29/2//S5Kv9Bujpp7dvr3XmUg7gu97V73/ffZvbkeMA1umFDmAdamxDAiRAAiQwowS66ABWod7UC/zRx0uV4I98Gt9JsvHiMsicsDgtB8hs4uPdfMQuI2StDPKb+dj98LznjddGuwOYcqDg+GE3chPgbRjuec9qBXUeAd/3vtV6KUECJEACJEACc0JgySkc5+re5iuM3df6/L99RB1CSAZtMN5VfVzCRzzTtHqQFx0+ORLgZCJKWF4SEz/eeGNel5dd5tznPufca17Tf1yc18oTAiIV7A7gFuZ1SfuBENW0KLnHHt699/79S18ab1biAH7rW879/vfOPe1pcX2sIQESIAESIIE5IzCpHcC9PVf/BXTJWLJ9BF02wGPR5ToNWfFodLlOi4wtQ7kE752461X8p1RM/JjrBGGn8H3vc+65zy0zscoBPOmkMn250iut5N/w9K94vuxl8RabFWzSPv/5zu2996hDG9fOGhIgARIgARKYeQKTcgAP8SQfUhH9tk9WuNxL2V26lX0ZHu3Kjl5IZjVf/x8fr/ERnxz+r49WD2REh0+OhH19yYoqrjUiMamCXAcQ7woinIEPOSfCN77h3NOfPhCwj4DtDqCtH7Qcf2rbbZ075pjx98MeSIAESIAESGBGCUzKAYTD9aeKeFsm49O83MN8XEPJ44Mht/soXg5knqrqkYTM6T76701Z9LgYslYG+VN9jAX0cYOKmc9hY+oalLf9YYkXvtC5Zz97YJDdAcTOXFcCbNtuO/8m51JdsYh2kAAJkAAJkMBUEcA7cV0L+BqWVXzEEe/pbegjwl99vMnHE3w818cv+vgOHyG7n4/+RbdFzpk/uM/4+CYf9/cR5Zv6uJOPL/FRAuqgA04hHMadfUSfaNv9YB1A7Ahap610FHpX0erC9+hd7jdWt/Z+tH3/r7SftuTxwZR/49VPBhIgARIgARIggRICXXQAP+AHsIMaxJmL00/yx5N8xKPbZ/n4KR9/6eOtPn7Fx918lHChTzzTxwN8fKOPl/r4Zh+/6aOEr/kEPnK6l49r+IhH0GjzDx+7H7SzBmvhENrHtE1GEXrEe+979z9Q0URvm22//33n8I7jIYe0qZW6SIAESIAESGDmCcgHI2Z+oGMa4Ape7/U+uBVWQHKCATty+lc98Isgoa9q0Tt5qV1C1B18sHeT4Sf7AIdSt+2Xdu8v7Aw5q92zlBaRAAmQAAl0hMANN9zgVlwRr/Qveq8fr3bNXejiDuDcTcLIgE8+2e91ntT/Xr1nYbMzEEI7gAGxoiKtcxqcPwyOzl/RFFOYBEiABEiABEBgUh8CIe0SAiee2P/qEnwdSixoZw0yNh9rx3ISIAESIAESIIG5J0AHsIungLzLl/rdXuvw2XydcbWho06/bEMCJEACJEACJDBRAnQAJ4o7szN5rGk/6aubW2ctJavbpdJWZ0qWdSRAAiRAAiRAAlNLgA5gF6cuZwfQ2v3DH9oS5kmABEiABEiABEggSIAOYBDLAhfWcQDx6xhNA3cAmxJkexIgARIgARKYCgJ0ALs4TXUcwDbGQQewDYrUQQIkQAIkQAKdJ0AHsItTJA5gG+/16fHRwdM0mCYBEiABEiCBuSVAB7CLUy8fAkl9CriO3bdV/NwyHcQ6VNmGBEiABEiABKaOAB3ALk6Z7AC27QDedFN6tHQA03xYSwIkQAIkQAIzQoAOYBcnsq4DCAfuJS9x7pWvDI/q5pvD5SwlARIgARIgARKYKwJ0ALs43XUdwCuucO6YY5z74hedO/NM59Zaa3h0b3zjcN7muANoiTBPAiRAAiRAAjNJgA5gF6dVHMDSD4Fo+d13d+6SS4ZHd9xxw3mbowNoiTBPAiRAAiRAAjNJgA5gF6e17odApB3GdNVV5SOjA1jOjC1IgARIgARIYAoJ0AHs4qTJDmCTD4H8/vddHBltIgESIAESIAES6AABOoAdmIQRE+o6gKUOo/QjBnAHUEjwSAIkQAIkQAIzTYAOYBenVxyzUoeuVP5udxsePR3AYR7MkQAJkAAJkMCMEqAD2MWJFQdQf6gjx8477siRGsgstdQgzRQJkAAJkAAJkMDcEKAD2MWplg9zlO7o/fvfZaPhDmAZL0qTAAmQAAmQwIwQoAPYxYmUHcBSB7DpDiAfAXfxbKBNJEACJEACJNA6ATqArSNtQeGkHEC7A9iC6VRBAiRAAiRAAiTQfQJ0ALs4R3UdQD4C7uJs0iYSIAESIAES6BwBOoCdmxJvkDiApR8CKXUAl1xyePR8BDzMgzkSIAESIAESmFECdAC7OLGhD4GccYZzW27p3G9+E7e49B1APgKOs2QNCZAACZAACcwwAbMFNMMjnaahyQ6g/hAInL+bbnLu8Y93Luboxcox9vXXd+6vfx2mYL8GhjuAw3yYIwESIAESIIEZJcAdwC5ObMgBhPOH8J//9I+hv6lHwN/5zmgLuwNIB3CUEUtIgARIgARIYAYJ0AHs4qSGHEBt53776dwgnfr9X7vbh1arrz5oyxQJkAAJkAAJkMDcEOAj4C5OtTiA+BAI4gknDFv5jncM5yX3vvdJavQoOnXN3ns7h13DV7yiX8odQE2HaRIgARIgARKYWQJ0ALs4tfpDIF/6knM77NDcypADuPLKzh133EA3HcABC6ZIgARIgARIYIYJ8BFwFydXnDV8COS7361v4TbbDNrar3xBzRJLDOqZIgESIAESIAESmBsCdAC7ONXaAazrpEHH2msPRrfMMoN0LMUdwBgZlpMACZAACZDATBGgA9jF6dQOoDwOrmPnFVcMWuFxrw3WKaQDaAkxTwIkQAIkQAIzSYDvAHZxWsUBxAdAmjiAl18+GB12EvFhj2OPdW6XXZy7172cW221QT1SG200nGeOBEiABEiABEhgJgnQAezitIrTh3cA6z4CRrtrrhke3VFHOXf44c6FvhIGknhn8Mgjndt44+F2zJEACZAACZAACcwUgS4+An63J3yqj7f4eF2A9iN92Vd9vNjHW308z8e3+KjDlj7TC8QHayGffqGP5/p4++Lj8/1x4YPsAMIBFGewjlUHHeQcvuz5Yx/rt4ZTGHP+IIH6HXd07uEP78vzLwmQAAmQAAmQwEwS6OIO4FKetH9O6U7zcacAdWxPXeXjy32EE7iZj5/10XtL7hAfdXiQz9ygCtBOwqY+8TUf3+vjt32E8/d1Hzf38dc+LlxowwGEM/ekJ/nR++Hbd/0WbmTsmQRIgARIgARIoAMEuugAyrcZ7xjhc4Qpv8Dn4cy9wEfrAF7py0K7iL7YvdXHH/u4LzI+4PhEH1H+Eh8XLmgHEI5ck0Dnrwk9tiUBEiABEiCBmSTQxUfAdUCv6Bv9K9DwTF92mY8/9dFvhw0FOI3mJzbcj3wZdhRjYWlfsYKKy8cEG5WLA4gPgTCQAAmQAAmQAAmQQMsEZsEBhCP3Yh8PU2zg9O3sI97xw87gn32EE/gEHyXgh3DV96QsKkY+9QO5e/j661X856JWbf+R9/7uuMO5o48Oa//854fL9Xf+DdcwRwIkQAIkQAIkQAJDBCblAO7tew19KEOXbTJkWV7moV7sOz5+wEc8zpUAh+9zPv7Ox9N8fIOPP/BxNx91QP864HmrLdP1+/oMdhslrqUrW0vLDmBK4bLLDtfaR8U2PyzNHAmQAAmQAAmQwBwTmNQ7gHg375gKzn+vqLfVG/iCn/kIR+9DtjKQ/5UvwwdHJOBL8uxuH74Yz+4KijyOty+Ouqz9dI4DuNxyw/3S4RvmwRwJkAAJkAAJkECUwKQcwKu9BYhtBez8wfk7ysd3Zyp9lJfDo2EJ2Bl8qo8HSIE/bu0jvoJmYUOOA2h3ABfWYvZOAiRAAiRAAiQwRQQm5QCWIFnbC6/iI4539XFDHxH+6uNNPsL5O9FHfIBjfx9lF++/Pn2Vjwj4JO/ffTzHR3ytDHb+8D4gooQDfeLnPr7TRzxGfp6PT/Fxcx8XNuQ4gHYH0FrMHUFLhHkSIAESIAESIIHFBLroAOJ9vh3UDJ25OI1P8Z7k44t89L9j5l62OPrDovAP/3fdfnKR07efT9/HR3xZNBzBZ/l4nI8SsNO3vY8f8vGDPv7Nx+18XNjvAPQGuCUrpgXOnf16Fzp8IMdAAiRAAiRAAiSQQcB7EgwNCOArYa73wa2wApItBXz9S2oX8O53d+5X/pXGRz5y0OF66zl34YWDPBzEW+H7MpAACZAACZAACWgCN/gfSVhxxRVRhD/+FxPmL0zqU8DzR7bJiPE1MPgJt1iAA8gdvxgdlpMACZAACZAACVQQoANYAWjBqpdeOt413v+T7wqMS7GGBEiABEiABEiABIIE6AAGsXSg0L7jp01aeeXRHUDuCGpCTJMACZAACZAACSQI0AFMwFnQqtQO4EorcQdwQSeHnZMACZAACZDAdBOgA9jV+Us5gMv7nyC2O35V+a6Ok3aRAAmQAAmQAAlMnAAdwIkjz+ww5QDecsuoA2jVWofQ1jNPAiRAAiRAAiQwtwToAHZ16lPvAF57bfUj4DXX7OrIaBcJkAAJkAAJkMACE6ADuMATEO0+tQN43XWjO4B2x+87+HETBhIgARIgARIgARIYJUAHcJRJN0pSDuDHPla9A7jBBt0YB60gARIgARIgARLoHIGK3xzrnL3zY1DsEfBFFzl33/v6Xzr++/yw4EhJgARIgARIgARaJcAdwFZxtqgstgMI5w+BXwTd58C/JEACJEACJEACxQToABYjm1CDmAMo3dt3/mxe5HgkARIgARIgARIgAUOADqAB0pls7BGwGEiHT0jwSAIkQAIkQAIkUEiADmAhsImJh3YAzzxz0L19BEyHcMCGKRIgARIgARIggSQBOoBJPAtYede7Dne+yy7ObbjhoIwO34AFUyRAAiRAAiRAAkUE6AAW4ZqgsHXw/ve/4c7tDuBwLXMkQAIkQAIkQAIkECVABzCKZoErrIPX6w0bZB1Emx+WZo4ESIAESIAESIAE7iRAB/BOFB1LWIeOO4AdmyCaQwIkQAIkQALTS4AOYFfnzjqAe+wxbKmtH65ljgRIgARIgARIgASiBOgARtEscIV+BPyEJzi39trDBtEBHObBHAmQAAmQAAmQQDYBOoDZqCYsqB28JQO/2KcdRJim5SdsKrsjARIgARIgARKYLgJ0ALs6X9qh02mx15bZvMjxSAIkQAIkQAIkQAKGAB1AA6QzWb3D9//buxdgu6r6juMnJb2hbR7CFAKBQNQqWA2BqkQyFEUgjGNbQaRgdWpqVQxTWxRsRKYkaErEtjwEm7EtUzAK4aFTiwrh0SZpG2DkWRSoGrmOMQEL5CESEmzT3+/evXr/bM8593Uea5/7XTO/7LUfZ591P+vk3v/d53HrFXdxfzaDZiAIIIAAAgggUAUBCsBcZykWfbGfxltvW9rHEgEEEEAAAQQQaCJAAdgEp6u7YoEX+2lQXAFMEiwRQAABBBBAYJQCFICjBOvY4bHoi/00gPK2qVPTHpYIIIAAAggggEBTAQrApjxd3Bmv8JWLPQ8r/mWQKVNqtWuv7eJguWsEEEAAAQQQqJJAnc8XqdLwe3isseiL/fQlxwJx48Za7aCD0h6WCCCAAAIIIIBAUwEKwKY8XdwZi77YT0Pae+9a7cILa7WdOyn+kglLBBBAAAEEEBiRAAXgiJi6cFC8wlevAPSQLrqoCwPjLhFAAAEEEECg6gK8BjDXGYxFX+znOl7GhQACCCCAAAKVEaAAzHWqYtEX+7mOl3EhgAACCCCAQGUEKABznaqRPAWc69gZFwIIIIAAAghkLZBjAXiBxDYozyvbGujt0fZyPlw6dq7W1yl6l0Ttx4reMVGbpMR2mlYeVXYVy1Pjzq7241W/2O/qoLhzBBBAAAEEEOgFgRwLwD7B3qSsHAb4j7T/wJBrw/HT1b9D2ay8UfmIcp7yMSW1Y9S5QVmlzCuWN2o5X+l+i0Vf7Dcb2XR/2TQEEEAAAQQQQKC5QI4F4FIN+TLlkeZDH7g6+KSOSfGVvtTeo44+J6W2SPm28lXlYsUFYLoKeI76LhJXKI8Xy7u09Pbut7E8BXyHvpwjjqjV7ryz++NnBAgggAACCCCQrUCOBeBIsa7SgU8r31L89G/8Wnx1b53ip3ZTW6POLGVOscHH3F7008LHLEgrdZb6kxs1X2ZLmVbnmNZsilf9Yr/Z2Y8+ulZ7+OFa7YQTmh3FPgQQQAABBBCY4AKxaKoSxV9osKcrJyqrlb9RPqmkdoA6T6WVYpnWvc+t0TFp/+BRL/33fK1uD9n00t0tXItFX+y38C44FQIIIIAAAghMTIFOFYDLxFt+00Z5/Q2jmILlOvZu5SHFxZ/f4PFxJTafP7b01G/cHvs+1seUt8Vz+OniGSEHx50t7ceiL/ZbeiecDAEEEEAAAQQmokCn/hKIn671lbpmrb/ZzmH23aP9flp2puIrfU8q5St5+2ubW7oS2OiYtH/w6Jf+u0urTvvbWF4D2P5RcQ8IIIAAAggg0AMCnSoA/Vo9p13tKJ34BSV9bIyvDvpNH35H8W7FbaGyWelX3HzMSYrfcJKaj9mQVrq6jFf9Yr+rg+LOEUAAAQQQQKAXBDpVAI7G6hAdvK/i5V7KkYrb95XnlN9VfHXPBZzf+Xu88pfK3ynp6tx16i9VrlEuVl6l+DWCn1LSU7xXqL9eWaJ8TXmHcqJyrNL9Fou+2O/+yBgBAggggAACCFRcIMcC0EXa+4Lrg0Xfhd5a5UXlbOVSxa9h/IHi1wB+XknNb9Tw1T1vu0/Zqvh4JzVf6TtTWa58WtmonKHcq3S/8RRw9+eAESCAAAIIINCjAjkWgItk7TRqt2mHM1zz5wgeN8xBN2u/k1+LV/1iP7+RMiIEEEAAAQQQqJiAr6DRchSIRV/s5zhWxoQAAggggAAClRKgAMx1ungKONeZYVwIIIAAAghUXoACMNcpjFf9Yj/X8TIuBBBAAAEEEKiMAAVgrlMVi77Yz3W8jAsBBBBAAAEEKiNAAZjrVPEUcK4zw7gQQAABBBCovAAFYK5TGK/6xX6u42VcCCCAAAIIIFAZAQrAXKcqFn2xn+t4GRcCCCCAAAIIVEaAAjDXqYpFX+znOl7GhQACCCCAAAKVEaAAzHWqeA1grjPDuBBAAAEEEKi8AAVgrlMYr/rFfq7jZVwIIIAAAgggUBkBCsBcpyoWfbGf63gZFwIIIIAAAghURoACMNep4ingXGeGcSGAAAIIIFB5AQrAXKcwXvWL/VzHy7gQQAABBBBAoDICFIC5TlUs+mI/1/EyLgQQQAABBBCojAAFYK5TxVPAuc4M40IAAQQQQKDyAhSAuU5hvOoX+7mOl3EhgAACCCCAQGUEKABznapY9MV+ruNlXAgggAACCCBQGQEKwFyniqeAc50ZxoUAAggggEDlBSgAc53CeNUv9nMdL+NCAAEEEEAAgcoIUADmOlWx6Iv9XMfLuBBAAAEEEECgMgIUgLlOVSz6Yj/X8TIuBBBAAAEEEKiMAAVgrlMVXwMY+7mOl3EhgAACCCCAQGUEKABznap41S/2cx0v40IAAQQQQACByghQAOY6VbHoi/1cx8u4EEAAAQQQQKAyAhSAuU5VfNqXAjDXWWJcCCCAAAIIVFKAAjDXaYtFX+znOl7GhQACCCCAAAKVEaAAzHWqYtEX+7mOl3EhgAACCCCAQGUEKABznSqeAs51ZhgXAggggAAClRegAMx1CuNVv9jPdbyMCwEEEEAAAQQqI0ABmOtUxaIv9nMdL+NCAAEEEEAAgcoIUADmOlU8BZzrzDAuBBBAAAEEKi9AAZjrFMarfrGf63gZFwIIIIAAAghURoACMNepikVf7Oc6XsaFAAIIIIAAApURyLEAvEB6G5TnlW11JBdp254G2b84/i0N9h9e7E+L09R5VNlVLE9NO7q+5Cngrk8BA0AAAQQQQKBXBXIsAPuEfZOysgH6Ddp+YClrtL5O+YkS22Faicd+L+w8Rn2fa5Uyr1jeqOV8pfstXvWL/e6PjBEggAACCCCAQMUFJmc4/qXFmBY1GNtObXdS20+dtyp/nDaEpQvCelcRfcg5yh3KCq+oeflmxdvfrXS3xaIv9rs7Ku4dAQQQQAABBHpAIMcrgKNl/UPdwE8X31znhg9q2xblLuX40n5fAby9tG2N1heUtsXVKVqZHjIt7mxpPxZ9sd/SO+FkCCCAAAIIIDARBXqhAHy/Ju46JV4VdNH3IcWv8Xun8l+Ki8DjlNQOUOeptFIsve7tjdr52rE9ZFOjA8e9ndcAjpuQEyCAAAIIIIBAfYFOFYDLdPeN3riRtr+h/hCbbvVVvN9Uri4d5YLv75UHlLuVs5VvKOcpsfm+Y5uklfK2uH+FVmaEHBx3trQfr/rFfkvvhJMhgAACCCCAwEQU6NRrAK8S7uphgPuH2V9v9we08SHl/no7S9vu0fp7w7Yn1S9f7fO7iMtXBcNNBt4tvCtuaFs/Fn2x37Y75MQIIIAAAgggMFEEOlUAPi1Qp5Vtqk72+4qflh1JO0oH+anh1Hxl8CTlsrRBy4XKhrDevS5PAXfPnntGAAF00CuIAAAPMUlEQVQEEECgxwU6VQCOhvEQHbyv4uVeypGK2/eV5wZ6g/+coYXH/+WwLXX9Tt5+5TtKn+Irf349oJPaFeqsV5YoX1PeoZyoHKt0v8WrfrHf/ZExAgQQQAABBBCouECOBeCnZPq+4Op38rr5Xbxr3SmaP/blq8rWtCEsXfT9tXKQ4jeHuBB8u/JNJTVf6TtTWa58WtmouKi8V+l+i0Vf7Hd/ZIwAAQQQQAABBCoukGMBuEimznCt2ce1fFY3doZrN+sAJ7/GU8D5zQkjQgABBBBAoEcEOvUu4B7h6uCXEa/6xX4Hh8BdIYAAAggggEBvClAA5jqvseiL/VzHy7gQQAABBBBAoDICFIC5ThVPAec6M4wLAQQQQACBygtQAOY6hfGqX+znOl7GhQACCCCAAAKVEaAAzHWqYtEX+7mOl3EhgAACCCCAQGUEKABznapY9MV+ruNlXAgggAACCCBQGQEKwFynitcA5jozjAsBBBBAAIHKC1AA5jqF8apf7Oc6XsaFAAIIIIAAApURoADMdapi0Rf7uY6XcSGAAAIIIIBAZQQoAHOdKp4CznVmGBcCCCCAAAKVF6AAzHUK41W/2M91vIwLAQQQQAABBCojQAGY61TFoi/2cx0v40IAAQQQQACByghQAOY6VTwFnOvMMC4EEEAAAQQqL0ABmOsUxqt+sZ/reBkXAggggAACCFRGgAIw16maPHloZBSAQxb0EEAAAQQQQGDcAhSA4yZs0wn22WfoxLt3D/XpIYAAAggggAAC4xSgABwnYNtuPmPG0Km3bh3q00MAAQQQQAABBMYpQAE4TsC23Ty+CeSZZ9p2N5wYAQQQQAABBCaeAAVgFeb82WerMErGiAACCCCAAAIVEaAArMJEUQBWYZYYIwIIIIAAApURoADMeapmzRoc3ckn5zxKxoYAAggggAACFRMInzVSsZFPhOE+8ECttn59rXbKKRPhq+VrRAABBBBAAIEOCVAAdgh6THczc2atdvrpY7opN0IAAQQQQAABBBoJ8BRwIxm2I4AAAggggAACPSpAAdijE8uXhQACCCCAAAIINBKgAGwkw3YEEEAAAQQQQKBHBSgAe3Ri+bIQQAABBBBAAIFGAhSAjWTYjgACCCCAAAII9KgABWCPTixfFgIIIIAAAggg0EiAArCRDNsRQAABBBBAAIEeFaAA7NGJ5ctCAAEEEEAAAQQaCVAANpJhOwIIIIAAAggg0KMCFIA9OrF8WQgggAACCCCAQCOB3ArAORro1coTyk5lo3KR0qfENlcr6xQf82PlQmWSEttpWnlU2VUsT4071ffxy5TNis+zVnmtQkMAAQQQQAABBHpaILcC8HBpe0xnKS7GPqp8WLlYSW26OncoLtzeqHxEOU/5mJLaMercoKxS5hXLG7Wcr6T25+r4Nn+i+DxPKj7vNIWGAAIIIIAAAgj0rED5qlmOX+jHNajFyiuKwbm/Qpmp+Oqe2ycUF4IHK3sUF38uFN+mpHabOluVdyv+ul1AXq5corhNUZ5SlihfUEbSfB/b1WrTp7tLQwABBBBAAIHcBXbs2FGbMWOGh+l/duQ+3naMb3I7Ttric3pyng3n9NW9dUoq/rxrjeKicI7ip499zGVKbD7mnGLDy7U8QLm9WPfC5/N5FyhfUEbc/ECiIYAAAggggEA1BPi5XavlXgC+Ug8lX9k7NzykXLj1h3V3feXOzftcAHqZtqk70Lzu7W5pWe+YQwcPqfuvrxI6qR3ozuzZs9M6SwQQQAABBBCojoBf9jUhr+J0qgBcJuClwzwe/Dq8+8Ixs9S/TblJ+Yew3d09pfVJxXrcHvve7WPK28rr9Y4pTj2wOF//Lo0b1L9Uuai0rRWrflBuUvy09k9bcULOUVcA57osLd+Ic8tJ654Q57osbdmIdVtYf+Gk7XT2uf1ysAnZOlUAXiXd1cMI94f9Lv7+Vblb+VDY7q7frJGu4KVd+xeddEWv0TFxv2/i82wpbuuFz5OOCZv/v7tCPRd8sfmpY6ddzcXfhPztpF2gDc6LcwOYFm/GucWgDU6HcwOYNmzGug2odU7ZDmd+ttaB7uamg3Tn31WuV/aqM5DF2rZV6Qv7/MYNfxxMuhJ4g/rfDPvdvVXxOd18nAs/vxM4NZ9vm3JW2tDlpd9V4iuUXtLaJ4Bz+2zjmXGOGu3r49w+2/KZsS6LtGcd5/a4ZndWX/n7nnKX4kLQV+hS1B1oflOIr/Bdp7xO8ef7bVfOVVJboM7PFReGhxfLF7Wcr6TmfS74fHufx+fbrPiScA6NB31nZgFnnDsj0Jl74fHcGWffC9adsca5M85dv5dFGoGvetVLHNxcraxXXlB8JW+pkq7+qTvQ3qV/H1d2K48p71Ri8/HLFN/e51mnuBDMpU3RQJYpXtLaJ4Bz+2zjmXGOGu3r49w+2/KZsS6LtGcd5/a4clYEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEBirwNm64ROK3518v/LbCm3kAv6LLd9S/MGhP1H+STlMic3vLLtSeVr5mfLPiv/qSmyHaOUWxft93OcUf14krb6A3f0O/svDbpwDxji6/lisLynPKM8rDymvV1JLn2rgj7LaqaxVXqvEto9WVin+2CzH/ZcptEEB/2GE5Yq/99rwB8qFyi8pqeGcJEa3PE6H+3upH5/+HnGKElurXP0JIf5ED8+fPxvY8+dz0xCohMAZGqU/uuYDymsU/zB9TjlEoY1M4DYdtkjxD8B5yteVHyq/pqS2Up1NyonKUcq/KP6hmj583MtHiu3e7+P8DeVKhfaLAv5Tjv7B+bDix2xqOCeJsS/30U37lX9UjlbmKCcor1RSW6KO/6qBP+7KH2e1WvEP22lKareq48f0MUXcv0WhDQpcoIV/0Xu7MkfxR4n9VPkzJTWck8Tolm/T4csVPz7rFYCtcPXnBfozgq9X/H/A9+X/E+cqNAQqIXCvRrmyNNLHtL6itI3VkQvsp0P9Tce/hbr5w8RdZJ/hlaLN0vJ/lJOLdX/D8rq3p3amOr4q6280tCGBqep+V3GRvFZJBSDOwmhB+4zO8W9NzuMrHFsU/xBNbYo625Szig3+ZdL/B+YX6168SfG2w7xCG/hF8eqSw1e0vqrYhnMJZ4yr5QKwVa6LNR4/5v3YT+0T6sS/Epa2s5RAvLQNSPcF+jSE1yu3l4bi9QWlbayOXMCFiNuzg4sB419WPzr7asm3leTsqyRe9/bU1qjjby6eI9qQwOfV/YZy59CmgZ6dcC6hjGH193Sb+5SbFL+k4UHlg0pqL1fnACU+nndpfZ0SH8/bte5fMFO7Rx1vS8ek7RN1+e/6wn1l9dUFwDwtj1XSnxXFuYBp8aJVrv6e7ce8H/up+Xv2LGVO2sBySGDyUJdeBgK/rjHspTxVGovX/Q2eNnqBSbrJpYq/ubugc7PlbmWrV0KLzj6mPA8+3rdjLobQzlT3txQ/BVxuOJdFxrb+Ct1sseLH8cWKnwb+nOIfdF9U0uOx/Hj1+qGKm49x8Vhu3pZuX9430dYv0Rc8Q3lc8dV/fy/208LXK27JCedBj1b92ypXn6e/NKg0V973RGnfhF+lAMzzIeBL5LG5iClvi/vpNxa4SruOUPyb/HCt7FzPvHzMcOfs5f2z9cVdoSxUXhjFF1o2xLk5np+p8RXATxaHPailX9+6WHEBmFrZEeckM7KlXxLyXuUPlO8oRyp+OYOfBbhWSQ3nJNHaZStc653Doyxvb+3IK3o2f2Oh5SPwtIbi3zzTb0RpZPurk36TSdtYDi9wpQ7x02fHK5vC4X6hcJ/iF9fHFp19THkefLyf0mQuBtX8FK/N7ld+XuTNWv5p0bcTzkIYZ9ui2z9aOsdjWk9vDPNj1a38eC0/nmcOHvaSf/fTGo/nQZK/0uIzymrlEcWv/btMOV9xw3nQodX/tsrV56n3f8Dj5TFeZ9YoAOugdHGTn170D9OTSmPw+obSNlYbC/jKh6/8+V1gb1WeUGKz8YtKdD5Q669TkvPdxbq3p7ZQHT/t5tvTarW7hDBX8ZWSFF+p+nKx7j7OQhhn+w/d/rDSOV6t9R8W2/z49g+/+Hju07qL8fh49tObRyup+Q0h3paOSdsn6vJX9YX/b+mL9y/k6eckziWcFq22ytXfs/1GPz/2U1uozmalP21giUDOAn4awoXg+5XXKP4N9DnlUIU2MoG/1WHbFP8A9G+EKb+ifmor1fmRcoJylOJi5iHFr/tx89JXAe5UvN/H/UjxVUVaY4G12nV52I1zwBhj16+vdCHtp4B/Q/FTlD9T3qOktkQdP+ZPVfyLzHWKf/BNU1K7VZ2HlTcV+U8tb1FogwLXaOFnCtLHwNjyv5VLlNRwThKjW07V4UcW2aPlR4t+uordClf/MuNfhPzY9/8Bz9925VyFhkBlBM7WSPuVdLXJv9XQRi7gbzD1siicYm/1Xcw9ozyv+AfhbCU2f3P6uuL9Ps7HT1FojQXWalcsAHFubDWaPb+jgx9RXlAeUz6oxOar3suULYqPWaf4h2Bs+2rlS8qOIu6/TKENCrhY9mPXV1Z3KhuV5UqfkhrOSWJ0y7fo8Hrfk68pTtMq17k633rF/wf8f2Gp4nPTEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQqKzA/wExMJe9L9JVagAAAABJRU5ErkJggg==\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10f95c048>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(episodes,cumulative_reward_history, c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
